import requests
import sys
import json
from datetime import datetime, timedelta

class RegenMedAIProTester:
    def __init__(self, base_url="https://7270ea2f-1d23-46a0-9a6e-bef595343dd6.preview.emergentagent.com"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
        self.tests_run = 0
        self.tests_passed = 0
        self.patient_id = None
        self.protocol_id = None
        self.headers = {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer demo-token'
        }

    def run_test(self, name, method, endpoint, expected_status, data=None, timeout=60):
        """Run a single API test"""
        url = f"{self.api_url}/{endpoint}" if endpoint else self.api_url
        
        self.tests_run += 1
        print(f"\nüîç Testing {name}...")
        print(f"   URL: {url}")
        
        try:
            if method == 'GET':
                response = requests.get(url, headers=self.headers, timeout=timeout)
            elif method == 'POST':
                response = requests.post(url, json=data, headers=self.headers, timeout=timeout)
            elif method == 'PUT':
                response = requests.put(url, json=data, headers=self.headers, timeout=timeout)

            success = response.status_code == expected_status
            if success:
                self.tests_passed += 1
                print(f"‚úÖ Passed - Status: {response.status_code}")
                try:
                    response_data = response.json()
                    # Truncate long responses for readability
                    response_str = json.dumps(response_data, indent=2)
                    if len(response_str) > 300:
                        response_str = response_str[:300] + "..."
                    print(f"   Response: {response_str}")
                    return True, response_data
                except:
                    return True, {}
            else:
                print(f"‚ùå Failed - Expected {expected_status}, got {response.status_code}")
                try:
                    error_data = response.json()
                    print(f"   Error: {error_data}")
                except:
                    print(f"   Error: {response.text}")
                return False, {}

        except Exception as e:
            print(f"‚ùå Failed - Error: {str(e)}")
            return False, {}

    def test_health_check(self):
        """Test health check endpoint with AI engine status"""
        success, response = self.run_test(
            "Health Check & AI Engine Status",
            "GET",
            "health",
            200
        )
        if success:
            print(f"   System Status: {response.get('status', 'unknown')}")
            services = response.get('services', {})
            print(f"   Database: {services.get('database', 'unknown')}")
            print(f"   AI Engine: {services.get('ai_engine', 'unknown')}")
            print(f"   Knowledge Base: {services.get('knowledge_base', 'unknown')}")
            print(f"   Version: {response.get('version', 'unknown')}")
        return success

    def test_root_endpoint(self):
        """Test root API endpoint"""
        success, response = self.run_test(
            "Root API Endpoint",
            "GET",
            "",
            200
        )
        if success:
            print(f"   Message: {response.get('message', 'No message')}")
        return success

    def test_create_patient(self):
        """Test comprehensive patient creation with regenerative medicine data"""
        patient_data = {
            "demographics": {
                "name": "Dr. Sarah Mitchell",
                "age": "58",
                "gender": "Female",
                "occupation": "Physician",
                "insurance": "Self-pay"
            },
            "chief_complaint": "Bilateral knee osteoarthritis seeking regenerative alternatives",
            "history_present_illness": "58-year-old female physician with progressive bilateral knee pain over 3 years. Pain worse with activity, morning stiffness lasting 30 minutes. Failed conservative management including NSAIDs, physical therapy, and corticosteroid injections. Seeking regenerative medicine options to avoid knee replacement surgery.",
            "past_medical_history": ["Osteoarthritis", "Hypertension", "Hypothyroidism"],
            "medications": ["Lisinopril 10mg daily", "Levothyroxine 75mcg daily", "Ibuprofen PRN"],
            "allergies": ["NKDA"],
            "vital_signs": {
                "temperature": "98.6",
                "blood_pressure": "128/82",
                "heart_rate": "72",
                "respiratory_rate": "16",
                "oxygen_saturation": "98",
                "weight": "145",
                "height": "5'6\""
            },
            "symptoms": ["bilateral knee pain", "morning stiffness", "decreased mobility", "functional limitation"],
            "lab_results": {
                "inflammatory_markers": {
                    "CRP": "2.1 mg/L",
                    "ESR": "18 mm/hr"
                },
                "complete_blood_count": {
                    "WBC": "6.2 K/uL",
                    "RBC": "4.5 M/uL",
                    "platelets": "285 K/uL"
                }
            },
            "imaging_data": [
                {
                    "type": "X-ray",
                    "location": "bilateral knees",
                    "findings": "Grade 2-3 osteoarthritis with joint space narrowing and osteophyte formation",
                    "date": "2024-01-15"
                },
                {
                    "type": "MRI",
                    "location": "right knee",
                    "findings": "Cartilage thinning, meniscal degeneration, mild bone marrow edema",
                    "date": "2024-02-01"
                }
            ],
            "genetic_data": {
                "regenerative_markers": {
                    "VEGF_polymorphism": "positive",
                    "collagen_synthesis_genes": "normal"
                }
            }
        }

        success, response = self.run_test(
            "Create Comprehensive Patient Record",
            "POST",
            "patients",
            200,
            data=patient_data
        )
        
        if success and 'patient_id' in response:
            self.patient_id = response['patient_id']
            print(f"   Created Patient ID: {self.patient_id}")
            print(f"   Patient Name: {response.get('demographics', {}).get('name', 'Unknown')}")
            return True
        return False

    def test_list_patients(self):
        """Test listing patients for practitioner"""
        success, response = self.run_test(
            "List Patients",
            "GET",
            "patients",
            200
        )
        
        if success:
            patient_count = len(response) if isinstance(response, list) else 0
            print(f"   Found {patient_count} patients")
            if patient_count > 0:
                print(f"   First patient: {response[0].get('demographics', {}).get('name', 'Unknown')}")
        return success

    def test_get_patient(self):
        """Test retrieving specific patient data"""
        if not self.patient_id:
            print("‚ùå No patient ID available for testing")
            return False

        success, response = self.run_test(
            "Get Patient Details",
            "GET",
            f"patients/{self.patient_id}",
            200
        )
        
        if success:
            print(f"   Retrieved Patient: {response.get('demographics', {}).get('name', 'Unknown')}")
            print(f"   Chief Complaint: {response.get('chief_complaint', 'None')[:50]}...")
        return success

    def test_analyze_patient(self):
        """Test comprehensive AI analysis of patient data"""
        if not self.patient_id:
            print("‚ùå No patient ID available for analysis testing")
            return False

        print("   This may take 30-60 seconds for AI processing...")
        success, response = self.run_test(
            "AI Patient Analysis (Regenerative Medicine)",
            "POST",
            f"patients/{self.patient_id}/analyze",
            200,
            data={},
            timeout=90
        )
        
        if success:
            diagnostic_results = response.get('diagnostic_results', [])
            print(f"   Generated {len(diagnostic_results)} diagnostic results")
            if diagnostic_results:
                top_result = diagnostic_results[0]
                print(f"   Primary Diagnosis: {top_result.get('diagnosis', 'Unknown')}")
                print(f"   Confidence Score: {top_result.get('confidence_score', 0):.2f}")
                print(f"   Regenerative Targets: {len(top_result.get('regenerative_targets', []))}")
                print(f"   Mechanisms Involved: {len(top_result.get('mechanisms_involved', []))}")
        return success

    def test_therapies_database(self):
        """Test comprehensive therapy database access"""
        success, response = self.run_test(
            "Therapy Database & Schools of Thought",
            "GET",
            "therapies",
            200
        )
        
        if success:
            therapies = response.get('therapies', [])
            schools = response.get('schools_of_thought', {})
            print(f"   Available Therapies: {len(therapies)}")
            print(f"   Schools of Thought: {len(schools)}")
            
            # Print therapy names
            therapy_names = [t.get('name', 'Unknown') for t in therapies[:3]]
            print(f"   Sample Therapies: {', '.join(therapy_names)}")
            
            # Print school names
            school_names = list(schools.keys())[:3]
            print(f"   Sample Schools: {', '.join(school_names)}")
        return success

    def test_generate_protocol_traditional(self):
        """Test protocol generation - Traditional Autologous"""
        return self._test_protocol_generation("traditional_autologous", "Traditional Autologous (US Legal)")

    def test_generate_protocol_biologics(self):
        """Test protocol generation - Biologics"""
        return self._test_protocol_generation("biologics", "Biologics & Allogenic")

    def test_generate_protocol_ai_optimized(self):
        """Test protocol generation - AI Optimized"""
        return self._test_protocol_generation("ai_optimized", "AI-Optimized Best Protocol")

    def test_generate_protocol_experimental(self):
        """Test protocol generation - Experimental Cutting Edge"""
        return self._test_protocol_generation("experimental", "Experimental & Cutting-Edge")

    def _test_protocol_generation(self, school_key, school_name):
        """Helper method to test protocol generation for different schools"""
        if not self.patient_id:
            print("‚ùå No patient ID available for protocol generation")
            return False

        protocol_data = {
            "patient_id": self.patient_id,
            "school_of_thought": school_key
        }

        print(f"   This may take 30-60 seconds for AI protocol generation...")
        success, response = self.run_test(
            f"Generate Protocol - {school_name}",
            "POST",
            "protocols/generate",
            200,
            data=protocol_data,
            timeout=90
        )
        
        if success:
            if not self.protocol_id:  # Store first protocol ID for later tests
                self.protocol_id = response.get('protocol_id')
            
            print(f"   Protocol ID: {response.get('protocol_id', 'Unknown')}")
            print(f"   School: {response.get('school_of_thought', 'Unknown')}")
            print(f"   Confidence Score: {response.get('confidence_score', 0):.2f}")
            print(f"   Protocol Steps: {len(response.get('protocol_steps', []))}")
            print(f"   Expected Outcomes: {len(response.get('expected_outcomes', []))}")
            print(f"   Legal Warnings: {len(response.get('legal_warnings', []))}")
            
            # Print first protocol step if available
            steps = response.get('protocol_steps', [])
            if steps:
                first_step = steps[0]
                print(f"   First Step: {first_step.get('therapy', 'Unknown')} - {first_step.get('dosage', 'Unknown')}")
        return success

    def test_get_protocol(self):
        """Test retrieving generated protocol"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for testing")
            return False

        success, response = self.run_test(
            "Get Protocol Details",
            "GET",
            f"protocols/{self.protocol_id}",
            200
        )
        
        if success:
            print(f"   Protocol School: {response.get('school_of_thought', 'Unknown')}")
            print(f"   Status: {response.get('status', 'Unknown')}")
            print(f"   Steps Count: {len(response.get('protocol_steps', []))}")
        return success

    def test_approve_protocol(self):
        """Test protocol approval"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for approval testing")
            return False

        success, response = self.run_test(
            "Approve Protocol",
            "PUT",
            f"protocols/{self.protocol_id}/approve",
            200
        )
        
        if success:
            print(f"   Approval Status: {response.get('status', 'Unknown')}")
            print(f"   Approved At: {response.get('approved_at', 'Unknown')}")
        return success

    def test_dashboard_analytics(self):
        """Test practitioner dashboard analytics"""
        success, response = self.run_test(
            "Dashboard Analytics",
            "GET",
            "analytics/dashboard",
            200
        )
        
        if success:
            stats = response.get('summary_stats', {})
            print(f"   Total Patients: {stats.get('total_patients', 0)}")
            print(f"   Protocols Pending: {stats.get('protocols_pending', 0)}")
            print(f"   Protocols Approved: {stats.get('protocols_approved', 0)}")
            print(f"   Outcomes Tracked: {stats.get('outcomes_tracked', 0)}")
            print(f"   Recent Activities: {len(response.get('recent_activities', []))}")
        return success

    def test_submit_outcome(self):
        """Test outcome data submission"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for outcome testing")
            return False

        outcome_data = {
            "protocol_id": self.protocol_id,
            "patient_id": self.patient_id,
            "followup_date": datetime.utcnow().isoformat(),
            "measurements": {
                "pain_scale": 3,
                "range_of_motion": "improved",
                "functional_score": 85
            },
            "practitioner_notes": "Patient reports significant improvement in knee pain and mobility after PRP treatment. No adverse events reported.",
            "patient_reported_outcomes": {
                "pain_improvement": "70%",
                "activity_level": "much improved",
                "satisfaction": "very satisfied"
            },
            "adverse_events": [],
            "satisfaction_score": 9
        }

        success, response = self.run_test(
            "Submit Patient Outcome",
            "POST",
            "outcomes",
            200,
            data=outcome_data
        )
        
        if success:
            print(f"   Outcome ID: {response.get('outcome_id', 'Unknown')}")
            print(f"   Satisfaction Score: {response.get('satisfaction_score', 'Unknown')}")
        return success

    # ========== FILE UPLOAD AND PROCESSING TESTING ==========

    def test_file_upload_patient_chart(self):
        """Test uploading and processing patient chart PDF"""
        if not self.patient_id:
            print("‚ùå No patient ID available for file upload testing")
            return False

        # Create a simulated PDF file content
        import base64
        simulated_pdf_content = base64.b64encode(b"SIMULATED_PATIENT_CHART_PDF_CONTENT_FOR_TESTING").decode()
        
        # Prepare multipart form data
        files = {
            'file': ('patient_chart.pdf', base64.b64decode(simulated_pdf_content), 'application/pdf')
        }
        data = {
            'patient_id': self.patient_id,
            'file_category': 'chart'
        }
        
        # Remove JSON headers for multipart upload
        upload_headers = {'Authorization': 'Bearer demo-token'}
        
        try:
            import requests
            response = requests.post(
                f"{self.api_url}/files/upload",
                files=files,
                data=data,
                headers=upload_headers,
                timeout=60
            )
            
            success = response.status_code == 200
            if success:
                self.tests_passed += 1
                response_data = response.json()
                print(f"‚úÖ Patient Chart Upload - Status: {response.status_code}")
                print(f"   File ID: {response_data.get('file_id', 'Unknown')}")
                print(f"   Processing Status: {response_data.get('status', 'Unknown')}")
                print(f"   Confidence Score: {response_data.get('confidence_score', 0):.2f}")
                print(f"   Medical Insights: {len(response_data.get('medical_insights', {}))}")
                return True
            else:
                print(f"‚ùå Failed - Expected 200, got {response.status_code}")
                print(f"   Error: {response.text}")
                return False
                
        except Exception as e:
            print(f"‚ùå Failed - Error: {str(e)}")
            return False
        finally:
            self.tests_run += 1

    def test_file_upload_genetic_data(self):
        """Test uploading and processing genetic test results"""
        if not self.patient_id:
            print("‚ùå No patient ID available for genetic file testing")
            return False

        # Create simulated genetic data JSON
        genetic_data = {
            "patient_id": self.patient_id,
            "test_type": "pharmacogenomics",
            "variants": [
                {"gene": "CYP2D6", "variant": "*1/*2", "phenotype": "normal_metabolizer"},
                {"gene": "COMT", "variant": "Val158Met", "phenotype": "intermediate"},
                {"gene": "COL1A1", "variant": "rs1800012", "phenotype": "favorable_healing"}
            ],
            "regenerative_markers": {
                "VEGF_polymorphism": "positive",
                "collagen_synthesis_genes": "normal",
                "inflammatory_response": "low_risk"
            }
        }
        
        import json
        genetic_json = json.dumps(genetic_data).encode()
        
        files = {
            'file': ('genetic_results.json', genetic_json, 'application/json')
        }
        data = {
            'patient_id': self.patient_id,
            'file_category': 'genetics'
        }
        
        upload_headers = {'Authorization': 'Bearer demo-token'}
        
        try:
            import requests
            response = requests.post(
                f"{self.api_url}/files/upload",
                files=files,
                data=data,
                headers=upload_headers,
                timeout=60
            )
            
            success = response.status_code == 200
            if success:
                self.tests_passed += 1
                response_data = response.json()
                print(f"‚úÖ Genetic Data Upload - Status: {response.status_code}")
                print(f"   File ID: {response_data.get('file_id', 'Unknown')}")
                print(f"   Regenerative Insights: {len(response_data.get('medical_insights', {}))}")
                print(f"   Confidence Score: {response_data.get('confidence_score', 0):.2f}")
                return True
            else:
                print(f"‚ùå Failed - Expected 200, got {response.status_code}")
                print(f"   Error: {response.text}")
                return False
                
        except Exception as e:
            print(f"‚ùå Failed - Error: {str(e)}")
            return False
        finally:
            self.tests_run += 1

    def test_file_upload_dicom_imaging(self):
        """Test uploading and processing DICOM imaging files"""
        if not self.patient_id:
            print("‚ùå No patient ID available for DICOM testing")
            return False

        # Create simulated DICOM file
        simulated_dicom = b"SIMULATED_DICOM_FILE_CONTENT_FOR_KNEE_MRI_TESTING"
        
        files = {
            'file': ('knee_mri.dcm', simulated_dicom, 'application/dicom')
        }
        data = {
            'patient_id': self.patient_id,
            'file_category': 'imaging'
        }
        
        upload_headers = {'Authorization': 'Bearer demo-token'}
        
        try:
            import requests
            response = requests.post(
                f"{self.api_url}/files/upload",
                files=files,
                data=data,
                headers=upload_headers,
                timeout=60
            )
            
            success = response.status_code == 200
            if success:
                self.tests_passed += 1
                response_data = response.json()
                print(f"‚úÖ DICOM Upload - Status: {response.status_code}")
                print(f"   File ID: {response_data.get('file_id', 'Unknown')}")
                print(f"   Processing Results: {len(response_data.get('processing_results', {}))}")
                print(f"   Medical Insights: {len(response_data.get('medical_insights', {}))}")
                return True
            else:
                print(f"‚ùå Failed - Expected 200, got {response.status_code}")
                print(f"   Error: {response.text}")
                return False
                
        except Exception as e:
            print(f"‚ùå Failed - Error: {str(e)}")
            return False
        finally:
            self.tests_run += 1

    def test_file_upload_lab_results(self):
        """Test uploading and processing lab results"""
        if not self.patient_id:
            print("‚ùå No patient ID available for lab results testing")
            return False

        # Create simulated lab results CSV
        lab_data = """Test,Value,Reference Range,Units,Status
CBC - WBC,6.2,4.0-11.0,K/uL,Normal
CBC - RBC,4.5,4.2-5.4,M/uL,Normal
CBC - Platelets,285,150-450,K/uL,Normal
ESR,18,0-30,mm/hr,Normal
CRP,2.1,<3.0,mg/L,Normal
Vitamin D,32,30-100,ng/mL,Normal
Vitamin C,1.2,0.4-2.0,mg/dL,Normal
Zinc,95,70-120,mcg/dL,Normal
Magnesium,2.1,1.7-2.2,mg/dL,Normal
PDGF,45,20-80,pg/mL,Normal
VEGF,125,62-707,pg/mL,Normal
IGF-1,180,109-284,ng/mL,Normal"""
        
        files = {
            'file': ('lab_results.csv', lab_data.encode(), 'text/csv')
        }
        data = {
            'patient_id': self.patient_id,
            'file_category': 'labs'
        }
        
        upload_headers = {'Authorization': 'Bearer demo-token'}
        
        try:
            import requests
            response = requests.post(
                f"{self.api_url}/files/upload",
                files=files,
                data=data,
                headers=upload_headers,
                timeout=60
            )
            
            success = response.status_code == 200
            if success:
                self.tests_passed += 1
                response_data = response.json()
                print(f"‚úÖ Lab Results Upload - Status: {response.status_code}")
                print(f"   File ID: {response_data.get('file_id', 'Unknown')}")
                print(f"   Processing Results: {len(response_data.get('processing_results', {}))}")
                print(f"   Regenerative Insights: {len(response_data.get('medical_insights', {}))}")
                return True
            else:
                print(f"‚ùå Failed - Expected 200, got {response.status_code}")
                print(f"   Error: {response.text}")
                return False
                
        except Exception as e:
            print(f"‚ùå Failed - Error: {str(e)}")
            return False
        finally:
            self.tests_run += 1

    def test_get_patient_files(self):
        """Test retrieving all uploaded files for a patient"""
        if not self.patient_id:
            print("‚ùå No patient ID available for file retrieval testing")
            return False

        success, response = self.run_test(
            "Get Patient Files",
            "GET",
            f"files/patient/{self.patient_id}",
            200
        )
        
        if success:
            uploaded_files = response.get('uploaded_files', [])
            processed_files = response.get('processed_files', [])
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Total Files: {response.get('total_files', 0)}")
            print(f"   Uploaded Files: {len(uploaded_files)}")
            print(f"   Processed Files: {len(processed_files)}")
        return success

    def test_comprehensive_patient_analysis(self):
        """Test comprehensive analysis combining all patient files"""
        if not self.patient_id:
            print("‚ùå No patient ID available for comprehensive analysis testing")
            return False

        success, response = self.run_test(
            "Comprehensive Patient File Analysis",
            "GET",
            f"files/comprehensive-analysis/{self.patient_id}",
            200,
            timeout=45
        )
        
        if success:
            analysis = response.get('comprehensive_analysis', {})
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            print(f"   Multi-modal Insights: {len(analysis.get('multi_modal_insights', {}))}")
            print(f"   Integrated Recommendations: {len(analysis.get('integrated_recommendations', []))}")
            print(f"   Confidence Level: {analysis.get('confidence_level', 0):.2f}")
        return success

    def test_file_based_protocol_generation(self):
        """Test generating protocol using comprehensive file analysis"""
        if not self.patient_id:
            print("‚ùå No patient ID available for file-based protocol testing")
            return False

        print("   This may take 30-60 seconds for file-based AI protocol generation...")
        success, response = self.run_test(
            "Generate Protocol from Files",
            "POST",
            f"protocols/generate-from-files?patient_id={self.patient_id}&school_of_thought=ai_optimized",
            200,
            data={},
            timeout=90
        )
        
        if success:
            protocol = response.get('protocol', {})
            print(f"   Protocol ID: {protocol.get('protocol_id', 'Unknown')}")
            print(f"   Files Analyzed: {response.get('file_insights_used', 0)}")
            print(f"   Enhancement Confidence: {response.get('enhancement_confidence', 0):.2f}")
            print(f"   Multi-modal Analysis: {len(response.get('multi_modal_analysis', {}))}")
            print(f"   Protocol Steps: {len(protocol.get('protocol_steps', []))}")
            
            # Store protocol ID for potential cleanup
            if not self.protocol_id:
                self.protocol_id = protocol.get('protocol_id')
        return success

    # ========== ADVANCED AI FEATURES TESTING ==========

    def test_advanced_system_status(self):
        """Test comprehensive advanced system status"""
        success, response = self.run_test(
            "Advanced System Status & Health",
            "GET",
            "advanced/system-status",
            200
        )
        
        if success:
            services = response.get('services', {})
            db_stats = response.get('database_stats', {})
            
            print(f"   Federated Learning: {services.get('federated_learning', {}).get('status', 'unknown')}")
            print(f"   Literature Integration: {services.get('literature_integration', {}).get('status', 'unknown')}")
            print(f"   DICOM Processing: {services.get('dicom_processing', {}).get('status', 'unknown')}")
            print(f"   Outcome Prediction: {services.get('outcome_prediction', {}).get('status', 'unknown')}")
            
            print(f"   Total Patients: {db_stats.get('total_patients', 0)}")
            print(f"   Total Protocols: {db_stats.get('total_protocols', 0)}")
            print(f"   Literature Papers: {db_stats.get('literature_papers', 0)}")
            print(f"   Federated Participants: {db_stats.get('federated_participants', 0)}")
        return success

    def test_federated_register_clinic(self):
        """Test federated learning clinic registration"""
        clinic_data = {
            "total_patients": 150,
            "avg_age": 58.5,
            "therapy_distribution": {
                "prp": 0.4,
                "bmac": 0.3,
                "exosomes": 0.3
            },
            "outcomes": [
                {"therapy": "prp", "success_rate": 0.85},
                {"therapy": "bmac", "success_rate": 0.82}
            ]
        }

        success, response = self.run_test(
            "Register Clinic for Federated Learning",
            "POST",
            "federated/register-clinic",
            200,
            data=clinic_data
        )
        
        if success:
            print(f"   Registration Status: {response.get('status', 'unknown')}")
            print(f"   Clinic ID: {response.get('clinic_id', 'unknown')}")
            print(f"   Privacy Level: {response.get('privacy_level', 'unknown')}")
        return success

    def test_federated_global_model_status(self):
        """Test global federated learning model status"""
        success, response = self.run_test(
            "Global Federated Model Status",
            "GET",
            "federated/global-model-status",
            200
        )
        
        if success:
            print(f"   Model Version: {response.get('model_version', 'unknown')}")
            print(f"   Participants: {response.get('participants', 0)}")
            print(f"   Status: {response.get('status', 'unknown')}")
            print(f"   Performance Improvement: {response.get('performance_improvement', 0):.3f}")
        return success

    def test_literature_latest_updates(self):
        """Test latest literature updates from PubMed integration"""
        success, response = self.run_test(
            "Latest Literature Updates",
            "GET",
            "literature/latest-updates",
            200,
            timeout=30
        )
        
        if success:
            processing_result = response.get('processing_result', {})
            recent_papers = response.get('recent_papers', [])
            
            print(f"   Processing Status: {processing_result.get('status', 'unknown')}")
            print(f"   Recent Papers Found: {len(recent_papers)}")
            print(f"   Total Papers in DB: {response.get('total_papers_in_database', 0)}")
            
            if recent_papers:
                first_paper = recent_papers[0]
                print(f"   Sample Paper: {first_paper.get('title', 'Unknown')[:50]}...")
        return success

    def test_literature_search(self):
        """Test literature database search functionality"""
        success, response = self.run_test(
            "Search Literature Database",
            "GET",
            "literature/search?query=regenerative%20medicine&limit=10",
            200
        )
        
        if success:
            papers = response.get('papers', [])
            print(f"   Query: {response.get('query', 'unknown')}")
            print(f"   Results Found: {response.get('total_results', 0)}")
            print(f"   Papers Returned: {len(papers)}")
            
            if papers:
                first_paper = papers[0]
                print(f"   Top Result: {first_paper.get('title', 'Unknown')[:50]}...")
                print(f"   Relevance Score: {first_paper.get('relevance_score', 0):.2f}")
        return success

    # ========== GOOGLE SCHOLAR INTEGRATION TESTING ==========

    def test_google_scholar_search_basic(self):
        """Test basic Google Scholar search functionality"""
        success, response = self.run_test(
            "Google Scholar Search - Basic Query",
            "GET",
            "literature/google-scholar-search?query=platelet%20rich%20plasma%20osteoarthritis&max_results=10",
            200,
            timeout=45
        )
        
        if success:
            papers = response.get('papers', [])
            print(f"   Query: {response.get('query', 'unknown')}")
            print(f"   Source: {response.get('source', 'unknown')}")
            print(f"   Papers Found: {response.get('total_results', 0)}")
            print(f"   Status: {response.get('status', 'unknown')}")
            
            if papers:
                first_paper = papers[0]
                print(f"   Top Result: {first_paper.get('title', 'Unknown')[:60]}...")
                print(f"   Authors: {', '.join(first_paper.get('authors', [])[:2])}")
                print(f"   Journal: {first_paper.get('journal', 'Unknown')}")
                print(f"   Year: {first_paper.get('year', 'Unknown')}")
                print(f"   Citations: {first_paper.get('citation_count', 0)}")
                print(f"   Relevance Score: {first_paper.get('relevance_score', 0):.2f}")
        return success

    def test_google_scholar_search_stem_cell(self):
        """Test Google Scholar search with stem cell therapy query"""
        success, response = self.run_test(
            "Google Scholar Search - Stem Cell Therapy",
            "GET",
            "literature/google-scholar-search?query=stem%20cell%20therapy&max_results=15",
            200,
            timeout=45
        )
        
        if success:
            papers = response.get('papers', [])
            print(f"   Query: {response.get('query', 'unknown')}")
            print(f"   Papers Found: {len(papers)}")
            print(f"   Search Timestamp: {response.get('search_timestamp', 'unknown')}")
            
            # Check for regenerative medicine relevance
            relevant_papers = [p for p in papers if p.get('relevance_score', 0) >= 0.5]
            print(f"   High Relevance Papers (‚â•0.5): {len(relevant_papers)}")
            
            if relevant_papers:
                top_paper = relevant_papers[0]
                print(f"   Top Relevant Paper: {top_paper.get('title', 'Unknown')[:50]}...")
                print(f"   Abstract Preview: {top_paper.get('abstract', 'No abstract')[:100]}...")
        return success

    def test_google_scholar_search_with_year_filter(self):
        """Test Google Scholar search with year filter"""
        success, response = self.run_test(
            "Google Scholar Search - With Year Filter",
            "GET",
            "literature/google-scholar-search?query=BMAC%20rotator%20cuff&max_results=10&year_filter=2023",
            200,
            timeout=45
        )
        
        if success:
            papers = response.get('papers', [])
            print(f"   Query: {response.get('query', 'unknown')}")
            print(f"   Year Filter Applied: 2023+")
            print(f"   Papers Found: {len(papers)}")
            
            # Check year distribution
            recent_papers = [p for p in papers if p.get('year', '').isdigit() and int(p.get('year', '0')) >= 2023]
            print(f"   Papers from 2023+: {len(recent_papers)}")
            
            if papers:
                sample_paper = papers[0]
                print(f"   Sample Paper Year: {sample_paper.get('year', 'Unknown')}")
                print(f"   Sample Title: {sample_paper.get('title', 'Unknown')[:50]}...")
        return success

    def test_google_scholar_error_handling(self):
        """Test Google Scholar search error handling"""
        success, response = self.run_test(
            "Google Scholar Search - Error Handling",
            "GET",
            "literature/google-scholar-search?query=&max_results=5",  # Empty query
            200,  # Should still return 200 with error in response
            timeout=30
        )
        
        if success:
            print(f"   Status: {response.get('status', 'unknown')}")
            if 'error' in response:
                print(f"   Error Handled: {response.get('error', 'unknown')[:50]}...")
            print(f"   Papers Returned: {len(response.get('papers', []))}")
            print(f"   Fallback Available: {'fallback_suggestion' in response}")
        return success

    def test_multi_source_search_comprehensive(self):
        """Test comprehensive multi-source search (PubMed + Google Scholar)"""
        success, response = self.run_test(
            "Multi-Source Search - Regenerative Medicine",
            "GET",
            "literature/multi-source-search?query=regenerative%20medicine&max_results_per_source=8",
            200,
            timeout=60
        )
        
        if success:
            papers = response.get('papers', [])
            source_stats = response.get('source_statistics', {})
            
            print(f"   Query: {response.get('query', 'unknown')}")
            print(f"   Search Type: {response.get('search_type', 'unknown')}")
            print(f"   Total Unique Papers: {response.get('total_unique_papers', 0)}")
            print(f"   Status: {response.get('status', 'unknown')}")
            
            # Source statistics
            pubmed_stats = source_stats.get('pubmed', {})
            scholar_stats = source_stats.get('google_scholar', {})
            
            print(f"   PubMed: {pubmed_stats.get('papers_found', 0)} papers, status: {pubmed_stats.get('status', 'unknown')}")
            print(f"   Google Scholar: {scholar_stats.get('papers_found', 0)} papers, status: {scholar_stats.get('status', 'unknown')}")
            
            if papers:
                # Check source diversity
                pubmed_papers = [p for p in papers if p.get('source') == 'pubmed']
                scholar_papers = [p for p in papers if p.get('source') == 'google_scholar']
                
                print(f"   PubMed Papers in Results: {len(pubmed_papers)}")
                print(f"   Google Scholar Papers in Results: {len(scholar_papers)}")
                
                # Show top result from each source
                if pubmed_papers:
                    top_pubmed = pubmed_papers[0]
                    print(f"   Top PubMed: {top_pubmed.get('title', 'Unknown')[:40]}... (PMID: {top_pubmed.get('pmid', 'N/A')})")
                
                if scholar_papers:
                    top_scholar = scholar_papers[0]
                    print(f"   Top Scholar: {top_scholar.get('title', 'Unknown')[:40]}... (Citations: {top_scholar.get('citation_count', 0)})")
        return success

    def test_multi_source_search_bmac(self):
        """Test multi-source search for BMAC rotator cuff"""
        success, response = self.run_test(
            "Multi-Source Search - BMAC Rotator Cuff",
            "GET",
            "literature/multi-source-search?query=BMAC%20rotator%20cuff&max_results_per_source=6",
            200,
            timeout=60
        )
        
        if success:
            papers = response.get('papers', [])
            source_stats = response.get('source_statistics', {})
            
            print(f"   Query: BMAC rotator cuff")
            print(f"   Total Unique Papers: {response.get('total_unique_papers', 0)}")
            print(f"   Deduplication Working: {len(papers) <= 12}")  # Should be ‚â§ max_results_per_source * 2
            
            # Check for relevant content
            bmac_papers = [p for p in papers if 'bmac' in p.get('title', '').lower() or 'bone marrow' in p.get('title', '').lower()]
            rotator_papers = [p for p in papers if 'rotator' in p.get('title', '').lower() or 'shoulder' in p.get('title', '').lower()]
            
            print(f"   BMAC-related Papers: {len(bmac_papers)}")
            print(f"   Rotator Cuff-related Papers: {len(rotator_papers)}")
            
            # Check relevance scores
            high_relevance = [p for p in papers if p.get('relevance_score', 0) >= 0.7]
            print(f"   High Relevance Papers (‚â•0.7): {len(high_relevance)}")
            
            if papers:
                top_paper = papers[0]
                print(f"   Top Result: {top_paper.get('title', 'Unknown')[:50]}...")
                print(f"   Top Relevance Score: {top_paper.get('relevance_score', 0):.2f}")
        return success

    def test_multi_source_deduplication(self):
        """Test deduplication functionality in multi-source search"""
        success, response = self.run_test(
            "Multi-Source Search - Deduplication Test",
            "GET",
            "literature/multi-source-search?query=platelet%20rich%20plasma&max_results_per_source=10",
            200,
            timeout=60
        )
        
        if success:
            papers = response.get('papers', [])
            source_stats = response.get('source_statistics', {})
            
            total_before_dedup = source_stats.get('pubmed', {}).get('papers_found', 0) + source_stats.get('google_scholar', {}).get('papers_found', 0)
            total_after_dedup = response.get('total_unique_papers', 0)
            
            print(f"   Papers Before Deduplication: {total_before_dedup}")
            print(f"   Papers After Deduplication: {total_after_dedup}")
            print(f"   Deduplication Effective: {total_after_dedup <= total_before_dedup}")
            
            # Check for potential duplicates (shouldn't find any)
            titles = [p.get('title', '').lower().strip() for p in papers]
            unique_titles = set(titles)
            
            print(f"   Unique Titles: {len(unique_titles)}")
            print(f"   Total Papers: {len(papers)}")
            print(f"   No Title Duplicates: {len(unique_titles) == len(papers)}")
            
            if papers:
                # Show source distribution
                sources = {}
                for paper in papers:
                    source = paper.get('source', 'unknown')
                    sources[source] = sources.get(source, 0) + 1
                
                print(f"   Source Distribution: {sources}")
        return success

    def test_literature_integration_evidence_extraction(self):
        """Test evidence extraction helper methods"""
        success, response = self.run_test(
            "Literature Integration - Evidence Extraction",
            "GET",
            "literature/search?query=osteoarthritis%20PRP&limit=5",
            200,
            timeout=30
        )
        
        if success:
            papers = response.get('papers', [])
            print(f"   Papers for Evidence Extraction: {len(papers)}")
            
            if papers:
                sample_paper = papers[0]
                
                # Check for evidence extraction fields
                evidence_fields = ['title', 'abstract', 'authors', 'journal', 'relevance_score']
                available_fields = [field for field in evidence_fields if field in sample_paper]
                
                print(f"   Evidence Fields Available: {len(available_fields)}/{len(evidence_fields)}")
                print(f"   Available Fields: {', '.join(available_fields)}")
                
                # Check for therapy implications
                abstract = sample_paper.get('abstract', '').lower()
                therapy_keywords = ['therapy', 'treatment', 'efficacy', 'outcome', 'safety', 'dosage']
                therapy_mentions = [kw for kw in therapy_keywords if kw in abstract]
                
                print(f"   Therapy Keywords Found: {len(therapy_mentions)}")
                print(f"   Keywords: {', '.join(therapy_mentions[:3])}")
                
                # Check for outcome data
                outcome_keywords = ['improvement', 'reduction', 'success', 'failure', 'adverse', 'benefit']
                outcome_mentions = [kw for kw in outcome_keywords if kw in abstract]
                
                print(f"   Outcome Keywords Found: {len(outcome_mentions)}")
                
                # Check for evidence level indicators
                evidence_keywords = ['randomized', 'controlled', 'trial', 'systematic', 'meta-analysis', 'cohort']
                evidence_mentions = [kw for kw in evidence_keywords if kw in abstract]
                
                print(f"   Evidence Level Indicators: {len(evidence_mentions)}")
        return success

    # ========== ADVANCED DIFFERENTIAL DIAGNOSIS SYSTEM TESTING ==========
    # Testing the Advanced Differential Diagnosis System after troubleshoot agent fixes

    def test_advanced_differential_diagnosis_comprehensive_differential(self):
        """Test POST /api/diagnosis/comprehensive-differential - Generate comprehensive differential diagnoses"""
        if not self.patient_id:
            print("‚ùå No patient ID available for comprehensive differential diagnosis testing")
            return False

        # Use realistic medical data as requested - osteoarthritis case
        differential_data = {
            "patient_id": self.patient_id,
            "clinical_presentation": {
                "chief_complaint": "Progressive bilateral knee pain and morning stiffness affecting daily activities",
                "symptom_duration": "3 years",
                "pain_characteristics": {
                    "intensity": 7,
                    "quality": "deep aching pain with morning stiffness lasting 45 minutes",
                    "aggravating_factors": ["prolonged standing", "stair climbing", "cold weather"],
                    "relieving_factors": ["rest", "heat application", "gentle movement"]
                },
                "functional_impact": {
                    "walking_distance": "limited to 2 blocks",
                    "stair_climbing": "requires handrail support",
                    "work_impact": "difficulty with prolonged standing during surgeries"
                }
            },
            "diagnostic_modalities": {
                "physical_examination": {
                    "inspection": "bilateral knee swelling, no erythema",
                    "palpation": "tenderness over medial joint lines bilaterally",
                    "range_of_motion": "flexion limited to 115 degrees bilaterally",
                    "special_tests": ["positive McMurray test bilateral", "negative drawer test"],
                    "gait_analysis": "antalgic gait with shortened stance phase"
                },
                "imaging": {
                    "xray_findings": "Grade 2-3 osteoarthritis with joint space narrowing, osteophyte formation, subchondral sclerosis",
                    "mri_findings": "cartilage thinning, meniscal degeneration, mild bone marrow edema, small effusions"
                },
                "laboratory": {
                    "inflammatory_markers": {"CRP": 2.1, "ESR": 18},
                    "autoimmune_markers": {"RF": "negative", "anti_CCP": "negative", "ANA": "negative"},
                    "metabolic_markers": {"uric_acid": 4.2, "vitamin_D": 32}
                }
            },
            "analysis_parameters": {
                "differential_count": 5,
                "confidence_threshold": 0.1,
                "include_rare_conditions": False,
                "regenerative_focus": True,
                "evidence_level_minimum": 2
            }
        }

        print("   Testing comprehensive differential diagnosis generation...")
        print("   This may take 60-90 seconds for AI analysis...")
        success, response = self.run_test(
            "Advanced Differential Diagnosis - Comprehensive Differential",
            "POST",
            "diagnosis/comprehensive-differential",
            200,
            data=differential_data,
            timeout=120
        )
        
        if success:
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            
            # Check for diagnosis_id in response
            diagnosis_id = response.get('diagnosis_id')
            if diagnosis_id:
                print(f"   Diagnosis ID Generated: {diagnosis_id}")
                # Store for later tests
                if not hasattr(self, 'diagnosis_id'):
                    self.diagnosis_id = diagnosis_id
            
            differential_diagnoses = response.get('differential_diagnoses', [])
            print(f"   Differential Diagnoses Generated: {len(differential_diagnoses)}")
            
            if differential_diagnoses:
                for i, diagnosis in enumerate(differential_diagnoses[:3], 1):
                    print(f"   Diagnosis {i}: {diagnosis.get('diagnosis', 'Unknown')}")
                    print(f"     Confidence: {diagnosis.get('confidence_score', 0):.2f}")
                    print(f"     ICD-10: {diagnosis.get('icd10_code', 'Unknown')}")
                    print(f"     Regenerative Suitability: {diagnosis.get('regenerative_suitability', 'Unknown')}")
            
            # Check for explainable AI analysis
            explainable_analysis = response.get('explainable_ai_analysis', {})
            if explainable_analysis:
                print(f"   Explainable AI Features: {len(explainable_analysis.get('feature_importance', []))}")
                print(f"   Transparency Score: {explainable_analysis.get('transparency_score', 0):.2f}")
            
            # Check for confidence analysis
            confidence_analysis = response.get('confidence_analysis', {})
            if confidence_analysis:
                print(f"   Confidence Intervals Generated: {len(confidence_analysis.get('confidence_intervals', []))}")
                print(f"   Overall Diagnostic Confidence: {confidence_analysis.get('overall_confidence', 0):.2f}")
            
            # Check for mechanism insights
            mechanism_insights = response.get('mechanism_insights', {})
            if mechanism_insights:
                print(f"   Mechanism Pathways Analyzed: {len(mechanism_insights.get('pathways', []))}")
                print(f"   Cellular Mechanisms: {len(mechanism_insights.get('cellular_mechanisms', []))}")
        
        return success

    def test_advanced_differential_diagnosis_engine_status(self):
        """Test GET /api/diagnosis/engine-status - Check differential diagnosis engine status"""
        success, response = self.run_test(
            "Advanced Differential Diagnosis - Engine Status",
            "GET",
            "diagnosis/engine-status",
            200,
            timeout=60
        )
        
        if success:
            print(f"   Engine Status: {response.get('status', 'Unknown')}")
            print(f"   Engine Version: {response.get('version', 'Unknown')}")
            
            capabilities = response.get('capabilities', {})
            if capabilities:
                print(f"   Multi-Modal Integration: {capabilities.get('multi_modal_integration', False)}")
                print(f"   Bayesian Analysis: {capabilities.get('bayesian_analysis', False)}")
                print(f"   Explainable AI: {capabilities.get('explainable_ai', False)}")
                print(f"   Confidence Analysis: {capabilities.get('confidence_analysis', False)}")
                print(f"   Mechanism Insights: {capabilities.get('mechanism_insights', False)}")
            
            performance_metrics = response.get('performance_metrics', {})
            if performance_metrics:
                print(f"   Diagnostic Accuracy: {performance_metrics.get('diagnostic_accuracy', 0):.1%}")
                print(f"   Average Analysis Time: {performance_metrics.get('avg_analysis_time', 0):.2f}s")
                print(f"   Total Diagnoses Processed: {performance_metrics.get('total_diagnoses', 0)}")
            
            # Check for system health indicators
            system_health = response.get('system_health', {})
            if system_health:
                print(f"   Database Connection: {system_health.get('database_status', 'Unknown')}")
                print(f"   AI Models Loaded: {system_health.get('models_loaded', 0)}")
        
        return success

    def test_advanced_differential_diagnosis_retrieval(self):
        """Test GET /api/diagnosis/{diagnosis_id} - Retrieve comprehensive diagnosis by ID"""
        # Check if we have a diagnosis_id from previous test
        if not hasattr(self, 'diagnosis_id') or not self.diagnosis_id:
            print("‚ùå No diagnosis ID available from previous test - creating one first...")
            
            # Create a diagnosis first using rotator cuff injury case
            if not self.patient_id:
                print("‚ùå No patient ID available for diagnosis creation")
                return False
                
            differential_data = {
                "patient_id": self.patient_id,
                "clinical_presentation": {
                    "chief_complaint": "Right shoulder pain and weakness with overhead activities",
                    "symptom_duration": "6 months",
                    "pain_characteristics": {
                        "intensity": 6,
                        "quality": "sharp pain with overhead motion, dull ache at rest",
                        "aggravating_factors": ["overhead reaching", "sleeping on affected side", "lifting"],
                        "relieving_factors": ["rest", "ice application", "avoiding overhead activities"]
                    }
                },
                "diagnostic_modalities": {
                    "physical_examination": {
                        "inspection": "mild deltoid atrophy",
                        "palpation": "tenderness over greater tuberosity",
                        "range_of_motion": "limited abduction to 120 degrees, painful arc 60-120 degrees",
                        "special_tests": ["positive Hawkins test", "positive empty can test", "positive drop arm test"]
                    },
                    "imaging": {
                        "mri_findings": "full-thickness rotator cuff tear involving supraspinatus tendon, mild retraction"
                    }
                },
                "analysis_parameters": {
                    "differential_count": 3,
                    "regenerative_focus": True
                }
            }

            create_success, create_response = self.run_test(
                "Setup: Create Diagnosis for Retrieval Test",
                "POST",
                "diagnosis/comprehensive-differential",
                200,
                data=differential_data,
                timeout=90
            )
            
            if not create_success:
                print("‚ùå Could not create diagnosis for retrieval testing")
                return False
            
            self.diagnosis_id = create_response.get('diagnosis_id')
            if not self.diagnosis_id:
                print("‚ùå No diagnosis ID returned for retrieval testing")
                return False

        success, response = self.run_test(
            "Advanced Differential Diagnosis - Get Diagnosis by ID",
            "GET",
            f"diagnosis/{self.diagnosis_id}",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Diagnosis ID: {response.get('diagnosis_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            print(f"   Analysis Timestamp: {response.get('analysis_timestamp', 'Unknown')}")
            
            differential_diagnoses = response.get('differential_diagnoses', [])
            print(f"   Retrieved Diagnoses: {len(differential_diagnoses)}")
            
            if differential_diagnoses:
                top_diagnosis = differential_diagnoses[0]
                print(f"   Top Diagnosis: {top_diagnosis.get('diagnosis', 'Unknown')}")
                print(f"   Confidence: {top_diagnosis.get('confidence_score', 0):.2f}")
                print(f"   ICD-10 Code: {top_diagnosis.get('icd10_code', 'Unknown')}")
            
            # Check for comprehensive analysis components
            multi_modal_analysis = response.get('multi_modal_analysis', {})
            if multi_modal_analysis:
                print(f"   Multi-Modal Data Integration: {multi_modal_analysis.get('integration_score', 0):.2f}")
            
            explainable_ai = response.get('explainable_ai_analysis', {})
            if explainable_ai:
                print(f"   Explainable AI Transparency: {explainable_ai.get('transparency_score', 0):.2f}")
            
            confidence_analysis = response.get('confidence_analysis', {})
            if confidence_analysis:
                print(f"   Diagnostic Confidence: {confidence_analysis.get('overall_confidence', 0):.2f}")
        
        return success

    def test_advanced_differential_diagnosis_chronic_pain_case(self):
        """Test Advanced Differential Diagnosis with chronic pain case"""
        if not self.patient_id:
            print("‚ùå No patient ID available for chronic pain diagnosis testing")
            return False

        # Chronic pain case as requested in review
        chronic_pain_data = {
            "patient_id": self.patient_id,
            "clinical_presentation": {
                "chief_complaint": "Chronic lower back pain radiating to left leg with numbness",
                "symptom_duration": "18 months",
                "pain_characteristics": {
                    "intensity": 8,
                    "quality": "burning pain with electric shock sensations down left leg",
                    "aggravating_factors": ["prolonged sitting", "bending forward", "coughing/sneezing"],
                    "relieving_factors": ["walking", "lying down", "extension exercises"]
                },
                "neurological_symptoms": {
                    "numbness": "left L5 dermatome distribution",
                    "weakness": "mild left foot dorsiflexion weakness",
                    "reflexes": "diminished left Achilles reflex"
                }
            },
            "diagnostic_modalities": {
                "physical_examination": {
                    "inspection": "normal lumbar lordosis, no visible deformity",
                    "palpation": "paravertebral muscle spasm L4-L5 level",
                    "range_of_motion": "flexion limited to 45 degrees due to pain",
                    "special_tests": ["positive straight leg raise at 45 degrees", "positive crossed straight leg raise"]
                },
                "imaging": {
                    "mri_findings": "L4-L5 disc herniation with left neural foraminal stenosis, mild central canal stenosis"
                },
                "laboratory": {
                    "inflammatory_markers": {"CRP": 1.2, "ESR": 12}
                }
            },
            "analysis_parameters": {
                "differential_count": 4,
                "confidence_threshold": 0.2,
                "include_rare_conditions": True,
                "regenerative_focus": True
            }
        }

        print("   Testing chronic pain differential diagnosis...")
        success, response = self.run_test(
            "Advanced Differential Diagnosis - Chronic Pain Case",
            "POST",
            "diagnosis/comprehensive-differential",
            200,
            data=chronic_pain_data,
            timeout=120
        )
        
        if success:
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            
            differential_diagnoses = response.get('differential_diagnoses', [])
            print(f"   Chronic Pain Differentials: {len(differential_diagnoses)}")
            
            if differential_diagnoses:
                for i, diagnosis in enumerate(differential_diagnoses[:3], 1):
                    print(f"   Diagnosis {i}: {diagnosis.get('diagnosis', 'Unknown')}")
                    print(f"     Confidence: {diagnosis.get('confidence_score', 0):.2f}")
                    print(f"     Regenerative Options: {len(diagnosis.get('regenerative_targets', []))}")
            
            # Check for mechanism insights specific to chronic pain
            mechanism_insights = response.get('mechanism_insights', {})
            if mechanism_insights:
                pain_pathways = mechanism_insights.get('pain_pathways', [])
                print(f"   Pain Pathways Analyzed: {len(pain_pathways)}")
                
                inflammatory_mechanisms = mechanism_insights.get('inflammatory_mechanisms', [])
                print(f"   Inflammatory Mechanisms: {len(inflammatory_mechanisms)}")
        
        return success

    # ========== CRITICAL PRIORITY FEATURES TESTING ==========
    # Testing the three newly implemented "Critical Priority" features

    def test_living_evidence_engine_protocol_evidence_mapping(self):
        """Test POST /api/evidence/protocol-evidence-mapping - Living Evidence Engine System"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for living evidence mapping testing")
            return False

        living_map_data = {
            "protocol_id": self.protocol_id,
            "condition": "osteoarthritis",
            "therapies": ["PRP", "BMAC"],
            "patient_factors": {
                "age": 58,
                "severity": "moderate",
                "previous_treatments": ["NSAIDs", "physical_therapy", "corticosteroid_injections"]
            },
            "evidence_requirements": {
                "minimum_evidence_level": 2,
                "include_real_world_data": True,
                "geographic_relevance": ["US", "EU"],
                "recency_threshold": "2020-01-01"
            }
        }

        print("   This may take 30-60 seconds for comprehensive evidence mapping...")
        success, response = self.run_test(
            "CRITICAL FEATURE: Living Evidence Engine - Generate Protocol Evidence Mapping",
            "POST",
            "evidence/protocol-evidence-mapping",
            200,
            data=living_map_data,
            timeout=90
        )
        
        if success:
            print(f"   Protocol ID: {response.get('protocol_id', 'Unknown')}")
            print(f"   Evidence Mapping Status: {response.get('status', 'Unknown')}")
            
            evidence_mapping = response.get('evidence_mapping', {})
            if evidence_mapping:
                print(f"   Total Evidence Sources: {evidence_mapping.get('total_sources', 0)}")
                print(f"   High Quality Evidence: {evidence_mapping.get('high_quality_sources', 0)}")
                print(f"   Evidence Strength Score: {evidence_mapping.get('evidence_strength_score', 0):.2f}")
            
            protocol_justification = response.get('protocol_justification', {})
            if protocol_justification:
                print(f"   Protocol Justification Score: {protocol_justification.get('justification_score', 0):.2f}")
                print(f"   Evidence-Based Recommendations: {len(protocol_justification.get('recommendations', []))}")
        return success

    def test_living_evidence_engine_living_reviews(self):
        """Test GET /api/evidence/living-reviews/{condition} - Living Evidence Engine System"""
        success, response = self.run_test(
            "CRITICAL FEATURE: Living Evidence Engine - Living Systematic Review",
            "GET",
            "evidence/living-reviews/osteoarthritis",
            200,
            timeout=60
        )
        
        if success:
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            print(f"   Condition Analyzed: {response.get('condition', 'Unknown')}")
            
            living_review = response.get('living_review', {})
            if living_review:
                print(f"   Review Quality Score: {living_review.get('review_quality_score', 0):.2f}")
                print(f"   Recent Evidence Count: {living_review.get('recent_evidence_count', 0)}")
                print(f"   Evidence Freshness: {living_review.get('evidence_freshness', 'Unknown')}")
                print(f"   Last Updated: {living_review.get('last_updated', 'Unknown')}")
            
            therapy_evidence = response.get('therapy_evidence', {})
            if therapy_evidence:
                for therapy, data in list(therapy_evidence.items())[:3]:
                    print(f"   {therapy} Evidence Quality: {data.get('evidence_quality', 0):.2f}")
        return success

    def test_living_evidence_engine_protocol_mapping_retrieval(self):
        """Test GET /api/evidence/protocol/{protocol_id}/evidence-mapping - Living Evidence Engine System"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for evidence mapping retrieval testing")
            return False

        success, response = self.run_test(
            "CRITICAL FEATURE: Living Evidence Engine - Get Protocol Evidence Mapping",
            "GET",
            f"evidence/protocol/{self.protocol_id}/evidence-mapping",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Mapping Status: {response.get('status', 'Unknown')}")
            print(f"   Protocol ID: {response.get('protocol_id', 'Unknown')}")
            
            evidence_mapping = response.get('evidence_mapping', {})
            if evidence_mapping:
                print(f"   Evidence Sources: {evidence_mapping.get('evidence_sources', 0)}")
                print(f"   Mapping Quality: {evidence_mapping.get('mapping_quality', 0):.2f}")
                print(f"   Evidence Strength: {evidence_mapping.get('evidence_strength', 'Unknown')}")
                print(f"   Last Updated: {evidence_mapping.get('last_updated', 'Unknown')}")
        return success

    def test_living_evidence_engine_alerts(self):
        """Test GET /api/evidence/alerts/{protocol_id} - Living Evidence Engine System"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for evidence alerts testing")
            return False

        success, response = self.run_test(
            "CRITICAL FEATURE: Living Evidence Engine - Evidence Change Alerts",
            "GET",
            f"evidence/alerts/{self.protocol_id}",
            200,
            timeout=60
        )
        
        if success:
            print(f"   Alert Status: {response.get('status', 'Unknown')}")
            print(f"   Protocol ID: {response.get('protocol_id', 'Unknown')}")
            
            evidence_alerts = response.get('evidence_alerts', [])
            print(f"   Evidence Alerts: {len(evidence_alerts)}")
            
            if evidence_alerts:
                for i, alert in enumerate(evidence_alerts[:3], 1):
                    print(f"   Alert {i}: {alert.get('alert_type', 'Unknown')} - {alert.get('severity', 'Unknown')}")
            
            monitoring_status = response.get('monitoring_status', {})
            if monitoring_status:
                print(f"   Monitoring Active: {monitoring_status.get('active', False)}")
                print(f"   Last Check: {monitoring_status.get('last_check', 'Unknown')}")
        return success

    def test_advanced_differential_diagnosis_comprehensive(self):
        """Test POST /api/diagnosis/comprehensive-differential - Advanced Differential Diagnosis System"""
        if not self.patient_id:
            print("‚ùå No patient ID available for advanced differential diagnosis testing")
            return False

        differential_data = {
            "patient_id": self.patient_id,
            "clinical_presentation": {
                "chief_complaint": "Bilateral knee pain and stiffness with functional limitation",
                "symptom_duration": "3 years",
                "pain_characteristics": {
                    "intensity": 6,
                    "quality": "aching with morning stiffness",
                    "aggravating_factors": ["activity", "weather changes"],
                    "relieving_factors": ["rest", "heat application"]
                }
            },
            "diagnostic_modalities": {
                "physical_examination": {
                    "inspection": "mild bilateral knee swelling",
                    "palpation": "tenderness over medial joint lines",
                    "range_of_motion": "flexion limited to 120 degrees",
                    "special_tests": ["positive McMurray test"]
                },
                "imaging": {
                    "xray": "Grade 2-3 osteoarthritis with joint space narrowing",
                    "mri": "cartilage thinning, meniscal degeneration"
                },
                "laboratory": {
                    "inflammatory_markers": {"CRP": 2.1, "ESR": 18},
                    "autoimmune_markers": {"RF": "negative", "anti_CCP": "negative"}
                }
            },
            "analysis_parameters": {
                "differential_count": 5,
                "confidence_threshold": 0.1,
                "include_rare_conditions": False,
                "regenerative_focus": True
            }
        }

        print("   This may take 45-75 seconds for comprehensive differential diagnosis analysis...")
        success, response = self.run_test(
            "CRITICAL FEATURE: Advanced Differential Diagnosis - Comprehensive Analysis",
            "POST",
            "diagnosis/comprehensive-differential",
            200,
            data=differential_data,
            timeout=120
        )
        
        if success:
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            
            differential_diagnoses = response.get('differential_diagnoses', [])
            print(f"   Differential Diagnoses Generated: {len(differential_diagnoses)}")
            
            if differential_diagnoses:
                for i, diagnosis in enumerate(differential_diagnoses[:3], 1):
                    print(f"   Diagnosis {i}: {diagnosis.get('diagnosis', 'Unknown')}")
                    print(f"     Confidence: {diagnosis.get('confidence_score', 0):.2f}")
                    print(f"     ICD-10: {diagnosis.get('icd10_code', 'Unknown')}")
                    print(f"     Regenerative Suitability: {diagnosis.get('regenerative_suitability', 'Unknown')}")
            
            diagnostic_reasoning = response.get('diagnostic_reasoning', {})
            if diagnostic_reasoning:
                print(f"   Evidence Integration Score: {diagnostic_reasoning.get('evidence_integration_score', 0):.2f}")
                print(f"   Clinical Decision Support: {diagnostic_reasoning.get('decision_support_level', 'Unknown')}")
        return success

    def test_advanced_differential_diagnosis_retrieval(self):
        """Test GET /api/diagnosis/{diagnosis_id} - Advanced Differential Diagnosis System"""
        # First create a diagnosis to get an ID
        if not self.patient_id:
            print("‚ùå No patient ID available for diagnosis retrieval testing")
            return False

        # Create a diagnosis first
        differential_data = {
            "patient_id": self.patient_id,
            "clinical_presentation": {
                "chief_complaint": "Bilateral knee pain and stiffness",
                "symptom_duration": "3 years"
            },
            "analysis_parameters": {
                "differential_count": 3,
                "regenerative_focus": True
            }
        }

        create_success, create_response = self.run_test(
            "Setup: Create Diagnosis for Retrieval",
            "POST",
            "diagnosis/comprehensive-differential",
            200,
            data=differential_data,
            timeout=90
        )
        
        if not create_success:
            print("‚ùå Could not create diagnosis for retrieval testing")
            return False
        
        diagnosis_id = create_response.get('diagnosis_id')
        if not diagnosis_id:
            print("‚ùå No diagnosis ID returned for retrieval testing")
            return False

        success, response = self.run_test(
            "CRITICAL FEATURE: Advanced Differential Diagnosis - Get Diagnosis",
            "GET",
            f"diagnosis/{diagnosis_id}",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Diagnosis ID: {response.get('diagnosis_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            
            differential_diagnoses = response.get('differential_diagnoses', [])
            print(f"   Retrieved Diagnoses: {len(differential_diagnoses)}")
            
            if differential_diagnoses:
                top_diagnosis = differential_diagnoses[0]
                print(f"   Top Diagnosis: {top_diagnosis.get('diagnosis', 'Unknown')}")
                print(f"   Confidence: {top_diagnosis.get('confidence_score', 0):.2f}")
        return success

    def test_advanced_differential_diagnosis_engine_status(self):
        """Test GET /api/diagnosis/engine-status - Advanced Differential Diagnosis System"""
        success, response = self.run_test(
            "CRITICAL FEATURE: Advanced Differential Diagnosis - Engine Status",
            "GET",
            "diagnosis/engine-status",
            200,
            timeout=60
        )
        
        if success:
            print(f"   Engine Status: {response.get('status', 'Unknown')}")
            print(f"   Engine Version: {response.get('version', 'Unknown')}")
            
            capabilities = response.get('capabilities', {})
            if capabilities:
                print(f"   Multi-Modal Integration: {capabilities.get('multi_modal_integration', False)}")
                print(f"   Bayesian Analysis: {capabilities.get('bayesian_analysis', False)}")
                print(f"   Explainable AI: {capabilities.get('explainable_ai', False)}")
                print(f"   Confidence Analysis: {capabilities.get('confidence_analysis', False)}")
            
            performance_metrics = response.get('performance_metrics', {})
            if performance_metrics:
                print(f"   Diagnostic Accuracy: {performance_metrics.get('diagnostic_accuracy', 0):.1%}")
                print(f"   Average Analysis Time: {performance_metrics.get('avg_analysis_time', 0):.2f}s")
        return success

    def test_enhanced_explainable_ai_enhanced_explanation(self):
        """Test POST /api/ai/enhanced-explanation - Enhanced Explainable AI System"""
        if not self.patient_id:
            print("‚ùå No patient ID available for enhanced AI explanation testing")
            return False

        explanation_data = {
            "patient_id": self.patient_id,
            "demographics": {
                "age": 58,
                "gender": "Female",
                "occupation": "Physician"
            },
            "medical_history": [
                "Osteoarthritis bilateral knees",
                "Hypertension controlled",
                "Previous corticosteroid injections failed"
            ],
            "symptoms": [
                "bilateral knee pain",
                "morning stiffness",
                "decreased mobility",
                "functional limitation"
            ],
            "lab_results": {
                "inflammatory_markers": {
                    "CRP": 2.1,
                    "ESR": 18
                }
            },
            "treatment_history": [
                "NSAIDs - partial relief",
                "Physical therapy - minimal improvement",
                "Corticosteroid injections - temporary relief"
            ]
        }

        print("   This may take 45-60 seconds for enhanced AI explanation generation...")
        success, response = self.run_test(
            "CRITICAL FEATURE: Enhanced Explainable AI - Generate Enhanced Explanation",
            "POST",
            "ai/enhanced-explanation",
            200,
            data=explanation_data,
            timeout=90
        )
        
        if success:
            print(f"   Explanation Status: {response.get('status', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            
            enhanced_explanation = response.get('enhanced_explanation', {})
            if enhanced_explanation:
                print(f"   Explanation ID: {enhanced_explanation.get('explanation_id', 'Unknown')}")
                print(f"   Transparency Score: {enhanced_explanation.get('transparency_score', 0):.2f}")
                print(f"   Feature Importance Count: {len(enhanced_explanation.get('feature_importance', []))}")
            
            explanation_result = response.get('explanation_result', {})
            if explanation_result:
                print(f"   Visual Components Generated: {len(explanation_result.get('visual_components', []))}")
                print(f"   Interactive Elements: {explanation_result.get('interactive_elements', 0)}")
        return success

    def test_enhanced_explainable_ai_explanation_retrieval(self):
        """Test GET /api/ai/enhanced-explanation/{explanation_id} - Enhanced Explainable AI System"""
        # First create an explanation to get an ID
        if not self.patient_id:
            print("‚ùå No patient ID available for explanation retrieval testing")
            return False

        explanation_data = {
            "patient_id": self.patient_id,
            "demographics": {"age": 58, "gender": "Female"},
            "medical_history": ["Osteoarthritis bilateral knees"]
        }

        create_success, create_response = self.run_test(
            "Setup: Create Enhanced Explanation",
            "POST",
            "ai/enhanced-explanation",
            200,
            data=explanation_data,
            timeout=60
        )
        
        if not create_success:
            print("‚ùå Could not create explanation for retrieval testing")
            return False
        
        explanation_id = create_response.get('enhanced_explanation', {}).get('explanation_id')
        if not explanation_id:
            print("‚ùå No explanation ID returned for retrieval testing")
            return False

        success, response = self.run_test(
            "CRITICAL FEATURE: Enhanced Explainable AI - Get Enhanced Explanation",
            "GET",
            f"ai/enhanced-explanation/{explanation_id}",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Explanation ID: {response.get('explanation_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Analysis Type: {response.get('analysis_type', 'Unknown')}")
            print(f"   Generated At: {response.get('generated_at', 'Unknown')}")
            
            enhanced_explanation = response.get('enhanced_explanation', {})
            if enhanced_explanation:
                print(f"   Transparency Score: {enhanced_explanation.get('transparency_score', 0):.2f}")
                print(f"   Feature Count: {len(enhanced_explanation.get('feature_importance', []))}")
        return success

    def test_enhanced_explainable_ai_visual_breakdown(self):
        """Test GET /api/ai/visual-breakdown/{explanation_id} - Enhanced Explainable AI System"""
        # First create an explanation to get an ID
        if not self.patient_id:
            print("‚ùå No patient ID available for visual breakdown testing")
            return False

        explanation_data = {
            "patient_id": self.patient_id,
            "demographics": {"age": 58, "gender": "Female"},
            "medical_history": ["Osteoarthritis bilateral knees"]
        }

        create_success, create_response = self.run_test(
            "Setup: Create Explanation for Visual Breakdown",
            "POST",
            "ai/enhanced-explanation",
            200,
            data=explanation_data,
            timeout=60
        )
        
        if not create_success:
            print("‚ùå Could not create explanation for visual breakdown testing")
            return False
        
        explanation_id = create_response.get('enhanced_explanation', {}).get('explanation_id')
        if not explanation_id:
            print("‚ùå No explanation ID returned for visual breakdown testing")
            return False

        success, response = self.run_test(
            "CRITICAL FEATURE: Enhanced Explainable AI - Get Visual Explanation",
            "GET",
            f"ai/visual-explanation/{explanation_id}",
            200,
            timeout=60
        )
        
        if success:
            print(f"   Breakdown Status: {response.get('status', 'Unknown')}")
            print(f"   Explanation ID: {response.get('explanation_id', 'Unknown')}")
            
            visual_breakdown = response.get('visual_breakdown', {})
            if visual_breakdown:
                print(f"   Visual Components: {len(visual_breakdown.get('visual_components', []))}")
                print(f"   SHAP Visualizations: {len(visual_breakdown.get('shap_visualizations', []))}")
                print(f"   LIME Visualizations: {len(visual_breakdown.get('lime_visualizations', []))}")
        return success

    def test_enhanced_explainable_ai_feature_interactions(self):
        """Test POST /api/ai/risk-assessment - Enhanced Explainable AI System"""
        if not self.patient_id:
            print("‚ùå No patient ID available for risk assessment testing")
            return False

        risk_data = {
            "patient_id": self.patient_id,
            "demographics": {
                "age": 58,
                "gender": "Female",
                "occupation": "Physician"
            },
            "medical_history": [
                "Osteoarthritis bilateral knees",
                "Hypertension controlled"
            ],
            "treatment_plan": {
                "treatment_type": "PRP",
                "injection_sites": ["bilateral knees"],
                "treatment_schedule": "single session with 6-month follow-up"
            }
        }

        success, response = self.run_test(
            "CRITICAL FEATURE: Enhanced Explainable AI - Risk Assessment",
            "POST",
            "ai/risk-assessment",
            200,
            data=risk_data,
            timeout=60
        )
        
        if success:
            print(f"   Risk Assessment Status: {response.get('status', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            
            risk_assessment = response.get('risk_assessment', {})
            if risk_assessment:
                print(f"   Assessment ID: {risk_assessment.get('assessment_id', 'Unknown')}")
                print(f"   Overall Risk Level: {risk_assessment.get('overall_risk_level', 'Unknown')}")
                print(f"   Risk Score: {risk_assessment.get('risk_score', 0):.2f}")
            
            feature_analysis = response.get('feature_analysis', {})
            if feature_analysis:
                print(f"   Risk Factors Analyzed: {len(feature_analysis.get('risk_factors', []))}")
                print(f"   Protective Factors: {len(feature_analysis.get('protective_factors', []))}")
        return success

    def test_enhanced_explainable_ai_transparency_assessment(self):
        """Test GET /api/ai/clinical-intelligence-status - Enhanced Explainable AI System"""
        success, response = self.run_test(
            "CRITICAL FEATURE: Enhanced Explainable AI - Clinical Intelligence Status",
            "GET",
            "ai/clinical-intelligence-status",
            200,
            timeout=45
        )
        
        if success:
            print(f"   System Status: {response.get('overall_status', 'Unknown')}")
            print(f"   Phase: {response.get('phase', 'Unknown')}")
            
            component_status = response.get('component_status', {})
            if component_status:
                visual_ai = component_status.get('visual_explainable_ai', {})
                print(f"   Visual Explainable AI: {visual_ai.get('status', 'Unknown')}")
                
                risk_assessment = component_status.get('personalized_risk_assessment', {})
                print(f"   Risk Assessment: {risk_assessment.get('status', 'Unknown')}")
            
            usage_statistics = response.get('usage_statistics', {})
            if usage_statistics:
                print(f"   Explanations Generated: {usage_statistics.get('visual_explanations_generated', 0)}")
                print(f"   Risk Assessments: {usage_statistics.get('risk_assessments_performed', 0)}")
        return success

    def test_prediction_model_performance(self):
        """Test ML prediction model performance metrics"""
        success, response = self.run_test(
            "ML Model Performance Metrics",
            "GET",
            "predictions/model-performance",
            200
        )
        
        if success:
            models = response.get('models', {})
            print(f"   Models Available: {len(models)}")
            print(f"   Total Predictions Made: {response.get('total_predictions', 0)}")
            
            for model_name, model_data in list(models.items())[:3]:  # Show first 3 models
                performance = model_data.get('performance', {})
                print(f"   Model '{model_name}': {model_data.get('model_type', 'unknown')} - Accuracy: {performance.get('accuracy', 0):.3f}")
        return success

    def test_treatment_outcome_prediction(self):
        """Test ML treatment outcome prediction"""
        if not self.patient_id:
            print("‚ùå No patient ID available for outcome prediction testing")
            return False

        prediction_request = {
            "patient_id": self.patient_id,
            "therapy_plan": {
                "therapy_name": "Platelet-Rich Plasma (PRP)",
                "dosage": "3-5ml",
                "delivery_method": "Intra-articular injection",
                "target_location": "bilateral knees"
            }
        }

        success, response = self.run_test(
            "ML Treatment Outcome Prediction",
            "POST",
            "predictions/treatment-outcome",
            200,
            data=prediction_request,
            timeout=45
        )
        
        if success:
            predictions = response.get('predictions', {})
            print(f"   Success Probability: {predictions.get('success_probability', 0):.2f}")
            print(f"   Expected Timeline: {predictions.get('expected_timeline', {}).get('most_likely', 'unknown')}")
            print(f"   Model Version: {response.get('model_version', 'unknown')}")
            
            risk_assessment = response.get('risk_assessment', {})
            if risk_assessment:
                print(f"   Risk Factors - Low: {len(risk_assessment.get('low_risk', []))}, Moderate: {len(risk_assessment.get('moderate_risk', []))}, High: {len(risk_assessment.get('high_risk', []))}")
        return success

    def test_dicom_analysis_simulation(self):
        """Test DICOM image analysis (simulated)"""
        if not self.patient_id:
            print("‚ùå No patient ID available for DICOM testing")
            return False

        # Simulate DICOM data (base64 encoded placeholder)
        import base64
        simulated_dicom = base64.b64encode(b"SIMULATED_DICOM_DATA_FOR_TESTING").decode()
        
        dicom_data = {
            "patient_id": self.patient_id,
            "modality": "MRI",
            "dicom_data": simulated_dicom,
            "study_description": "Bilateral knee MRI for regenerative medicine assessment"
        }

        success, response = self.run_test(
            "DICOM Image Analysis (AI-Powered)",
            "POST",
            "imaging/analyze-dicom",
            200,
            data=dicom_data,
            timeout=30
        )
        
        if success:
            print(f"   Processing ID: {response.get('processing_id', 'unknown')}")
            print(f"   Analysis Status: {response.get('status', 'unknown')}")
            print(f"   Modality Processed: {response.get('modality', 'unknown')}")
            
            findings = response.get('findings', {})
            if findings:
                print(f"   AI Findings: {len(findings)} detected")
        return success

    def test_imaging_analysis_history(self):
        """Test imaging analysis history retrieval"""
        if not self.patient_id:
            print("‚ùå No patient ID available for imaging history testing")
            return False

        success, response = self.run_test(
            "Imaging Analysis History",
            "GET",
            f"imaging/analysis-history/{self.patient_id}",
            200
        )
        
        if success:
            analyses = response.get('analyses', [])
            print(f"   Patient ID: {response.get('patient_id', 'unknown')}")
            print(f"   Total Analyses: {response.get('total_analyses', 0)}")
            print(f"   Analyses Returned: {len(analyses)}")
        return success

    # ========== CRITICAL FEATURE 1: LIVING EVIDENCE ENGINE & PROTOCOL JUSTIFICATION ==========

    def test_critical_feature_1_protocol_evidence_mapping(self):
        """Test POST /api/evidence/protocol-evidence-mapping - CRITICAL FEATURE 1"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for evidence mapping testing")
            return False

        evidence_mapping_data = {
            "protocol_id": self.protocol_id,
            "condition": "osteoarthritis",
            "therapies": ["PRP", "BMAC"],
            "patient_factors": {
                "age": 58,
                "severity": "moderate",
                "previous_treatments": ["NSAIDs", "physical_therapy"]
            },
            "evidence_requirements": {
                "minimum_evidence_level": 2,
                "include_real_world_data": True,
                "geographic_relevance": ["US", "EU"]
            }
        }

        print("   This may take 30-60 seconds for comprehensive evidence mapping...")
        success, response = self.run_test(
            "CRITICAL FEATURE 1: Protocol Evidence Mapping",
            "POST",
            "evidence/protocol-evidence-mapping",
            200,
            data=evidence_mapping_data,
            timeout=90
        )
        
        if success:
            print(f"   Protocol ID: {response.get('protocol_id', 'Unknown')}")
            print(f"   Evidence Mapping Status: {response.get('status', 'Unknown')}")
            
            evidence_summary = response.get('evidence_summary', {})
            if evidence_summary:
                print(f"   Total Evidence Sources: {evidence_summary.get('total_sources', 0)}")
                print(f"   High Quality Evidence: {evidence_summary.get('high_quality_sources', 0)}")
                print(f"   Evidence Confidence: {evidence_summary.get('overall_confidence', 0):.2f}")
            
            protocol_justification = response.get('protocol_justification', {})
            if protocol_justification:
                print(f"   Justification Score: {protocol_justification.get('justification_score', 0):.2f}")
                print(f"   Evidence-Based Recommendations: {len(protocol_justification.get('recommendations', []))}")
                
            automated_analysis = response.get('automated_analysis', {})
            if automated_analysis:
                print(f"   AI Summary Generated: {automated_analysis.get('ai_summary_available', False)}")
                print(f"   Literature Papers Analyzed: {automated_analysis.get('papers_analyzed', 0)}")
        return success

    def test_critical_feature_1_living_reviews(self):
        """Test GET /api/evidence/living-reviews/osteoarthritis - CRITICAL FEATURE 1"""
        success, response = self.run_test(
            "CRITICAL FEATURE 1: Living Evidence Reviews - Osteoarthritis",
            "GET",
            "evidence/living-reviews/osteoarthritis?therapies=PRP,BMAC&include_real_world=true&evidence_level=2",
            200,
            timeout=60
        )
        
        if success:
            print(f"   Condition: {response.get('condition', 'Unknown')}")
            print(f"   Review Status: {response.get('status', 'Unknown')}")
            
            living_review = response.get('living_review', {})
            if living_review:
                print(f"   Last Updated: {living_review.get('last_updated', 'Unknown')}")
                print(f"   Evidence Sources: {living_review.get('total_evidence_sources', 0)}")
                print(f"   Therapy Coverage: {len(living_review.get('therapies_covered', []))}")
                
            therapy_analysis = response.get('therapy_analysis', {})
            if therapy_analysis:
                prp_data = therapy_analysis.get('PRP', {})
                bmac_data = therapy_analysis.get('BMAC', {})
                
                if prp_data:
                    print(f"   PRP Evidence Quality: {prp_data.get('evidence_quality', 'Unknown')}")
                    print(f"   PRP Success Rate: {prp_data.get('pooled_success_rate', 0):.1%}")
                
                if bmac_data:
                    print(f"   BMAC Evidence Quality: {bmac_data.get('evidence_quality', 'Unknown')}")
                    print(f"   BMAC Success Rate: {bmac_data.get('pooled_success_rate', 0):.1%}")
                    
            real_world_data = response.get('real_world_evidence', {})
            if real_world_data:
                print(f"   Real-World Studies: {real_world_data.get('studies_included', 0)}")
                print(f"   Patient Population: {real_world_data.get('total_patients', 0)}")
        return success

    def test_critical_feature_1_evidence_alerts(self):
        """Test GET /api/evidence/alerts/{protocol_id} - CRITICAL FEATURE 1"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for evidence alerts testing")
            return False

        success, response = self.run_test(
            "CRITICAL FEATURE 1: Evidence Change Alerts",
            "GET",
            f"evidence/alerts/{self.protocol_id}",
            200,
            timeout=30
        )
        
        if success:
            print(f"   Protocol ID: {response.get('protocol_id', 'Unknown')}")
            print(f"   Alert Status: {response.get('status', 'Unknown')}")
            
            alerts = response.get('alerts', [])
            print(f"   Active Alerts: {len(alerts)}")
            
            if alerts:
                for i, alert in enumerate(alerts[:3], 1):  # Show first 3 alerts
                    print(f"   Alert {i}: {alert.get('alert_type', 'Unknown')} - {alert.get('severity', 'Unknown')}")
                    print(f"     Message: {alert.get('message', 'No message')[:60]}...")
                    
            evidence_changes = response.get('evidence_changes', {})
            if evidence_changes:
                print(f"   New Evidence Available: {evidence_changes.get('new_evidence_count', 0)}")
                print(f"   Updated Guidelines: {evidence_changes.get('updated_guidelines', 0)}")
                print(f"   Safety Updates: {evidence_changes.get('safety_updates', 0)}")
                
            monitoring_status = response.get('monitoring_status', {})
            if monitoring_status:
                print(f"   Monitoring Active: {monitoring_status.get('active', False)}")
                print(f"   Last Check: {monitoring_status.get('last_check', 'Unknown')}")
        return success

    def test_critical_feature_1_engine_status(self):
        """Test GET /api/evidence/engine-status - CRITICAL FEATURE 1"""
        success, response = self.run_test(
            "CRITICAL FEATURE 1: Evidence Engine Status",
            "GET",
            "evidence/engine-status",
            200,
            timeout=30
        )
        
        if success:
            print(f"   Engine Status: {response.get('status', 'Unknown')}")
            print(f"   Engine Version: {response.get('version', 'Unknown')}")
            
            capabilities = response.get('capabilities', {})
            if capabilities:
                print(f"   Living Reviews: {capabilities.get('living_reviews', False)}")
                print(f"   Protocol Mapping: {capabilities.get('protocol_evidence_mapping', False)}")
                print(f"   Real-time Alerts: {capabilities.get('evidence_alerts', False)}")
                print(f"   AI Synthesis: {capabilities.get('ai_synthesis', False)}")
                
            database_stats = response.get('database_stats', {})
            if database_stats:
                print(f"   Evidence Sources: {database_stats.get('total_evidence_sources', 0)}")
                print(f"   Active Protocols: {database_stats.get('active_protocols', 0)}")
                print(f"   Literature Papers: {database_stats.get('literature_papers', 0)}")
                
            performance_metrics = response.get('performance_metrics', {})
            if performance_metrics:
                print(f"   Average Response Time: {performance_metrics.get('avg_response_time', 0):.2f}s")
                print(f"   Success Rate: {performance_metrics.get('success_rate', 0):.1%}")
        return success

    # ========== CRITICAL FEATURE 2: ADVANCED MULTI-MODAL AI CLINICAL DECISION SUPPORT ==========

    def test_critical_feature_2_comprehensive_differential(self):
        """Test POST /api/diagnosis/comprehensive-differential - CRITICAL FEATURE 2"""
        if not self.patient_id:
            print("‚ùå No patient ID available for comprehensive differential testing")
            return False

        multi_modal_data = {
            "patient_id": self.patient_id,
            "clinical_data": {
                "chief_complaint": "Bilateral knee pain and stiffness",
                "history_present_illness": "Progressive bilateral knee pain over 3 years, worse with activity",
                "physical_exam": {
                    "inspection": "Mild bilateral knee swelling",
                    "palpation": "Tenderness over medial joint lines",
                    "range_of_motion": "Flexion limited to 120 degrees bilaterally",
                    "special_tests": "Positive McMurray test bilaterally"
                }
            },
            "laboratory_data": {
                "inflammatory_markers": {
                    "CRP": 2.1,
                    "ESR": 18,
                    "RF": "negative",
                    "anti_CCP": "negative"
                },
                "metabolic_panel": {
                    "glucose": 95,
                    "creatinine": 0.9,
                    "uric_acid": 4.2
                }
            },
            "imaging_data": {
                "xray_findings": "Grade 2-3 osteoarthritis with joint space narrowing",
                "mri_findings": "Cartilage thinning, meniscal degeneration, mild bone marrow edema"
            },
            "genetic_data": {
                "collagen_variants": "COL2A1 normal",
                "inflammatory_markers": "IL1B normal variant"
            },
            "wearable_data": {
                "activity_level": "moderately active",
                "pain_patterns": "worse in morning and after activity",
                "sleep_quality": "disrupted by pain"
            },
            "patient_reported_outcomes": {
                "pain_scale": 6,
                "functional_limitation": "moderate",
                "quality_of_life_impact": "significant"
            }
        }

        print("   This may take 60-90 seconds for comprehensive multi-modal AI analysis...")
        success, response = self.run_test(
            "CRITICAL FEATURE 2: Comprehensive Multi-Modal Differential Diagnosis",
            "POST",
            "diagnosis/comprehensive-differential",
            200,
            data=multi_modal_data,
            timeout=120
        )
        
        if success:
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            
            differential_diagnoses = response.get('differential_diagnoses', [])
            print(f"   Differential Diagnoses Generated: {len(differential_diagnoses)}")
            
            if differential_diagnoses:
                for i, diagnosis in enumerate(differential_diagnoses[:3], 1):  # Show top 3
                    print(f"   Diagnosis {i}: {diagnosis.get('diagnosis', 'Unknown')}")
                    print(f"     Confidence: {diagnosis.get('confidence_score', 0):.2f}")
                    print(f"     ICD-10: {diagnosis.get('icd10_code', 'Unknown')}")
                    print(f"     Evidence Sources: {len(diagnosis.get('supporting_evidence', []))}")
                    
            multi_modal_integration = response.get('multi_modal_integration', {})
            if multi_modal_integration:
                print(f"   Data Modalities Integrated: {multi_modal_integration.get('modalities_count', 0)}")
                print(f"   Integration Confidence: {multi_modal_integration.get('integration_confidence', 0):.2f}")
                print(f"   Key Correlations Found: {len(multi_modal_integration.get('key_correlations', []))}")
                
            ai_reasoning = response.get('ai_reasoning', {})
            if ai_reasoning:
                print(f"   Bayesian Analysis: {ai_reasoning.get('bayesian_analysis_performed', False)}")
                print(f"   Decision Tree Depth: {ai_reasoning.get('decision_tree_depth', 0)}")
                print(f"   Confidence Intervals: {ai_reasoning.get('confidence_intervals_calculated', False)}")
        return success

    def test_critical_feature_2_explainable_ai_analysis(self):
        """Test POST /api/diagnosis/explainable-ai-analysis - CRITICAL FEATURE 2"""
        if not self.patient_id:
            print("‚ùå No patient ID available for explainable AI analysis testing")
            return False

        analysis_request = {
            "patient_id": self.patient_id,
            "diagnoses": [
                {
                    "diagnosis": "M17.0 - Bilateral primary osteoarthritis of knee",
                    "confidence_score": 0.85,
                    "icd10_code": "M17.0"
                },
                {
                    "diagnosis": "M06.9 - Rheumatoid arthritis, unspecified",
                    "confidence_score": 0.15,
                    "icd10_code": "M06.9"
                }
            ],
            "explanation_type": "comprehensive",
            "include_feature_importance": True,
            "include_decision_path": True,
            "include_counterfactuals": True
        }

        print("   This may take 45-60 seconds for explainable AI analysis...")
        success, response = self.run_test(
            "CRITICAL FEATURE 2: Explainable AI Analysis",
            "POST",
            "diagnosis/explainable-ai-analysis",
            200,
            data=analysis_request,
            timeout=90
        )
        
        if success:
            print(f"   Analysis ID: {response.get('analysis_id', 'Unknown')}")
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            
            feature_importance = response.get('feature_importance', {})
            if feature_importance:
                print(f"   Feature Analysis Method: {feature_importance.get('method', 'Unknown')}")
                
                top_features = feature_importance.get('top_features', [])
                print(f"   Top Contributing Features: {len(top_features)}")
                
                for i, feature in enumerate(top_features[:5], 1):  # Show top 5
                    print(f"     Feature {i}: {feature.get('feature_name', 'Unknown')} - Impact: {feature.get('importance_score', 0):.3f}")
                    
            decision_path = response.get('decision_path', {})
            if decision_path:
                print(f"   Decision Path Nodes: {len(decision_path.get('path_nodes', []))}")
                print(f"   Critical Decision Points: {len(decision_path.get('critical_points', []))}")
                
            shap_analysis = response.get('shap_analysis', {})
            if shap_analysis:
                print(f"   SHAP Base Value: {shap_analysis.get('base_value', 0):.3f}")
                print(f"   SHAP Values Calculated: {len(shap_analysis.get('shap_values', []))}")
                print(f"   Final Prediction: {shap_analysis.get('final_prediction', 0):.3f}")
                
            lime_analysis = response.get('lime_analysis', {})
            if lime_analysis:
                print(f"   LIME Explanations: {len(lime_analysis.get('explanations', []))}")
                print(f"   Local Fidelity Score: {lime_analysis.get('fidelity_score', 0):.3f}")
        return success

    def test_critical_feature_2_confidence_analysis(self):
        """Test POST /api/diagnosis/confidence-analysis - CRITICAL FEATURE 2"""
        if not self.patient_id:
            print("‚ùå No patient ID available for confidence analysis testing")
            return False

        confidence_request = {
            "patient_id": self.patient_id,
            "diagnosis": "M17.0 - Bilateral primary osteoarthritis of knee",
            "confidence_score": 0.85,
            "analysis_type": "bayesian_credible_intervals",
            "monte_carlo_samples": 1000,
            "uncertainty_quantification": True
        }

        print("   This may take 30-45 seconds for Bayesian confidence analysis...")
        success, response = self.run_test(
            "CRITICAL FEATURE 2: Bayesian Confidence Analysis",
            "POST",
            "diagnosis/confidence-analysis",
            200,
            data=confidence_request,
            timeout=75
        )
        
        if success:
            print(f"   Analysis ID: {response.get('analysis_id', 'Unknown')}")
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            
            bayesian_analysis = response.get('bayesian_analysis', {})
            if bayesian_analysis:
                credible_interval = bayesian_analysis.get('credible_interval', {})
                if credible_interval:
                    print(f"   95% Credible Interval: [{credible_interval.get('lower', 0):.3f}, {credible_interval.get('upper', 0):.3f}]")
                    print(f"   Posterior Mean: {credible_interval.get('mean', 0):.3f}")
                    print(f"   Posterior Std: {credible_interval.get('std', 0):.3f}")
                    
                print(f"   Prior Distribution: {bayesian_analysis.get('prior_distribution', 'Unknown')}")
                print(f"   Likelihood Function: {bayesian_analysis.get('likelihood_function', 'Unknown')}")
                
            uncertainty_metrics = response.get('uncertainty_metrics', {})
            if uncertainty_metrics:
                print(f"   Epistemic Uncertainty: {uncertainty_metrics.get('epistemic_uncertainty', 0):.3f}")
                print(f"   Aleatoric Uncertainty: {uncertainty_metrics.get('aleatoric_uncertainty', 0):.3f}")
                print(f"   Total Uncertainty: {uncertainty_metrics.get('total_uncertainty', 0):.3f}")
                
            monte_carlo_results = response.get('monte_carlo_results', {})
            if monte_carlo_results:
                print(f"   MC Samples: {monte_carlo_results.get('samples_used', 0)}")
                print(f"   MC Convergence: {monte_carlo_results.get('convergence_achieved', False)}")
                print(f"   Effective Sample Size: {monte_carlo_results.get('effective_sample_size', 0)}")
        return success

    def test_critical_feature_2_mechanism_insights(self):
        """Test GET /api/diagnosis/mechanism-insights/Osteoarthritis - CRITICAL FEATURE 2"""
        success, response = self.run_test(
            "CRITICAL FEATURE 2: Cellular Mechanism Insights - Osteoarthritis",
            "GET",
            "diagnosis/mechanism-insights/Osteoarthritis?include_pathways=true&include_targets=true&detail_level=comprehensive",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Diagnosis: {response.get('diagnosis', 'Unknown')}")
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            
            cellular_mechanisms = response.get('cellular_mechanisms', {})
            if cellular_mechanisms:
                pathways = cellular_mechanisms.get('pathways', [])
                print(f"   Cellular Pathways: {len(pathways)}")
                
                for i, pathway in enumerate(pathways[:3], 1):  # Show top 3
                    print(f"     Pathway {i}: {pathway.get('pathway_name', 'Unknown')}")
                    print(f"       Key Molecules: {', '.join(pathway.get('key_molecules', [])[:3])}")
                    print(f"       Therapeutic Relevance: {pathway.get('therapeutic_relevance', 'Unknown')}")
                    
            molecular_targets = response.get('molecular_targets', {})
            if molecular_targets:
                regenerative_targets = molecular_targets.get('regenerative_targets', [])
                print(f"   Regenerative Targets: {len(regenerative_targets)}")
                
                for i, target in enumerate(regenerative_targets[:3], 1):  # Show top 3
                    print(f"     Target {i}: {target.get('target_name', 'Unknown')}")
                    print(f"       Target Type: {target.get('target_type', 'Unknown')}")
                    print(f"       Druggability Score: {target.get('druggability_score', 0):.2f}")
                    
            therapeutic_implications = response.get('therapeutic_implications', {})
            if therapeutic_implications:
                print(f"   Therapy Recommendations: {len(therapeutic_implications.get('recommended_therapies', []))}")
                print(f"   Mechanism-Based Rationale: {therapeutic_implications.get('mechanism_rationale_available', False)}")
                
                biomarkers = therapeutic_implications.get('predictive_biomarkers', [])
                print(f"   Predictive Biomarkers: {len(biomarkers)}")
        return success

    def test_critical_feature_2_engine_status(self):
        """Test GET /api/diagnosis/engine-status - CRITICAL FEATURE 2"""
        success, response = self.run_test(
            "CRITICAL FEATURE 2: Diagnostic Engine Status",
            "GET",
            "diagnosis/engine-status",
            200,
            timeout=30
        )
        
        if success:
            print(f"   Engine Status: {response.get('status', 'Unknown')}")
            print(f"   Engine Version: {response.get('version', 'Unknown')}")
            
            capabilities = response.get('capabilities', {})
            if capabilities:
                print(f"   Multi-Modal Integration: {capabilities.get('multi_modal_integration', False)}")
                print(f"   Bayesian Analysis: {capabilities.get('bayesian_analysis', False)}")
                print(f"   Explainable AI: {capabilities.get('explainable_ai', False)}")
                print(f"   Confidence Analysis: {capabilities.get('confidence_analysis', False)}")
                print(f"   Mechanism Insights: {capabilities.get('mechanism_insights', False)}")
                
            performance_metrics = response.get('performance_metrics', {})
            if performance_metrics:
                print(f"   Diagnostic Accuracy: {performance_metrics.get('diagnostic_accuracy', 0):.1%}")
                print(f"   Average Analysis Time: {performance_metrics.get('avg_analysis_time', 0):.2f}s")
                print(f"   Multi-Modal Success Rate: {performance_metrics.get('multi_modal_success_rate', 0):.1%}")
                
            model_statistics = response.get('model_statistics', {})
            if model_statistics:
                print(f"   Total Diagnoses Processed: {model_statistics.get('total_diagnoses', 0)}")
                print(f"   Unique Conditions: {model_statistics.get('unique_conditions', 0)}")
                print(f"   AI Models Active: {model_statistics.get('active_models', 0)}")
        return success

    # ========== CRITICAL FEATURE 3: ENHANCED EXPLAINABLE AI INTEGRATION ==========

    def test_critical_feature_3_integrated_shap_lime(self):
        """Test integrated SHAP/LIME analysis within differential diagnosis - CRITICAL FEATURE 3"""
        if not self.patient_id:
            print("‚ùå No patient ID available for integrated SHAP/LIME testing")
            return False

        # First generate a comprehensive differential diagnosis
        differential_request = {
            "patient_id": self.patient_id,
            "include_explainable_ai": True,
            "explanation_methods": ["SHAP", "LIME", "integrated_analysis"],
            "feature_importance_threshold": 0.05,
            "generate_visual_explanations": True
        }

        print("   This may take 60-90 seconds for integrated SHAP/LIME analysis...")
        success, response = self.run_test(
            "CRITICAL FEATURE 3: Integrated SHAP/LIME in Differential Diagnosis",
            "POST",
            "diagnosis/comprehensive-differential",
            200,
            data=differential_request,
            timeout=120
        )
        
        if success:
            print(f"   Analysis Status: {response.get('status', 'Unknown')}")
            
            explainable_ai = response.get('explainable_ai_integration', {})
            if explainable_ai:
                print(f"   SHAP Analysis Available: {explainable_ai.get('shap_analysis_available', False)}")
                print(f"   LIME Analysis Available: {explainable_ai.get('lime_analysis_available', False)}")
                print(f"   Integrated Analysis: {explainable_ai.get('integrated_analysis_available', False)}")
                
                shap_results = explainable_ai.get('shap_results', {})
                if shap_results:
                    print(f"   SHAP Base Value: {shap_results.get('base_value', 0):.3f}")
                    print(f"   SHAP Feature Count: {len(shap_results.get('feature_values', []))}")
                    print(f"   SHAP Explanation Quality: {shap_results.get('explanation_quality', 0):.2f}")
                    
                lime_results = explainable_ai.get('lime_results', {})
                if lime_results:
                    print(f"   LIME Local Fidelity: {lime_results.get('local_fidelity', 0):.3f}")
                    print(f"   LIME Feature Explanations: {len(lime_results.get('feature_explanations', []))}")
                    print(f"   LIME Stability Score: {lime_results.get('stability_score', 0):.3f}")
                    
            visual_explanations = response.get('visual_explanations', {})
            if visual_explanations:
                print(f"   Visual Breakdown URLs Generated: {len(visual_explanations.get('breakdown_urls', []))}")
                print(f"   Feature Importance Charts: {visual_explanations.get('feature_importance_chart_available', False)}")
                print(f"   Decision Path Visualization: {visual_explanations.get('decision_path_viz_available', False)}")
        return success

    def test_critical_feature_3_visual_breakdown_urls(self):
        """Test visual breakdown URLs generation - CRITICAL FEATURE 3"""
        if not self.patient_id:
            print("‚ùå No patient ID available for visual breakdown testing")
            return False

        visual_request = {
            "patient_id": self.patient_id,
            "analysis_type": "comprehensive_visual_breakdown",
            "chart_types": ["feature_importance", "shap_waterfall", "lime_explanation", "decision_tree"],
            "output_format": "interactive_html",
            "include_clinical_interpretation": True
        }

        success, response = self.run_test(
            "CRITICAL FEATURE 3: Visual Breakdown URLs Generation",
            "POST",
            "diagnosis/explainable-ai-analysis",
            200,
            data=visual_request,
            timeout=60
        )
        
        if success:
            print(f"   Visual Analysis ID: {response.get('analysis_id', 'Unknown')}")
            
            visual_outputs = response.get('visual_outputs', {})
            if visual_outputs:
                breakdown_urls = visual_outputs.get('breakdown_urls', [])
                print(f"   Visual Breakdown URLs: {len(breakdown_urls)}")
                
                for i, url_info in enumerate(breakdown_urls[:3], 1):  # Show first 3
                    print(f"     URL {i}: {url_info.get('chart_type', 'Unknown')} - {url_info.get('url', 'No URL')[:50]}...")
                    print(f"       Interactive: {url_info.get('interactive', False)}")
                    print(f"       Clinical Context: {url_info.get('clinical_context_included', False)}")
                    
                chart_metadata = visual_outputs.get('chart_metadata', {})
                if chart_metadata:
                    print(f"   Chart Generation Time: {chart_metadata.get('generation_time', 0):.2f}s")
                    print(f"   Chart Quality Score: {chart_metadata.get('quality_score', 0):.2f}")
                    
            clinical_interpretation = response.get('clinical_interpretation', {})
            if clinical_interpretation:
                print(f"   Clinical Insights Generated: {len(clinical_interpretation.get('insights', []))}")
                print(f"   Practitioner Recommendations: {len(clinical_interpretation.get('recommendations', []))}")
        return success

    def test_critical_feature_3_uncertainty_quantification(self):
        """Test uncertainty quantification and confidence intervals - CRITICAL FEATURE 3"""
        if not self.patient_id:
            print("‚ùå No patient ID available for uncertainty quantification testing")
            return False

        uncertainty_request = {
            "patient_id": self.patient_id,
            "uncertainty_methods": ["bayesian", "bootstrap", "monte_carlo", "ensemble"],
            "confidence_levels": [0.90, 0.95, 0.99],
            "quantify_model_uncertainty": True,
            "quantify_data_uncertainty": True,
            "include_prediction_intervals": True
        }

        print("   This may take 45-60 seconds for comprehensive uncertainty quantification...")
        success, response = self.run_test(
            "CRITICAL FEATURE 3: Uncertainty Quantification & Confidence Intervals",
            "POST",
            "diagnosis/confidence-analysis",
            200,
            data=uncertainty_request,
            timeout=90
        )
        
        if success:
            print(f"   Uncertainty Analysis ID: {response.get('analysis_id', 'Unknown')}")
            
            uncertainty_results = response.get('uncertainty_quantification', {})
            if uncertainty_results:
                model_uncertainty = uncertainty_results.get('model_uncertainty', {})
                if model_uncertainty:
                    print(f"   Model Uncertainty (Epistemic): {model_uncertainty.get('epistemic', 0):.4f}")
                    print(f"   Data Uncertainty (Aleatoric): {model_uncertainty.get('aleatoric', 0):.4f}")
                    print(f"   Total Uncertainty: {model_uncertainty.get('total', 0):.4f}")
                    
                confidence_intervals = uncertainty_results.get('confidence_intervals', {})
                for level, interval in confidence_intervals.items():
                    if isinstance(interval, dict):
                        print(f"   {level}% CI: [{interval.get('lower', 0):.3f}, {interval.get('upper', 0):.3f}]")
                        
                prediction_intervals = uncertainty_results.get('prediction_intervals', {})
                if prediction_intervals:
                    print(f"   Prediction Intervals Available: {len(prediction_intervals)}")
                    
            ensemble_analysis = response.get('ensemble_analysis', {})
            if ensemble_analysis:
                print(f"   Ensemble Models Used: {ensemble_analysis.get('models_count', 0)}")
                print(f"   Ensemble Agreement: {ensemble_analysis.get('agreement_score', 0):.3f}")
                print(f"   Prediction Variance: {ensemble_analysis.get('prediction_variance', 0):.4f}")
                
            bootstrap_results = response.get('bootstrap_results', {})
            if bootstrap_results:
                print(f"   Bootstrap Samples: {bootstrap_results.get('samples_used', 0)}")
                print(f"   Bootstrap CI Width: {bootstrap_results.get('ci_width', 0):.4f}")
        return success

    def test_critical_feature_3_monte_carlo_simulation(self):
        """Test Monte Carlo scenario simulation - CRITICAL FEATURE 3"""
        if not self.patient_id:
            print("‚ùå No patient ID available for Monte Carlo simulation testing")
            return False

        simulation_request = {
            "patient_id": self.patient_id,
            "simulation_type": "treatment_outcome_scenarios",
            "monte_carlo_samples": 10000,
            "scenario_parameters": {
                "treatment_variations": ["PRP_standard", "PRP_high_concentration", "BMAC", "combination_therapy"],
                "patient_factors": ["age_variation", "severity_variation", "comorbidity_presence"],
                "environmental_factors": ["seasonal_effects", "activity_level", "compliance_rates"]
            },
            "outcome_metrics": ["pain_reduction", "functional_improvement", "adverse_events", "patient_satisfaction"],
            "confidence_level": 0.95,
            "include_sensitivity_analysis": True
        }

        print("   This may take 60-90 seconds for Monte Carlo scenario simulation...")
        success, response = self.run_test(
            "CRITICAL FEATURE 3: Monte Carlo Scenario Simulation",
            "POST",
            "diagnosis/confidence-analysis",
            200,
            data=simulation_request,
            timeout=120
        )
        
        if success:
            print(f"   Simulation ID: {response.get('analysis_id', 'Unknown')}")
            
            monte_carlo_results = response.get('monte_carlo_simulation', {})
            if monte_carlo_results:
                print(f"   Samples Completed: {monte_carlo_results.get('samples_completed', 0)}")
                print(f"   Convergence Achieved: {monte_carlo_results.get('convergence_achieved', False)}")
                print(f"   Simulation Quality: {monte_carlo_results.get('simulation_quality', 0):.3f}")
                
                scenario_outcomes = monte_carlo_results.get('scenario_outcomes', {})
                for scenario, outcome in scenario_outcomes.items():
                    if isinstance(outcome, dict):
                        print(f"   {scenario}: Success Rate {outcome.get('success_rate', 0):.1%}, CI [{outcome.get('ci_lower', 0):.2f}, {outcome.get('ci_upper', 0):.2f}]")
                        
            sensitivity_analysis = response.get('sensitivity_analysis', {})
            if sensitivity_analysis:
                print(f"   Most Influential Factor: {sensitivity_analysis.get('most_influential_factor', 'Unknown')}")
                print(f"   Factor Sensitivity Score: {sensitivity_analysis.get('max_sensitivity_score', 0):.3f}")
                
                parameter_rankings = sensitivity_analysis.get('parameter_rankings', [])
                print(f"   Parameter Rankings: {len(parameter_rankings)}")
                
                for i, param in enumerate(parameter_rankings[:3], 1):  # Show top 3
                    print(f"     Rank {i}: {param.get('parameter', 'Unknown')} - Sensitivity: {param.get('sensitivity', 0):.3f}")
                    
            risk_assessment = response.get('risk_assessment', {})
            if risk_assessment:
                print(f"   Low Risk Scenarios: {risk_assessment.get('low_risk_percentage', 0):.1%}")
                print(f"   High Risk Scenarios: {risk_assessment.get('high_risk_percentage', 0):.1%}")
                print(f"   Expected Value: {risk_assessment.get('expected_value', 0):.2f}")
        return success

    # ========== EVIDENCE SYNTHESIS SYSTEM TESTING ==========

    def test_evidence_synthesis_status(self):
        """Test evidence synthesis system status"""
        success, response = self.run_test(
            "Evidence Synthesis System Status",
            "GET",
            "evidence/synthesis-status",
            200
        )
        
        if success:
            print(f"   Synthesis Engine Status: {response.get('synthesis_engine_status', 'unknown')}")
            
            literature_db = response.get('literature_database', {})
            if literature_db:
                print(f"   Literature Database Papers: {literature_db.get('total_papers', 0)}")
                print(f"   Database Status: {literature_db.get('status', 'unknown')}")
            
            print(f"   Recent Syntheses: {response.get('recent_syntheses', 0)}")
            
            capabilities = response.get('capabilities', [])
            print(f"   System Capabilities: {len(capabilities)}")
            if capabilities:
                print(f"   Key Capabilities: {', '.join(capabilities[:3])}")
        return success

    def test_evidence_synthesis_osteoarthritis(self):
        """Test evidence synthesis for osteoarthritis condition"""
        synthesis_request = {
            "condition": "osteoarthritis",
            "existing_evidence": [
                {
                    "source": "clinical_experience",
                    "finding": "PRP shows good results in knee osteoarthritis",
                    "confidence": 0.8
                }
            ]
        }

        print("   This may take 30-60 seconds for comprehensive literature analysis...")
        success, response = self.run_test(
            "Evidence Synthesis - Osteoarthritis Protocol",
            "POST",
            "evidence/synthesize-protocol",
            200,
            data=synthesis_request,
            timeout=90
        )
        
        if success:
            print(f"   Synthesis Status: {response.get('status', 'unknown')}")
            print(f"   Condition: {response.get('condition', 'unknown')}")
            
            synthesis_result = response.get('synthesis_result', {})
            if synthesis_result:
                print(f"   Evidence Sources: {synthesis_result.get('evidence_sources', 0)}")
                print(f"   Synthesis Confidence: {synthesis_result.get('synthesis_confidence', 0):.2f}")
                print(f"   Protocol Generated: {synthesis_result.get('protocol_generated', False)}")
                
                # Check for protocol recommendations
                recommendations = synthesis_result.get('protocol_recommendations', [])
                if recommendations:
                    print(f"   Protocol Recommendations: {len(recommendations)}")
                    print(f"   Top Recommendation: {recommendations[0].get('therapy', 'Unknown')[:50]}...")
        return success

    def test_evidence_synthesis_rotator_cuff(self):
        """Test evidence synthesis for rotator cuff injury condition"""
        synthesis_request = {
            "condition": "rotator cuff injury",
            "existing_evidence": []
        }

        print("   This may take 30-60 seconds for comprehensive literature analysis...")
        success, response = self.run_test(
            "Evidence Synthesis - Rotator Cuff Protocol",
            "POST",
            "evidence/synthesize-protocol",
            200,
            data=synthesis_request,
            timeout=90
        )
        
        if success:
            print(f"   Synthesis Status: {response.get('status', 'unknown')}")
            print(f"   Condition: {response.get('condition', 'unknown')}")
            
            synthesis_result = response.get('synthesis_result', {})
            if synthesis_result:
                print(f"   Evidence Sources: {synthesis_result.get('evidence_sources', 0)}")
                print(f"   Synthesis Confidence: {synthesis_result.get('synthesis_confidence', 0):.2f}")
                
                # Check for literature analysis
                literature_analysis = synthesis_result.get('literature_analysis', {})
                if literature_analysis:
                    print(f"   Literature Papers Analyzed: {literature_analysis.get('papers_analyzed', 0)}")
                    print(f"   Evidence Quality Score: {literature_analysis.get('evidence_quality_score', 0):.2f}")
        return success

    def test_evidence_synthesis_missing_condition(self):
        """Test evidence synthesis error handling when condition is missing"""
        synthesis_request = {
            "existing_evidence": []
        }

        success, response = self.run_test(
            "Evidence Synthesis - Missing Condition (Error Handling)",
            "POST",
            "evidence/synthesize-protocol",
            400,  # Expecting 400 Bad Request
            data=synthesis_request
        )
        
        if success:
            print(f"   Error Handling: Correctly returned 400 for missing condition")
            print(f"   Error Detail: {response.get('detail', 'No detail provided')}")
        return success

    def test_evidence_synthesis_invalid_condition(self):
        """Test evidence synthesis with invalid/unsupported condition"""
        synthesis_request = {
            "condition": "invalid_medical_condition_xyz123",
            "existing_evidence": []
        }

        print("   Testing with invalid condition...")
        success, response = self.run_test(
            "Evidence Synthesis - Invalid Condition",
            "POST",
            "evidence/synthesize-protocol",
            200,  # Should still return 200 but with appropriate status
            data=synthesis_request,
            timeout=60
        )
        
        if success:
            print(f"   Synthesis Status: {response.get('status', 'unknown')}")
            
            # Check if system handles invalid condition gracefully
            if response.get('status') in ['synthesis_failed', 'synthesis_completed']:
                print(f"   System handled invalid condition appropriately")
                if 'error' in response:
                    print(f"   Error Message: {response.get('error', 'No error message')[:50]}...")
                if response.get('fallback_available'):
                    print(f"   Fallback Available: {response.get('fallback_available')}")
        return success

    # ========== PHASE 3: GLOBAL KNOWLEDGE ENGINE TESTING ==========

    def test_phase3_global_knowledge_system_status(self):
        """Test Phase 3 Global Knowledge Engine system status"""
        success, response = self.run_test(
            "Phase 3: Global Knowledge Engine Status",
            "GET",
            "global-knowledge/system-status",
            200
        )
        
        if success:
            print(f"   System Status: {response.get('status', 'unknown')}")
            
            services = response.get('services', {})
            if services:
                print(f"   Global Regulatory Intelligence: {services.get('global_regulatory_intelligence', {}).get('status', 'unknown')}")
                print(f"   International Protocol Library: {services.get('international_protocol_library', {}).get('status', 'unknown')}")
                print(f"   Community Collaboration: {services.get('community_collaboration_platform', {}).get('status', 'unknown')}")
            
            database_stats = response.get('database_stats', {})
            if database_stats:
                print(f"   Regulatory Databases: {database_stats.get('regulatory_databases', 0)}")
                print(f"   International Protocols: {database_stats.get('international_protocols', 0)}")
                print(f"   Community Members: {database_stats.get('community_members', 0)}")
        return success

    def test_international_protocol_search_fix(self):
        """Test FIXED International Protocol Library - osteoarthritis search"""
        success, response = self.run_test(
            "FIXED: International Protocol Search - Osteoarthritis",
            "GET",
            "protocols/international-search?condition=osteoarthritis&max_results=10",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Condition: {response.get('condition', 'unknown')}")
            print(f"   Search Status: {response.get('status', 'unknown')}")
            
            protocols = response.get('protocols', [])
            print(f"   Protocols Found: {len(protocols)}")
            
            if protocols:
                first_protocol = protocols[0]
                print(f"   First Protocol: {first_protocol.get('protocol_name', 'Unknown')}")
                print(f"   Medical Tradition: {first_protocol.get('medical_tradition', 'Unknown')}")
                print(f"   Country: {first_protocol.get('country', 'Unknown')}")
                print(f"   Integration Level: {first_protocol.get('integration_level', 'Unknown')}")
                print(f"   Evidence Level: {first_protocol.get('evidence_level', 'Unknown')}")
            
            search_metadata = response.get('search_metadata', {})
            if search_metadata:
                print(f"   Search Timestamp: {search_metadata.get('search_timestamp', 'Unknown')}")
                print(f"   Total Available: {search_metadata.get('total_available_protocols', 0)}")
        return success

    def test_international_protocol_multiple_traditions(self):
        """Test international protocol search across multiple medical traditions"""
        success, response = self.run_test(
            "International Protocol Search - Multiple Traditions",
            "GET",
            "protocols/international-search?condition=osteoarthritis&medical_traditions=Western,TCM,Ayurvedic&max_results=15",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Condition: {response.get('condition', 'unknown')}")
            protocols = response.get('protocols', [])
            print(f"   Protocols Found: {len(protocols)}")
            
            # Check tradition diversity
            traditions = {}
            for protocol in protocols:
                tradition = protocol.get('medical_tradition', 'Unknown')
                traditions[tradition] = traditions.get(tradition, 0) + 1
            
            print(f"   Medical Traditions Represented: {len(traditions)}")
            for tradition, count in traditions.items():
                print(f"     {tradition}: {count} protocols")
            
            # Check integration levels
            integration_levels = {}
            for protocol in protocols:
                level = protocol.get('integration_level', 'Unknown')
                integration_levels[level] = integration_levels.get(level, 0) + 1
            
            print(f"   Integration Levels: {list(integration_levels.keys())}")
        return success

    def test_international_protocol_integration_filter(self):
        """Test international protocol search with integration level filtering"""
        success, response = self.run_test(
            "International Protocol Search - Integration Filter",
            "GET",
            "protocols/international-search?condition=osteoarthritis&integration_level=high&max_results=8",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Condition: {response.get('condition', 'unknown')}")
            print(f"   Integration Filter: high")
            
            protocols = response.get('protocols', [])
            print(f"   High Integration Protocols: {len(protocols)}")
            
            if protocols:
                # Verify all protocols have high integration level
                high_integration_count = sum(1 for p in protocols if p.get('integration_level') == 'high')
                print(f"   Correctly Filtered: {high_integration_count}/{len(protocols)} protocols")
                
                # Show sample protocol details
                sample = protocols[0]
                print(f"   Sample Protocol: {sample.get('protocol_name', 'Unknown')}")
                print(f"   Evidence Quality: {sample.get('evidence_quality', 'Unknown')}")
                print(f"   Clinical Validation: {sample.get('clinical_validation', 'Unknown')}")
        return success

    def test_community_peer_consultation_fix(self):
        """Test FIXED Community Collaboration Platform - peer consultation with optional case_summary"""
        consultation_data = {
            "consultation_type": "regenerative_medicine_case",
            "patient_demographics": {
                "age": 58,
                "gender": "Female",
                "condition": "Bilateral knee osteoarthritis"
            },
            "clinical_question": "Seeking advice on optimal PRP protocol for 58-year-old female physician with bilateral knee osteoarthritis. Patient has failed conservative management and wants to avoid knee replacement. What are your experiences with PRP vs BMAC for this patient profile?",
            "urgency_level": "routine",
            "expertise_sought": ["regenerative_medicine", "orthopedics", "sports_medicine"],
            "anonymized": True
        }

        success, response = self.run_test(
            "FIXED: Community Peer Consultation - Optional case_summary",
            "POST",
            "community/peer-consultation",
            200,
            data=consultation_data,
            timeout=30
        )
        
        if success:
            print(f"   Consultation ID: {response.get('consultation_id', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
            print(f"   Consultation Type: {response.get('consultation_type', 'unknown')}")
            print(f"   Urgency Level: {response.get('urgency_level', 'unknown')}")
            print(f"   Expertise Areas: {len(response.get('expertise_sought', []))}")
            
            matching_experts = response.get('matching_experts', [])
            print(f"   Matching Experts Found: {len(matching_experts)}")
            
            if matching_experts:
                print(f"   Expert Specialties: {', '.join([e.get('specialty', 'Unknown') for e in matching_experts[:3]])}")
            
            estimated_response = response.get('estimated_response_time', {})
            if estimated_response:
                print(f"   Estimated Response Time: {estimated_response.get('typical_response', 'Unknown')}")
        return success

    def test_community_peer_consultation_minimal_data(self):
        """Test peer consultation with minimal required data (testing validation fix)"""
        minimal_consultation = {
            "consultation_type": "treatment_advice",
            "clinical_question": "What is your preferred PRP preparation protocol for knee osteoarthritis?",
            "urgency_level": "routine"
        }

        success, response = self.run_test(
            "Community Peer Consultation - Minimal Data",
            "POST",
            "community/peer-consultation",
            200,
            data=minimal_consultation,
            timeout=30
        )
        
        if success:
            print(f"   Consultation ID: {response.get('consultation_id', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
            print(f"   Minimal Data Accepted: True")
            
            # Verify the system handled missing optional fields gracefully
            print(f"   Patient Demographics: {response.get('patient_demographics', 'Not provided')}")
            print(f"   Case Summary: {response.get('case_summary', 'Not provided')}")
            print(f"   Expertise Sought: {response.get('expertise_sought', 'General')}")
        return success

    def test_regulatory_treatment_status_prp(self):
        """Test regulatory status for PRP treatment"""
        success, response = self.run_test(
            "Regulatory Treatment Status - PRP",
            "GET",
            "regulatory/treatment-status/PRP",
            200
        )
        
        if success:
            print(f"   Treatment: {response.get('treatment', 'unknown')}")
            print(f"   Global Status: {response.get('global_status', 'unknown')}")
            
            country_status = response.get('country_status', {})
            if country_status:
                print(f"   Countries Covered: {len(country_status)}")
                # Show status for major countries
                for country in ['US', 'EU', 'Canada', 'Australia']:
                    if country in country_status:
                        status = country_status[country].get('status', 'unknown')
                        print(f"   {country}: {status}")
            
            regulatory_insights = response.get('regulatory_insights', {})
            if regulatory_insights:
                print(f"   Regulatory Insights Available: {len(regulatory_insights)}")
        return success

    def test_regulatory_treatment_status_bmac(self):
        """Test regulatory status for BMAC treatment"""
        success, response = self.run_test(
            "Regulatory Treatment Status - BMAC",
            "GET",
            "regulatory/treatment-status/BMAC",
            200
        )
        
        if success:
            print(f"   Treatment: {response.get('treatment', 'unknown')}")
            print(f"   Global Status: {response.get('global_status', 'unknown')}")
            
            country_status = response.get('country_status', {})
            if country_status:
                print(f"   Countries with Data: {len(country_status)}")
                
            harmonization = response.get('harmonization_assessment', {})
            if harmonization:
                print(f"   Harmonization Score: {harmonization.get('harmonization_score', 0):.2f}")
                print(f"   Regulatory Complexity: {harmonization.get('regulatory_complexity', 'unknown')}")
        return success

    def test_regulatory_treatment_status_stem_cells(self):
        """Test regulatory status for stem cell treatments"""
        success, response = self.run_test(
            "Regulatory Treatment Status - Stem Cells",
            "GET",
            "regulatory/treatment-status/stem_cells",
            200
        )
        
        if success:
            print(f"   Treatment: {response.get('treatment', 'unknown')}")
            print(f"   Global Status: {response.get('global_status', 'unknown')}")
            
            country_status = response.get('country_status', {})
            if country_status:
                # Check for restrictive countries
                restricted_count = sum(1 for country_data in country_status.values() 
                                     if country_data.get('status') in ['restricted', 'clinical_trials_only'])
                print(f"   Restricted/Clinical Trials Only: {restricted_count} countries")
            
            regulatory_insights = response.get('regulatory_insights', {})
            if regulatory_insights:
                print(f"   Key Insights: {len(regulatory_insights.get('key_insights', []))}")
        return success

    def test_regulatory_country_specific_us(self):
        """Test country-specific regulatory status for US"""
        success, response = self.run_test(
            "Regulatory Status - US Specific",
            "GET",
            "regulatory/treatment-status/PRP?country=US",
            200
        )
        
        if success:
            print(f"   Treatment: {response.get('treatment', 'unknown')}")
            print(f"   Country: {response.get('country', 'unknown')}")
            
            country_status = response.get('country_status', {})
            us_status = country_status.get('US', {})
            if us_status:
                print(f"   US Status: {us_status.get('status', 'unknown')}")
                print(f"   FDA Classification: {us_status.get('fda_classification', 'unknown')}")
                print(f"   Legal Framework: {us_status.get('legal_framework', 'unknown')}")
        return success

    def test_cross_jurisdictional_comparison(self):
        """Test cross-jurisdictional regulatory comparison"""
        comparison_data = {
            "treatments": ["PRP", "BMAC", "stem_cells"],
            "countries": ["US", "EU", "Canada", "Australia"],
            "analysis_type": "comprehensive_comparison"
        }

        success, response = self.run_test(
            "Cross-Jurisdictional Regulatory Comparison",
            "POST",
            "regulatory/cross-jurisdictional-comparison",
            200,
            data=comparison_data,
            timeout=45
        )
        
        if success:
            print(f"   Analysis Type: {response.get('analysis_type', 'unknown')}")
            print(f"   Treatments Analyzed: {len(response.get('treatments', []))}")
            print(f"   Countries Analyzed: {len(response.get('countries', []))}")
            
            comparison_matrix = response.get('comparison_matrix', {})
            if comparison_matrix:
                print(f"   Comparison Matrix Available: Yes")
                
            harmonization = response.get('harmonization_assessment', {})
            if harmonization:
                print(f"   Overall Harmonization Score: {harmonization.get('overall_score', 0):.2f}")
                print(f"   Most Harmonized Treatment: {harmonization.get('most_harmonized_treatment', 'unknown')}")
                print(f"   Least Harmonized Treatment: {harmonization.get('least_harmonized_treatment', 'unknown')}")
            
            regulatory_insights = response.get('regulatory_insights', [])
            print(f"   Regulatory Insights Generated: {len(regulatory_insights)}")
        return success

    def test_international_protocol_search_osteoarthritis(self):
        """Test international protocol search for osteoarthritis"""
        success, response = self.run_test(
            "International Protocol Search - Osteoarthritis",
            "GET",
            "protocols/international-search?condition=osteoarthritis&max_results=10",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Condition: {response.get('condition', 'unknown')}")
            print(f"   Search Type: {response.get('search_type', 'unknown')}")
            
            protocols = response.get('protocols', [])
            print(f"   Protocols Found: {len(protocols)}")
            
            if protocols:
                # Check medical traditions represented
                traditions = set()
                for protocol in protocols:
                    tradition = protocol.get('medical_tradition', 'unknown')
                    traditions.add(tradition)
                
                print(f"   Medical Traditions: {', '.join(traditions)}")
                
                # Show sample protocol
                sample_protocol = protocols[0]
                print(f"   Sample Protocol: {sample_protocol.get('protocol_name', 'Unknown')[:50]}...")
                print(f"   Tradition: {sample_protocol.get('medical_tradition', 'Unknown')}")
                print(f"   Country: {sample_protocol.get('country_origin', 'Unknown')}")
                print(f"   Integration Level: {sample_protocol.get('integration_compatibility', {}).get('level', 'unknown')}")
            
            cross_tradition = response.get('cross_tradition_analysis', {})
            if cross_tradition:
                print(f"   Cross-Tradition Recommendations: {len(cross_tradition.get('recommendations', []))}")
        return success

    def test_international_protocol_search_multiple_traditions(self):
        """Test international protocol search across multiple medical traditions"""
        success, response = self.run_test(
            "International Protocol Search - Multiple Traditions",
            "GET",
            "protocols/international-search?condition=rotator_cuff_injury&traditions=Western,TCM,Ayurvedic&max_results=15",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Condition: {response.get('condition', 'unknown')}")
            
            protocols = response.get('protocols', [])
            print(f"   Total Protocols: {len(protocols)}")
            
            # Analyze tradition distribution
            tradition_count = {}
            for protocol in protocols:
                tradition = protocol.get('medical_tradition', 'unknown')
                tradition_count[tradition] = tradition_count.get(tradition, 0) + 1
            
            print(f"   Tradition Distribution:")
            for tradition, count in tradition_count.items():
                print(f"     {tradition}: {count} protocols")
            
            integration_analysis = response.get('integration_compatibility_assessment', {})
            if integration_analysis:
                print(f"   Integration Assessment Available: Yes")
                print(f"   Recommended Integration Level: {integration_analysis.get('recommended_level', 'unknown')}")
            
            cross_tradition = response.get('cross_tradition_analysis', {})
            if cross_tradition:
                synergies = cross_tradition.get('potential_synergies', [])
                print(f"   Potential Synergies Identified: {len(synergies)}")
        return success

    def test_international_protocol_integration_levels(self):
        """Test international protocol integration compatibility levels"""
        success, response = self.run_test(
            "International Protocol Integration Levels",
            "GET",
            "protocols/international-search?condition=chronic_pain&integration_level=comprehensive&max_results=8",
            200,
            timeout=45
        )
        
        if success:
            protocols = response.get('protocols', [])
            print(f"   Protocols with Comprehensive Integration: {len(protocols)}")
            
            # Check integration levels
            integration_levels = {}
            for protocol in protocols:
                level = protocol.get('integration_compatibility', {}).get('level', 'unknown')
                integration_levels[level] = integration_levels.get(level, 0) + 1
            
            print(f"   Integration Level Distribution:")
            for level, count in integration_levels.items():
                print(f"     {level}: {count} protocols")
            
            # Check for comprehensive integration protocols
            comprehensive_protocols = [p for p in protocols 
                                     if p.get('integration_compatibility', {}).get('level') == 'comprehensive']
            
            if comprehensive_protocols:
                sample = comprehensive_protocols[0]
                compatibility = sample.get('integration_compatibility', {})
                print(f"   Sample Comprehensive Protocol:")
                print(f"     Name: {sample.get('protocol_name', 'Unknown')[:40]}...")
                print(f"     Compatibility Score: {compatibility.get('compatibility_score', 0):.2f}")
                print(f"     Integration Barriers: {len(compatibility.get('integration_barriers', []))}")
        return success

    def test_peer_consultation_emergency(self):
        """Test peer consultation request with emergency urgency"""
        consultation_data = {
            "case_description": "58-year-old physician with bilateral knee osteoarthritis. Failed conservative management. Considering PRP vs BMAC vs stem cell therapy. Patient has diabetes and hypertension. Seeking expert opinion on optimal regenerative approach.",
            "condition": "osteoarthritis",
            "urgency_level": "emergency",
            "expertise_needed": ["regenerative_medicine", "orthopedics", "diabetes_management"],
            "patient_demographics": {
                "age": 58,
                "gender": "female",
                "comorbidities": ["diabetes", "hypertension"]
            },
            "specific_questions": [
                "Which regenerative therapy is most appropriate given diabetes?",
                "How does hypertension affect treatment outcomes?",
                "What are the contraindications for each therapy?"
            ]
        }

        success, response = self.run_test(
            "Peer Consultation Request - Emergency",
            "POST",
            "community/peer-consultation",
            200,
            data=consultation_data,
            timeout=30
        )
        
        if success:
            print(f"   Consultation ID: {response.get('consultation_id', 'unknown')}")
            print(f"   Urgency Level: {response.get('urgency_level', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
            
            expert_matching = response.get('expert_matching', {})
            if expert_matching:
                print(f"   Experts Matched: {expert_matching.get('experts_matched', 0)}")
                print(f"   Expected Response Time: {expert_matching.get('expected_response_time', 'unknown')}")
                print(f"   Matching Score: {expert_matching.get('matching_score', 0):.2f}")
            
            consultation_workflow = response.get('consultation_workflow', {})
            if consultation_workflow:
                print(f"   Next Steps: {len(consultation_workflow.get('next_steps', []))}")
                
            # Store consultation ID for potential follow-up tests
            if response.get('consultation_id'):
                self.consultation_id = response.get('consultation_id')
        return success

    def test_peer_consultation_routine(self):
        """Test peer consultation request with routine urgency"""
        consultation_data = {
            "case_description": "45-year-old athlete with rotator cuff injury. Exploring regenerative options before considering surgery. Looking for protocol recommendations and outcome expectations.",
            "condition": "rotator_cuff_injury",
            "urgency_level": "routine",
            "expertise_needed": ["sports_medicine", "regenerative_medicine"],
            "patient_demographics": {
                "age": 45,
                "gender": "male",
                "activity_level": "high"
            },
            "specific_questions": [
                "Best regenerative protocol for athletes?",
                "Expected recovery timeline?",
                "Return to sport considerations?"
            ]
        }

        success, response = self.run_test(
            "Peer Consultation Request - Routine",
            "POST",
            "community/peer-consultation",
            200,
            data=consultation_data,
            timeout=30
        )
        
        if success:
            print(f"   Consultation ID: {response.get('consultation_id', 'unknown')}")
            print(f"   Urgency Level: {response.get('urgency_level', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
            
            expert_matching = response.get('expert_matching', {})
            if expert_matching:
                print(f"   Experts Matched: {expert_matching.get('experts_matched', 0)}")
                print(f"   Expected Response Time: {expert_matching.get('expected_response_time', 'unknown')}")
            
            community_insights = response.get('community_insights', {})
            if community_insights:
                print(f"   Similar Cases Found: {community_insights.get('similar_cases_found', 0)}")
                print(f"   Community Recommendations: {len(community_insights.get('community_recommendations', []))}")
        return success

    def test_share_protocol_public(self):
        """Test sharing a protocol with the community (public level)"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for sharing testing")
            return False

        sharing_data = {
            "protocol_id": self.protocol_id,
            "sharing_level": "public",
            "anonymization_level": "full",
            "sharing_purpose": "knowledge_sharing",
            "protocol_summary": {
                "condition": "osteoarthritis",
                "therapy_type": "PRP",
                "outcome_summary": "Significant improvement in pain and function",
                "key_learnings": [
                    "PRP effective for moderate osteoarthritis",
                    "Patient selection important for outcomes",
                    "Combination with PT enhances results"
                ]
            },
            "consent_obtained": True
        }

        success, response = self.run_test(
            "Share Protocol - Public Level",
            "POST",
            "community/share-protocol",
            200,
            data=sharing_data,
            timeout=30
        )
        
        if success:
            print(f"   Sharing ID: {response.get('sharing_id', 'unknown')}")
            print(f"   Sharing Level: {response.get('sharing_level', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
            
            anonymization = response.get('anonymization_applied', {})
            if anonymization:
                print(f"   Anonymization Level: {anonymization.get('level', 'unknown')}")
                print(f"   Data Protection: {anonymization.get('data_protection_applied', False)}")
            
            community_impact = response.get('community_impact', {})
            if community_impact:
                print(f"   Expected Reach: {community_impact.get('expected_reach', 'unknown')}")
                print(f"   Knowledge Contribution Score: {community_impact.get('knowledge_contribution_score', 0):.2f}")
        return success

    def test_share_protocol_professional(self):
        """Test sharing a protocol with professional network"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for professional sharing testing")
            return False

        sharing_data = {
            "protocol_id": self.protocol_id,
            "sharing_level": "professional_network",
            "anonymization_level": "partial",
            "sharing_purpose": "peer_review",
            "target_specialties": ["regenerative_medicine", "orthopedics", "sports_medicine"],
            "protocol_summary": {
                "condition": "bilateral_knee_osteoarthritis",
                "therapy_type": "AI_optimized_protocol",
                "outcome_summary": "Excellent functional improvement with minimal adverse events",
                "innovation_aspects": [
                    "AI-guided therapy selection",
                    "Personalized dosing protocol",
                    "Multi-modal outcome tracking"
                ]
            },
            "consent_obtained": True
        }

        success, response = self.run_test(
            "Share Protocol - Professional Network",
            "POST",
            "community/share-protocol",
            200,
            data=sharing_data,
            timeout=30
        )
        
        if success:
            print(f"   Sharing ID: {response.get('sharing_id', 'unknown')}")
            print(f"   Sharing Level: {response.get('sharing_level', 'unknown')}")
            print(f"   Target Specialties: {len(response.get('target_specialties', []))}")
            
            peer_review = response.get('peer_review_workflow', {})
            if peer_review:
                print(f"   Peer Review Initiated: {peer_review.get('initiated', False)}")
                print(f"   Expected Reviewers: {peer_review.get('expected_reviewers', 0)}")
            
            professional_impact = response.get('professional_impact', {})
            if professional_impact:
                print(f"   Professional Network Reach: {professional_impact.get('network_reach', 0)}")
        return success

    def test_community_insights_collective_intelligence(self):
        """Test community insights and collective intelligence"""
        success, response = self.run_test(
            "Community Insights - Collective Intelligence",
            "GET",
            "community/insights?condition=osteoarthritis&therapy_type=regenerative&time_period=6_months",
            200,
            timeout=30
        )
        
        if success:
            print(f"   Condition: {response.get('condition', 'unknown')}")
            print(f"   Therapy Type: {response.get('therapy_type', 'unknown')}")
            print(f"   Time Period: {response.get('time_period', 'unknown')}")
            
            collective_insights = response.get('collective_insights', {})
            if collective_insights:
                print(f"   Total Cases Analyzed: {collective_insights.get('total_cases_analyzed', 0)}")
                print(f"   Success Rate: {collective_insights.get('overall_success_rate', 0):.1%}")
                print(f"   Average Improvement: {collective_insights.get('average_improvement', 0):.1f}%")
            
            trending_protocols = response.get('trending_protocols', [])
            print(f"   Trending Protocols: {len(trending_protocols)}")
            
            if trending_protocols:
                top_protocol = trending_protocols[0]
                print(f"   Top Trending: {top_protocol.get('protocol_name', 'Unknown')}")
                print(f"   Usage Growth: {top_protocol.get('usage_growth', 0):.1%}")
            
            expert_consensus = response.get('expert_consensus', {})
            if expert_consensus:
                print(f"   Expert Consensus Available: Yes")
                print(f"   Consensus Strength: {expert_consensus.get('consensus_strength', 0):.2f}")
            
            emerging_trends = response.get('emerging_trends', [])
            print(f"   Emerging Trends Identified: {len(emerging_trends)}")
        return success

    def test_community_insights_therapy_comparison(self):
        """Test community insights for therapy comparison"""
        success, response = self.run_test(
            "Community Insights - Therapy Comparison",
            "GET",
            "community/insights?analysis_type=therapy_comparison&therapies=PRP,BMAC,stem_cells&condition=joint_disorders",
            200,
            timeout=30
        )
        
        if success:
            print(f"   Analysis Type: {response.get('analysis_type', 'unknown')}")
            
            therapy_comparison = response.get('therapy_comparison', {})
            if therapy_comparison:
                print(f"   Therapies Compared: {len(therapy_comparison.get('therapies', []))}")
                
                # Show comparison results
                for therapy_name, therapy_data in list(therapy_comparison.get('therapies', {}).items())[:3]:
                    success_rate = therapy_data.get('success_rate', 0)
                    case_count = therapy_data.get('case_count', 0)
                    print(f"   {therapy_name}: {success_rate:.1%} success rate ({case_count} cases)")
            
            community_preferences = response.get('community_preferences', {})
            if community_preferences:
                print(f"   Most Preferred Therapy: {community_preferences.get('most_preferred', 'unknown')}")
                print(f"   Preference Score: {community_preferences.get('preference_score', 0):.2f}")
            
            real_world_evidence = response.get('real_world_evidence', {})
            if real_world_evidence:
                print(f"   Real-World Evidence Quality: {real_world_evidence.get('evidence_quality', 'unknown')}")
                print(f"   Data Points: {real_world_evidence.get('total_data_points', 0)}")
        return success

    # ========== PHASE 2: AI CLINICAL INTELLIGENCE TESTING ==========

    def test_phase2_clinical_intelligence_status(self):
        """Test Phase 2 AI Clinical Intelligence system status"""
        success, response = self.run_test(
            "Phase 2: AI Clinical Intelligence Status",
            "GET",
            "ai/clinical-intelligence-status",
            200
        )
        
        if success:
            print(f"   System Status: {response.get('status', 'unknown')}")
            
            components = response.get('components', {})
            if components:
                print(f"   Visual Explainable AI: {components.get('visual_explainable_ai', {}).get('status', 'unknown')}")
                print(f"   Comparative Analytics: {components.get('comparative_effectiveness_analytics', {}).get('status', 'unknown')}")
                print(f"   Risk Assessment: {components.get('personalized_risk_assessment', {}).get('status', 'unknown')}")
            
            usage_stats = response.get('usage_statistics', {})
            if usage_stats:
                print(f"   Explanations Generated: {usage_stats.get('explanations_generated', 0)}")
                print(f"   Comparisons Performed: {usage_stats.get('comparisons_performed', 0)}")
                print(f"   Risk Assessments: {usage_stats.get('risk_assessments_performed', 0)}")
        return success

    def test_visual_explainable_ai_generation(self):
        """Test Visual Explainable AI with SHAP/LIME explanation generation"""
        if not self.patient_id:
            print("‚ùå No patient ID available for visual explanation testing")
            return False

        # Create realistic regenerative medicine patient data for explanation
        patient_data = {
            "patient_id": self.patient_id,
            "demographics": {
                "age": 58,
                "gender": "Female",
                "occupation": "Physician"
            },
            "medical_history": [
                "Osteoarthritis bilateral knees",
                "Hypertension controlled",
                "Previous corticosteroid injections failed"
            ],
            "symptoms": [
                "bilateral knee pain",
                "morning stiffness",
                "decreased mobility",
                "functional limitation"
            ],
            "lab_results": {
                "inflammatory_markers": {
                    "CRP": 2.1,
                    "ESR": 18
                }
            },
            "treatment_history": [
                "NSAIDs - partial relief",
                "Physical therapy - minimal improvement",
                "Corticosteroid injections - temporary relief"
            ]
        }

        print("   This may take 30-60 seconds for SHAP/LIME analysis...")
        success, response = self.run_test(
            "Visual Explainable AI - SHAP/LIME Generation",
            "POST",
            "ai/visual-explanation",
            200,
            data=patient_data,
            timeout=90
        )
        
        if success:
            print(f"   Explanation ID: {response.get('explanation_id', 'Unknown')}")
            print(f"   Analysis Type: {response.get('analysis_type', 'Unknown')}")
            
            feature_importance = response.get('feature_importance', {})
            if feature_importance:
                print(f"   Feature Importance Factors: {len(feature_importance)}")
                # Show top 3 features
                sorted_features = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)
                for i, (feature, importance) in enumerate(sorted_features[:3]):
                    print(f"   Top {i+1} Feature: {feature} (importance: {importance:.3f})")
            
            shap_analysis = response.get('shap_analysis', {})
            if shap_analysis:
                print(f"   SHAP Base Value: {shap_analysis.get('base_value', 0):.2f}")
                print(f"   SHAP Final Prediction: {shap_analysis.get('final_prediction', 0):.2f}")
            
            transparency_score = response.get('transparency_score', 0)
            print(f"   Transparency Score: {transparency_score:.2f}")
            
            # Store explanation ID for retrieval test
            if response.get('explanation_id'):
                self.explanation_id = response.get('explanation_id')
        return success

    def test_visual_explanation_retrieval(self):
        """Test GET Visual Explanation retrieval"""
        if not hasattr(self, 'explanation_id') or not self.explanation_id:
            print("‚ùå No explanation ID available for retrieval testing")
            return False

        success, response = self.run_test(
            "GET Visual Explanation Retrieval",
            "GET",
            f"ai/visual-explanation/{self.explanation_id}",
            200
        )
        
        if success:
            print(f"   Retrieved Explanation ID: {response.get('explanation_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Analysis Type: {response.get('analysis_type', 'Unknown')}")
            print(f"   Generated At: {response.get('generated_at', 'Unknown')}")
            
            feature_importance = response.get('feature_importance', {})
            print(f"   Feature Importance Factors: {len(feature_importance)}")
            
            explanation_confidence = response.get('explanation_confidence', 0)
            print(f"   Explanation Confidence: {explanation_confidence:.2f}")
        return success

    def test_comparative_effectiveness_analytics(self):
        """Test Comparative Effectiveness Analytics with multiple treatments"""
        # Test data with multiple regenerative medicine treatments
        comparison_data = {
            "treatments": ["PRP", "BMAC", "stem_cells"],
            "condition": "osteoarthritis",
            "patient_demographics": {
                "age_range": "50-65",
                "gender": "mixed",
                "severity": "moderate"
            },
            "outcome_measures": [
                "pain_reduction",
                "functional_improvement",
                "patient_satisfaction",
                "adverse_events"
            ],
            "analysis_type": "head_to_head_comparison"
        }

        print("   This may take 30-60 seconds for comparative analysis...")
        success, response = self.run_test(
            "Comparative Effectiveness Analytics - Treatment Comparison",
            "POST",
            "analytics/treatment-comparison",
            200,
            data=comparison_data,
            timeout=90
        )
        
        if success:
            print(f"   Comparison ID: {response.get('comparison_id', 'Unknown')}")
            print(f"   Analysis Type: {response.get('analysis_type', 'Unknown')}")
            
            head_to_head = response.get('head_to_head_analysis', {})
            if head_to_head:
                print(f"   Head-to-Head Comparisons: {len(head_to_head)}")
                
            treatment_ranking = response.get('treatment_ranking', [])
            if treatment_ranking:
                print(f"   Treatment Rankings: {len(treatment_ranking)}")
                for i, treatment in enumerate(treatment_ranking[:3]):
                    rank = i + 1
                    name = treatment.get('treatment', 'Unknown')
                    score = treatment.get('overall_score', 0)
                    print(f"   Rank {rank}: {name} (score: {score:.2f})")
            
            cost_effectiveness = response.get('cost_effectiveness', {})
            if cost_effectiveness:
                print(f"   Cost-Effectiveness Analysis: Available")
                print(f"   Most Cost-Effective: {cost_effectiveness.get('most_cost_effective', 'Unknown')}")
            
            network_meta = response.get('network_meta_analysis', {})
            if network_meta:
                print(f"   Network Meta-Analysis: {network_meta.get('status', 'Unknown')}")
            
            # Store comparison ID for retrieval test
            if response.get('comparison_id'):
                self.comparison_id = response.get('comparison_id')
        return success

    def test_treatment_comparison_retrieval(self):
        """Test GET Treatment Comparison retrieval"""
        if not hasattr(self, 'comparison_id') or not self.comparison_id:
            print("‚ùå No comparison ID available for retrieval testing")
            return False

        success, response = self.run_test(
            "GET Treatment Comparison Retrieval",
            "GET",
            f"analytics/treatment-comparison/{self.comparison_id}",
            200
        )
        
        if success:
            print(f"   Retrieved Comparison ID: {response.get('comparison_id', 'Unknown')}")
            print(f"   Treatments Compared: {len(response.get('treatments', []))}")
            print(f"   Analysis Completed: {response.get('analysis_completed_at', 'Unknown')}")
            
            treatment_ranking = response.get('treatment_ranking', [])
            print(f"   Treatment Rankings Available: {len(treatment_ranking)}")
            
            statistical_significance = response.get('statistical_significance', {})
            if statistical_significance:
                print(f"   Statistical Significance: Available")
        return success

    def test_treatment_effectiveness_data(self):
        """Test treatment effectiveness data endpoint"""
        success, response = self.run_test(
            "Treatment Effectiveness Data",
            "GET",
            "analytics/treatment-effectiveness?condition=osteoarthritis&treatment=PRP",
            200
        )
        
        if success:
            print(f"   Condition: {response.get('condition', 'Unknown')}")
            print(f"   Treatment: {response.get('treatment', 'Unknown')}")
            
            effectiveness_data = response.get('effectiveness_data', {})
            if effectiveness_data:
                print(f"   Success Rate: {effectiveness_data.get('success_rate', 0):.1%}")
                print(f"   Pain Reduction: {effectiveness_data.get('pain_reduction_avg', 0):.1f}%")
                print(f"   Functional Improvement: {effectiveness_data.get('functional_improvement_avg', 0):.1f}%")
                print(f"   Patient Satisfaction: {effectiveness_data.get('patient_satisfaction_avg', 0):.1f}/10")
            
            sample_size = response.get('sample_size', 0)
            print(f"   Sample Size: {sample_size} patients")
            
            evidence_level = response.get('evidence_level', 'Unknown')
            print(f"   Evidence Level: {evidence_level}")
        return success

    def test_personalized_risk_assessment(self):
        """Test Personalized Risk Assessment system"""
        if not self.patient_id:
            print("‚ùå No patient ID available for risk assessment testing")
            return False

        # Comprehensive patient data for risk assessment
        risk_assessment_data = {
            "patient_id": self.patient_id,
            "demographics": {
                "age": 58,
                "gender": "Female",
                "bmi": 28.5,
                "smoking_status": "never"
            },
            "medical_history": [
                "Osteoarthritis bilateral knees",
                "Hypertension controlled",
                "Type 2 diabetes controlled"
            ],
            "current_medications": [
                "Lisinopril 10mg daily",
                "Metformin 1000mg twice daily",
                "Ibuprofen PRN"
            ],
            "lab_results": {
                "HbA1c": 6.8,
                "CRP": 2.1,
                "ESR": 18,
                "platelet_count": 285000
            },
            "proposed_treatment": {
                "therapy": "PRP",
                "injection_site": "bilateral knees",
                "planned_sessions": 3
            },
            "risk_factors": [
                "diabetes",
                "elevated_BMI",
                "chronic_NSAID_use"
            ]
        }

        print("   This may take 30-60 seconds for comprehensive risk analysis...")
        success, response = self.run_test(
            "Personalized Risk Assessment",
            "POST",
            "ai/risk-assessment",
            200,
            data=risk_assessment_data,
            timeout=90
        )
        
        if success:
            print(f"   Assessment ID: {response.get('assessment_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            
            risk_stratification = response.get('risk_stratification', {})
            if risk_stratification:
                print(f"   Overall Risk Level: {risk_stratification.get('overall_risk_level', 'Unknown')}")
                print(f"   Treatment Success Probability: {risk_stratification.get('treatment_success_probability', 0):.1%}")
                print(f"   Adverse Event Risk: {risk_stratification.get('adverse_event_risk', 0):.1%}")
            
            risk_factors = response.get('risk_factors', {})
            if risk_factors:
                high_risk = risk_factors.get('high_risk', [])
                moderate_risk = risk_factors.get('moderate_risk', [])
                low_risk = risk_factors.get('low_risk', [])
                print(f"   High Risk Factors: {len(high_risk)}")
                print(f"   Moderate Risk Factors: {len(moderate_risk)}")
                print(f"   Low Risk Factors: {len(low_risk)}")
            
            monitoring_plan = response.get('monitoring_plan', {})
            if monitoring_plan:
                print(f"   Monitoring Plan: Available")
                recommendations = monitoring_plan.get('recommendations', [])
                print(f"   Monitoring Recommendations: {len(recommendations)}")
            
            risk_benefit_ratio = response.get('risk_benefit_ratio', {})
            if risk_benefit_ratio:
                ratio = risk_benefit_ratio.get('ratio', 0)
                print(f"   Risk-Benefit Ratio: {ratio:.2f}")
                recommendation = risk_benefit_ratio.get('recommendation', 'Unknown')
                print(f"   Clinical Recommendation: {recommendation}")
            
            # Store assessment ID for retrieval test
            if response.get('assessment_id'):
                self.assessment_id = response.get('assessment_id')
        return success

    def test_risk_assessment_retrieval(self):
        """Test GET Risk Assessment retrieval"""
        if not hasattr(self, 'assessment_id') or not self.assessment_id:
            print("‚ùå No assessment ID available for retrieval testing")
            return False

        success, response = self.run_test(
            "GET Risk Assessment Retrieval",
            "GET",
            f"ai/risk-assessment/{self.assessment_id}",
            200
        )
        
        if success:
            print(f"   Retrieved Assessment ID: {response.get('assessment_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Assessment Date: {response.get('assessment_date', 'Unknown')}")
            
            risk_stratification = response.get('risk_stratification', {})
            if risk_stratification:
                print(f"   Overall Risk Level: {risk_stratification.get('overall_risk_level', 'Unknown')}")
                print(f"   Success Probability: {risk_stratification.get('treatment_success_probability', 0):.1%}")
            
            comprehensive_analysis = response.get('comprehensive_analysis', {})
            if comprehensive_analysis:
                print(f"   Comprehensive Analysis: Available")
        return success

    def test_patient_cohort_risk_stratification(self):
        """Test Patient Cohort Risk Stratification with proper request structure"""
        # Use proper JSON structure as mentioned in the review request
        stratification_data = {
            "patient_cohort": [
                {
                    "patient_id": "patient_001",
                    "age": 58,
                    "gender": "Female",
                    "condition": "osteoarthritis",
                    "severity": "moderate",
                    "comorbidities": ["diabetes", "hypertension"]
                },
                {
                    "patient_id": "patient_002", 
                    "age": 62,
                    "gender": "Male",
                    "condition": "osteoarthritis",
                    "severity": "severe",
                    "comorbidities": ["obesity"]
                },
                {
                    "patient_id": "patient_003",
                    "age": 45,
                    "gender": "Female", 
                    "condition": "osteoarthritis",
                    "severity": "mild",
                    "comorbidities": []
                }
            ],
            "treatment_type": "PRP"
        }

        print("   This may take 30-60 seconds for cohort risk stratification...")
        success, response = self.run_test(
            "Patient Cohort Risk Stratification",
            "POST",
            "ai/risk-stratification",
            200,
            data=stratification_data,
            timeout=90
        )
        
        if success:
            print(f"   Stratification ID: {response.get('stratification_id', 'Unknown')}")
            print(f"   Cohort Size: {response.get('cohort_size', 0)}")
            print(f"   Treatment Type: {response.get('treatment_type', 'Unknown')}")
            
            risk_groups = response.get('risk_groups', {})
            if risk_groups:
                high_risk = risk_groups.get('high_risk', [])
                moderate_risk = risk_groups.get('moderate_risk', [])
                low_risk = risk_groups.get('low_risk', [])
                print(f"   High Risk Patients: {len(high_risk)}")
                print(f"   Moderate Risk Patients: {len(moderate_risk)}")
                print(f"   Low Risk Patients: {len(low_risk)}")
            
            cohort_statistics = response.get('cohort_statistics', {})
            if cohort_statistics:
                avg_success_prob = cohort_statistics.get('average_success_probability', 0)
                print(f"   Average Success Probability: {avg_success_prob:.1%}")
                
                risk_distribution = cohort_statistics.get('risk_distribution', {})
                if risk_distribution:
                    print(f"   Risk Distribution: High {risk_distribution.get('high', 0):.1%}, Moderate {risk_distribution.get('moderate', 0):.1%}, Low {risk_distribution.get('low', 0):.1%}")
            
            recommendations = response.get('cohort_recommendations', [])
            if recommendations:
                print(f"   Cohort Recommendations: {len(recommendations)}")
        return success

    # ========== CLINICAL TRIALS.GOV INTEGRATION TESTING ==========

    def test_clinical_trials_search_osteoarthritis(self):
        """Test clinical trials search for osteoarthritis with default recruitment status"""
        success, response = self.run_test(
            "Clinical Trials Search - Osteoarthritis (RECRUITING)",
            "GET",
            "clinical-trials/search?condition=osteoarthritis&recruitment_status=RECRUITING",
            200,
            timeout=45
        )
        
        if success:
            trials = response.get('trials', [])
            print(f"   Search Condition: {response.get('search_condition', 'unknown')}")
            print(f"   Recruitment Status: {response.get('recruitment_status', 'unknown')}")
            print(f"   Trials Found: {response.get('total_count', 0)}")
            print(f"   Source: {response.get('source', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
            
            if trials:
                top_trial = trials[0]
                print(f"   Top Trial: {top_trial.get('title', 'Unknown')[:60]}...")
                print(f"   NCT ID: {top_trial.get('nct_id', 'Unknown')}")
                print(f"   Overall Status: {top_trial.get('overall_status', 'Unknown')}")
                print(f"   Relevance Score: {top_trial.get('relevance_score', 0):.2f}")
                
                # Check for regenerative medicine interventions
                interventions = top_trial.get('interventions', [])
                regen_interventions = [i for i in interventions if i.get('regenerative_medicine', False)]
                print(f"   Regenerative Interventions: {len(regen_interventions)}")
                
                if regen_interventions:
                    categories = [i.get('category', 'Unknown') for i in regen_interventions]
                    print(f"   Intervention Categories: {', '.join(set(categories))}")
        return success

    def test_clinical_trials_search_rotator_cuff_stem_cell(self):
        """Test clinical trials search for rotator cuff injury with stem cell therapy intervention"""
        success, response = self.run_test(
            "Clinical Trials Search - Rotator Cuff + Stem Cell",
            "GET",
            "clinical-trials/search?condition=rotator%20cuff%20injury&intervention=stem%20cell%20therapy&max_results=15",
            200,
            timeout=45
        )
        
        if success:
            trials = response.get('trials', [])
            print(f"   Search Condition: {response.get('search_condition', 'unknown')}")
            print(f"   Intervention Filter: {response.get('intervention_filter', 'unknown')}")
            print(f"   Trials Found: {len(trials)}")
            print(f"   Search Timestamp: {response.get('search_timestamp', 'unknown')}")
            
            if trials:
                # Check for stem cell interventions
                stem_cell_trials = []
                for trial in trials:
                    interventions = trial.get('interventions', [])
                    for intervention in interventions:
                        if intervention.get('category') == 'Stem Cells':
                            stem_cell_trials.append(trial)
                            break
                
                print(f"   Stem Cell Trials: {len(stem_cell_trials)}")
                
                if stem_cell_trials:
                    top_stem_trial = stem_cell_trials[0]
                    print(f"   Top Stem Cell Trial: {top_stem_trial.get('title', 'Unknown')[:50]}...")
                    print(f"   Study Type: {top_stem_trial.get('study_type', 'Unknown')}")
                    print(f"   Phases: {', '.join(top_stem_trial.get('phases', []))}")
                    
                    # Check locations
                    locations = top_stem_trial.get('locations', [])
                    if locations:
                        print(f"   Locations Available: {len(locations)}")
                        first_location = locations[0]
                        print(f"   Sample Location: {first_location.get('city', 'Unknown')}, {first_location.get('country', 'Unknown')}")
        return success

    def test_clinical_trials_search_knee_pain_prp(self):
        """Test clinical trials search for knee pain with PRP intervention"""
        success, response = self.run_test(
            "Clinical Trials Search - Knee Pain + PRP",
            "GET",
            "clinical-trials/search?condition=knee%20pain&intervention=PRP&max_results=12",
            200,
            timeout=45
        )
        
        if success:
            trials = response.get('trials', [])
            print(f"   Search Condition: knee pain")
            print(f"   Intervention Filter: PRP")
            print(f"   Trials Found: {len(trials)}")
            
            if trials:
                # Check for PRP interventions
                prp_trials = []
                for trial in trials:
                    interventions = trial.get('interventions', [])
                    for intervention in interventions:
                        if intervention.get('category') == 'PRP' or 'prp' in intervention.get('name', '').lower():
                            prp_trials.append(trial)
                            break
                
                print(f"   PRP-Related Trials: {len(prp_trials)}")
                
                if prp_trials:
                    top_prp_trial = prp_trials[0]
                    print(f"   Top PRP Trial: {top_prp_trial.get('title', 'Unknown')[:50]}...")
                    print(f"   Brief Summary: {top_prp_trial.get('brief_summary', 'No summary')[:100]}...")
                    
                    # Check eligibility
                    print(f"   Eligible Ages: {', '.join(top_prp_trial.get('eligible_ages', []))}")
                    print(f"   Gender: {top_prp_trial.get('gender', 'Unknown')}")
        return success

    def test_clinical_trials_json_api_parsing(self):
        """Test JSON API parsing and trial data extraction quality"""
        success, response = self.run_test(
            "Clinical Trials - JSON API Parsing Quality",
            "GET",
            "clinical-trials/search?condition=osteoarthritis&max_results=5",
            200,
            timeout=30
        )
        
        if success:
            trials = response.get('trials', [])
            print(f"   Trials for Parsing Test: {len(trials)}")
            
            if trials:
                sample_trial = trials[0]
                
                # Check required fields are present
                required_fields = ['nct_id', 'title', 'overall_status', 'brief_summary', 'conditions', 'interventions']
                available_fields = [field for field in required_fields if field in sample_trial and sample_trial[field]]
                
                print(f"   Required Fields Present: {len(available_fields)}/{len(required_fields)}")
                print(f"   Available Fields: {', '.join(available_fields)}")
                
                # Check data quality
                nct_id = sample_trial.get('nct_id', '')
                print(f"   NCT ID Format Valid: {nct_id.startswith('NCT') and len(nct_id) >= 8}")
                
                title_length = len(sample_trial.get('title', ''))
                print(f"   Title Length: {title_length} characters")
                
                summary_length = len(sample_trial.get('brief_summary', ''))
                print(f"   Summary Length: {summary_length} characters")
                
                # Check intervention categorization
                interventions = sample_trial.get('interventions', [])
                categorized_interventions = [i for i in interventions if i.get('category') != 'other']
                print(f"   Categorized Interventions: {len(categorized_interventions)}/{len(interventions)}")
                
                # Check relevance scoring
                relevance_score = sample_trial.get('relevance_score', 0)
                print(f"   Relevance Score: {relevance_score:.3f} (0.0-1.0 range)")
                print(f"   Relevance Score Valid: {0.0 <= relevance_score <= 1.0}")
                
                # Check trial URL
                trial_url = sample_trial.get('trial_url', '')
                print(f"   Trial URL Valid: {trial_url.startswith('https://clinicaltrials.gov/')}")
        return success

    def test_clinical_trials_intervention_categorization(self):
        """Test intervention categorization for regenerative medicine"""
        success, response = self.run_test(
            "Clinical Trials - Intervention Categorization",
            "GET",
            "clinical-trials/search?condition=cartilage%20repair&max_results=10",
            200,
            timeout=30
        )
        
        if success:
            trials = response.get('trials', [])
            print(f"   Trials for Categorization Test: {len(trials)}")
            
            # Collect all intervention categories
            all_categories = {}
            regen_intervention_count = 0
            total_intervention_count = 0
            
            for trial in trials:
                interventions = trial.get('interventions', [])
                for intervention in interventions:
                    total_intervention_count += 1
                    category = intervention.get('category', 'unknown')
                    all_categories[category] = all_categories.get(category, 0) + 1
                    
                    if intervention.get('regenerative_medicine', False):
                        regen_intervention_count += 1
            
            print(f"   Total Interventions: {total_intervention_count}")
            print(f"   Regenerative Medicine Interventions: {regen_intervention_count}")
            print(f"   Categories Found: {len(all_categories)}")
            
            # Show category distribution
            for category, count in sorted(all_categories.items(), key=lambda x: x[1], reverse=True):
                print(f"   {category}: {count} interventions")
            
            # Check for expected regenerative categories
            expected_categories = ['PRP', 'BMAC', 'Stem Cells', 'Exosomes']
            found_categories = [cat for cat in expected_categories if cat in all_categories]
            print(f"   Expected Regen Categories Found: {len(found_categories)}/{len(expected_categories)}")
            print(f"   Found Categories: {', '.join(found_categories)}")
        return success

    def test_clinical_trials_relevance_scoring(self):
        """Test relevance scoring algorithm for different conditions and interventions"""
        conditions_to_test = [
            ("osteoarthritis", "Expected high relevance for joint conditions"),
            ("rotator cuff", "Expected high relevance for shoulder conditions"),
            ("cartilage defect", "Expected high relevance for cartilage repair")
        ]
        
        all_passed = True
        
        for condition, description in conditions_to_test:
            success, response = self.run_test(
                f"Clinical Trials - Relevance Scoring ({condition})",
                "GET",
                f"clinical-trials/search?condition={condition.replace(' ', '%20')}&max_results=8",
                200,
                timeout=30
            )
            
            if success:
                trials = response.get('trials', [])
                print(f"   Condition: {condition}")
                print(f"   Trials Found: {len(trials)}")
                
                if trials:
                    # Check relevance score distribution
                    relevance_scores = [trial.get('relevance_score', 0) for trial in trials]
                    avg_relevance = sum(relevance_scores) / len(relevance_scores)
                    max_relevance = max(relevance_scores)
                    min_relevance = min(relevance_scores)
                    
                    print(f"   Average Relevance: {avg_relevance:.3f}")
                    print(f"   Max Relevance: {max_relevance:.3f}")
                    print(f"   Min Relevance: {min_relevance:.3f}")
                    print(f"   Score Range Valid: {0.0 <= min_relevance <= max_relevance <= 1.0}")
                    
                    # Check if trials are sorted by relevance (descending)
                    is_sorted = all(relevance_scores[i] >= relevance_scores[i+1] for i in range(len(relevance_scores)-1))
                    print(f"   Trials Sorted by Relevance: {is_sorted}")
                    
                    # Show top trial details
                    top_trial = trials[0]
                    print(f"   Top Trial: {top_trial.get('title', 'Unknown')[:40]}...")
                    print(f"   Top Relevance: {top_trial.get('relevance_score', 0):.3f}")
                else:
                    print(f"   No trials found for {condition}")
            else:
                all_passed = False
                print(f"   Failed to test relevance scoring for {condition}")
        
        return all_passed

    def test_clinical_trials_patient_matching_osteoarthritis(self):
        """Test patient-specific trial matching for osteoarthritis with PRP and stem cell preferences"""
        success, response = self.run_test(
            "Clinical Trials - Patient Matching (Osteoarthritis + PRP/Stem Cell)",
            "GET",
            "clinical-trials/patient-matching?condition=osteoarthritis&therapy_preferences=PRP,stem%20cell",
            200,
            timeout=45
        )
        
        if success:
            matching_trials = response.get('matching_trials', [])
            print(f"   Patient Condition: {response.get('patient_condition', 'unknown')}")
            print(f"   Therapy Preferences: {response.get('therapy_preferences', [])}")
            print(f"   Total Matches: {response.get('total_matches', 0)}")
            print(f"   Status: {response.get('status', 'unknown')}")
            
            if matching_trials:
                top_match = matching_trials[0]
                print(f"   Top Match: {top_match.get('title', 'Unknown')[:50]}...")
                print(f"   Match Score: {top_match.get('match_score', 0):.3f}")
                
                # Check match reasons
                match_reasons = top_match.get('match_reasons', [])
                print(f"   Match Reasons: {len(match_reasons)}")
                if match_reasons:
                    print(f"   Top Reason: {match_reasons[0]}")
                
                # Check eligibility considerations
                eligibility = top_match.get('eligibility_considerations', {})
                print(f"   Age Range: {', '.join(eligibility.get('age_range', []))}")
                print(f"   Gender: {eligibility.get('gender', 'Unknown')}")
                print(f"   Study Type: {eligibility.get('study_type', 'Unknown')}")
                
                # Check next steps
                next_steps = top_match.get('next_steps', [])
                print(f"   Next Steps Provided: {len(next_steps)}")
                if next_steps:
                    print(f"   First Step: {next_steps[0][:60]}...")
            
            # Check recommendations
            recommendations = response.get('recommendations', [])
            print(f"   Recommendations: {len(recommendations)}")
            if recommendations:
                print(f"   Top Recommendation: {recommendations[0]}")
        return success

    def test_clinical_trials_patient_matching_shoulder_bmac(self):
        """Test patient-specific trial matching for shoulder injury with BMAC preference"""
        success, response = self.run_test(
            "Clinical Trials - Patient Matching (Shoulder + BMAC)",
            "GET",
            "clinical-trials/patient-matching?condition=shoulder%20injury&therapy_preferences=BMAC",
            200,
            timeout=45
        )
        
        if success:
            matching_trials = response.get('matching_trials', [])
            print(f"   Patient Condition: shoulder injury")
            print(f"   Therapy Preferences: BMAC")
            print(f"   Total Matches: {len(matching_trials)}")
            print(f"   Status: {response.get('status', 'unknown')}")
            
            if matching_trials:
                # Check match score distribution
                match_scores = [trial.get('match_score', 0) for trial in matching_trials]
                avg_match_score = sum(match_scores) / len(match_scores)
                print(f"   Average Match Score: {avg_match_score:.3f}")
                print(f"   Match Score Range: {min(match_scores):.3f} - {max(match_scores):.3f}")
                
                # Check if matches are sorted by score
                is_sorted = all(match_scores[i] >= match_scores[i+1] for i in range(len(match_scores)-1))
                print(f"   Matches Sorted by Score: {is_sorted}")
                
                # Check for BMAC-specific matches
                bmac_matches = []
                for trial in matching_trials:
                    interventions = trial.get('interventions', [])
                    for intervention in interventions:
                        if intervention.get('category') == 'BMAC' or 'bmac' in intervention.get('name', '').lower():
                            bmac_matches.append(trial)
                            break
                
                print(f"   BMAC-Specific Matches: {len(bmac_matches)}")
                
                if bmac_matches:
                    top_bmac_match = bmac_matches[0]
                    print(f"   Top BMAC Match: {top_bmac_match.get('title', 'Unknown')[:40]}...")
                    print(f"   BMAC Match Score: {top_bmac_match.get('match_score', 0):.3f}")
            else:
                print(f"   No matches found - checking recommendations")
                recommendations = response.get('recommendations', [])
                if recommendations:
                    print(f"   Fallback Recommendations: {len(recommendations)}")
                    print(f"   Top Recommendation: {recommendations[0]}")
        return success

    def test_clinical_trials_match_scoring_algorithm(self):
        """Test patient-trial match scoring algorithm and compatibility assessment"""
        success, response = self.run_test(
            "Clinical Trials - Match Scoring Algorithm",
            "GET",
            "clinical-trials/patient-matching?condition=knee%20osteoarthritis&therapy_preferences=PRP,stem%20cell,BMAC",
            200,
            timeout=45
        )
        
        if success:
            matching_trials = response.get('matching_trials', [])
            print(f"   Condition: knee osteoarthritis")
            print(f"   Preferences: PRP, stem cell, BMAC")
            print(f"   Total Matches: {len(matching_trials)}")
            
            if matching_trials:
                # Analyze match score distribution
                match_scores = [trial.get('match_score', 0) for trial in matching_trials]
                
                # Check score validity (0.0-1.0 range)
                valid_scores = all(0.0 <= score <= 1.0 for score in match_scores)
                print(f"   All Match Scores Valid (0.0-1.0): {valid_scores}")
                
                # Check minimum threshold (should be >= 0.3 based on implementation)
                min_threshold_met = all(score >= 0.3 for score in match_scores)
                print(f"   All Scores Meet Minimum Threshold (‚â•0.3): {min_threshold_met}")
                
                # Show score distribution
                high_scores = [s for s in match_scores if s >= 0.7]
                medium_scores = [s for s in match_scores if 0.5 <= s < 0.7]
                low_scores = [s for s in match_scores if 0.3 <= s < 0.5]
                
                print(f"   High Scores (‚â•0.7): {len(high_scores)}")
                print(f"   Medium Scores (0.5-0.7): {len(medium_scores)}")
                print(f"   Low Scores (0.3-0.5): {len(low_scores)}")
                
                # Check top match details
                top_match = matching_trials[0]
                print(f"   Top Match Score: {top_match.get('match_score', 0):.3f}")
                
                # Verify match reasons are provided
                match_reasons = top_match.get('match_reasons', [])
                print(f"   Match Reasons Provided: {len(match_reasons) > 0}")
                
                # Verify eligibility considerations
                eligibility = top_match.get('eligibility_considerations', {})
                eligibility_fields = ['age_range', 'gender', 'locations', 'study_type']
                provided_fields = [field for field in eligibility_fields if field in eligibility]
                print(f"   Eligibility Fields Provided: {len(provided_fields)}/{len(eligibility_fields)}")
                
                # Verify next steps are generated
                next_steps = top_match.get('next_steps', [])
                print(f"   Next Steps Generated: {len(next_steps) > 0}")
                if next_steps:
                    print(f"   Next Steps Count: {len(next_steps)}")
        return success

    def test_clinical_trials_database_storage(self):
        """Test database storage of trial data with proper indexing"""
        # First, perform a search to populate database
        success1, response1 = self.run_test(
            "Clinical Trials - Database Storage (Search First)",
            "GET",
            "clinical-trials/search?condition=regenerative%20medicine&max_results=5",
            200,
            timeout=30
        )
        
        if not success1:
            return False
        
        # Then perform another search to test database retrieval
        success2, response2 = self.run_test(
            "Clinical Trials - Database Storage (Verify Storage)",
            "GET",
            "clinical-trials/search?condition=regenerative%20medicine&max_results=5",
            200,
            timeout=20  # Should be faster due to caching
        )
        
        if success2:
            trials = response2.get('trials', [])
            print(f"   Trials Retrieved: {len(trials)}")
            print(f"   Search Timestamp: {response2.get('search_timestamp', 'unknown')}")
            
            if trials:
                sample_trial = trials[0]
                
                # Check database-specific fields
                db_fields = ['search_condition', 'extracted_at', 'trial_url']
                available_db_fields = [field for field in db_fields if field in sample_trial]
                print(f"   Database Fields Present: {len(available_db_fields)}/{len(db_fields)}")
                
                # Check NCT ID format (should be indexed)
                nct_id = sample_trial.get('nct_id', '')
                print(f"   NCT ID (Index Key): {nct_id}")
                print(f"   NCT ID Format Valid: {nct_id.startswith('NCT')}")
                
                # Check search condition tracking
                search_condition = sample_trial.get('search_condition', '')
                print(f"   Search Condition Tracked: {len(search_condition) > 0}")
                
                # Check extraction timestamp
                extracted_at = sample_trial.get('extracted_at')
                print(f"   Extraction Timestamp Present: {extracted_at is not None}")
        
        return success1 and success2

    def test_clinical_trials_error_handling(self):
        """Test error handling for API issues and invalid queries"""
        # Test with empty condition
        success1, response1 = self.run_test(
            "Clinical Trials - Error Handling (Empty Condition)",
            "GET",
            "clinical-trials/search?condition=&max_results=5",
            200,  # Should handle gracefully
            timeout=20
        )
        
        if success1:
            print(f"   Empty Condition Status: {response1.get('status', 'unknown')}")
            if 'error' in response1:
                print(f"   Error Handled: {response1.get('error', 'No error')[:50]}...")
            print(f"   Trials Returned: {len(response1.get('trials', []))}")
            print(f"   Fallback Suggestion: {'fallback_suggestion' in response1}")
        
        # Test with very specific/rare condition
        success2, response2 = self.run_test(
            "Clinical Trials - Error Handling (Rare Condition)",
            "GET",
            "clinical-trials/search?condition=extremely_rare_condition_xyz123&max_results=5",
            200,
            timeout=30
        )
        
        if success2:
            print(f"   Rare Condition Status: {response2.get('status', 'unknown')}")
            trials = response2.get('trials', [])
            print(f"   Trials Found: {len(trials)}")
            
            if len(trials) == 0:
                print(f"   No Results Handled Gracefully: True")
                if 'fallback_suggestion' in response2:
                    print(f"   Fallback Suggestion: {response2.get('fallback_suggestion', 'None')}")
        
        return success1 and success2

    # ========== CRITICAL PRIORITY FEATURES TESTING ==========
    
    def test_living_evidence_engine_protocol_mapping(self):
        """Test Living Evidence Engine - Protocol Evidence Mapping"""
        if not self.patient_id:
            print("‚ùå No patient ID available for evidence mapping testing")
            return False

        mapping_data = {
            "protocol_id": self.protocol_id or "test_protocol_001",
            "condition": "osteoarthritis",
            "intervention": "platelet rich plasma",
            "evidence_requirements": {
                "minimum_evidence_level": "Level II",
                "include_recent_studies": True,
                "include_systematic_reviews": True
            }
        }

        print("   This may take 30-45 seconds for evidence mapping...")
        success, response = self.run_test(
            "Living Evidence Engine - Protocol Evidence Mapping",
            "POST",
            "evidence/protocol-evidence-mapping",
            200,
            data=mapping_data,
            timeout=60
        )
        
        if success:
            print(f"   Mapping ID: {response.get('mapping_id', 'Unknown')}")
            print(f"   Evidence Sources: {len(response.get('evidence_sources', []))}")
            print(f"   Quality Score: {response.get('evidence_quality_score', 0):.2f}")
            print(f"   Systematic Reviews: {len(response.get('systematic_reviews', []))}")
            print(f"   Recent Studies: {len(response.get('recent_studies', []))}")
        return success

    def test_living_evidence_engine_living_reviews(self):
        """Test Living Evidence Engine - Living Systematic Reviews"""
        success, response = self.run_test(
            "Living Evidence Engine - Living Systematic Reviews",
            "GET",
            "evidence/living-reviews/osteoarthritis",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Review ID: {response.get('review_id', 'Unknown')}")
            print(f"   Condition: {response.get('condition', 'Unknown')}")
            print(f"   Total Studies: {response.get('total_studies', 0)}")
            print(f"   Last Updated: {response.get('last_updated', 'Unknown')}")
            print(f"   Evidence Quality: {response.get('evidence_quality', 'Unknown')}")
            print(f"   Therapy Evidence: {len(response.get('therapy_evidence', []))}")
        return success

    def test_living_evidence_engine_protocol_retrieval(self):
        """Test Living Evidence Engine - Protocol Evidence Mapping Retrieval"""
        protocol_id = self.protocol_id or "test_protocol_001"
        
        success, response = self.run_test(
            "Living Evidence Engine - Protocol Evidence Mapping Retrieval",
            "GET",
            f"evidence/protocol/{protocol_id}/evidence-mapping",
            200
        )
        
        if success:
            print(f"   Protocol ID: {response.get('protocol_id', 'Unknown')}")
            print(f"   Mapping Status: {response.get('mapping_status', 'Unknown')}")
            print(f"   Evidence Features: {len(response.get('evidence_features', []))}")
            print(f"   Last Mapping Update: {response.get('last_mapping_update', 'Unknown')}")
        return success

    def test_living_evidence_engine_alerts(self):
        """Test Living Evidence Engine - Evidence Change Alerts"""
        protocol_id = self.protocol_id or "test_protocol_001"
        
        success, response = self.run_test(
            "Living Evidence Engine - Evidence Change Alerts",
            "GET",
            f"evidence/alerts/{protocol_id}",
            200
        )
        
        if success:
            print(f"   Protocol ID: {response.get('protocol_id', 'Unknown')}")
            print(f"   Alert System Status: {response.get('alert_system_status', 'Unknown')}")
            print(f"   Active Alerts: {len(response.get('active_alerts', []))}")
            print(f"   Evidence Changes: {len(response.get('evidence_changes', []))}")
        return success

    def test_advanced_differential_diagnosis_comprehensive(self):
        """Test Advanced Differential Diagnosis - Comprehensive Analysis"""
        if not self.patient_id:
            print("‚ùå No patient ID available for differential diagnosis testing")
            return False

        diagnosis_data = {
            "patient_id": self.patient_id,
            "clinical_presentation": {
                "chief_complaint": "bilateral knee pain and stiffness",
                "symptoms": ["joint pain", "morning stiffness", "decreased mobility"],
                "duration": "3 years progressive",
                "severity": "moderate to severe"
            },
            "diagnostic_requirements": {
                "include_differential_ranking": True,
                "include_confidence_intervals": True,
                "include_mechanism_analysis": True
            }
        }

        print("   This may take 30-45 seconds for comprehensive differential diagnosis...")
        success, response = self.run_test(
            "Advanced Differential Diagnosis - Comprehensive Analysis",
            "POST",
            "diagnosis/comprehensive-differential",
            200,
            data=diagnosis_data,
            timeout=60
        )
        
        if success:
            print(f"   Diagnosis ID: {response.get('diagnosis_id', 'Unknown')}")
            print(f"   Primary Diagnosis: {response.get('primary_diagnosis', 'Unknown')}")
            print(f"   Differential Count: {len(response.get('differential_diagnoses', []))}")
            print(f"   Confidence Score: {response.get('confidence_score', 0):.2f}")
            print(f"   Mechanism Insights: {len(response.get('mechanism_insights', []))}")
            print(f"   Evidence Quality: {response.get('evidence_quality', 'Unknown')}")
        return success

    def test_advanced_differential_diagnosis_retrieval(self):
        """Test Advanced Differential Diagnosis - Diagnosis Retrieval"""
        # Use a test diagnosis ID
        diagnosis_id = "test_diagnosis_001"
        
        success, response = self.run_test(
            "Advanced Differential Diagnosis - Diagnosis Retrieval",
            "GET",
            f"diagnosis/{diagnosis_id}",
            200
        )
        
        if success:
            print(f"   Diagnosis ID: {response.get('diagnosis_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Analysis Type: {response.get('analysis_type', 'Unknown')}")
            print(f"   Generated At: {response.get('generated_at', 'Unknown')}")
        return success

    def test_advanced_differential_diagnosis_engine_status(self):
        """Test Advanced Differential Diagnosis - Engine Status"""
        success, response = self.run_test(
            "Advanced Differential Diagnosis - Engine Status",
            "GET",
            "diagnosis/engine-status",
            200
        )
        
        if success:
            print(f"   Engine Status: {response.get('engine_status', 'Unknown')}")
            print(f"   Models Active: {response.get('models_active', 0)}")
            print(f"   Diagnostic Capabilities: {len(response.get('diagnostic_capabilities', []))}")
            print(f"   Last Updated: {response.get('last_updated', 'Unknown')}")
        return success

    def test_enhanced_explainable_ai_explanation(self):
        """Test Enhanced Explainable AI - Generate Enhanced Explanation"""
        if not self.patient_id:
            print("‚ùå No patient ID available for explainable AI testing")
            return False

        explanation_data = {
            "patient_id": self.patient_id,
            "analysis_type": "comprehensive_diagnosis",
            "explanation_requirements": {
                "include_shap_analysis": True,
                "include_lime_breakdown": True,
                "include_feature_interactions": True,
                "visualization_level": "detailed"
            },
            "model_context": {
                "model_type": "regenerative_medicine_ai",
                "prediction_target": "treatment_recommendation"
            }
        }

        print("   This may take 30-45 seconds for enhanced AI explanation generation...")
        success, response = self.run_test(
            "Enhanced Explainable AI - Generate Enhanced Explanation",
            "POST",
            "ai/enhanced-explanation",
            200,
            data=explanation_data,
            timeout=60
        )
        
        if success:
            self.explanation_id = response.get('explanation_id')
            print(f"   Explanation ID: {response.get('explanation_id', 'Unknown')}")
            print(f"   SHAP Analysis: {response.get('shap_analysis', {}).get('status', 'Unknown')}")
            print(f"   LIME Breakdown: {response.get('lime_breakdown', {}).get('status', 'Unknown')}")
            print(f"   Feature Count: {len(response.get('feature_importance', []))}")
            print(f"   Transparency Score: {response.get('transparency_score', 0):.2f}")
            print(f"   Explanation Quality: {response.get('explanation_quality', 'Unknown')}")
        return success

    def test_enhanced_explainable_ai_retrieval(self):
        """Test Enhanced Explainable AI - Explanation Retrieval"""
        explanation_id = getattr(self, 'explanation_id', 'test_explanation_001')
        
        success, response = self.run_test(
            "Enhanced Explainable AI - Explanation Retrieval",
            "GET",
            f"ai/enhanced-explanation/{explanation_id}",
            200
        )
        
        if success:
            print(f"   Explanation ID: {response.get('explanation_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Analysis Type: {response.get('analysis_type', 'Unknown')}")
            print(f"   Generated At: {response.get('generated_at', 'Unknown')}")
        return success

    def test_enhanced_explainable_ai_visual_breakdown(self):
        """Test Enhanced Explainable AI - Visual Breakdown"""
        explanation_id = getattr(self, 'explanation_id', 'test_explanation_001')
        
        success, response = self.run_test(
            "Enhanced Explainable AI - Visual Breakdown",
            "GET",
            f"ai/visual-breakdown/{explanation_id}",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Explanation ID: {response.get('explanation_id', 'Unknown')}")
            print(f"   Visual Components: {len(response.get('visual_components', []))}")
            print(f"   SHAP Visualizations: {len(response.get('shap_visualizations', []))}")
            print(f"   LIME Visualizations: {len(response.get('lime_visualizations', []))}")
            print(f"   Interactive Elements: {response.get('interactive_elements', 0)}")
        return success

    def test_enhanced_explainable_ai_feature_interactions(self):
        """Test Enhanced Explainable AI - Feature Interactions"""
        interaction_data = {
            "patient_id": self.patient_id or "test_patient_001",
            "feature_set": [
                "age", "diagnosis_confidence", "symptom_severity", 
                "medical_history", "lab_results", "imaging_findings"
            ],
            "interaction_analysis": {
                "include_pairwise_interactions": True,
                "include_higher_order_interactions": True,
                "significance_threshold": 0.05
            }
        }

        success, response = self.run_test(
            "Enhanced Explainable AI - Feature Interactions",
            "POST",
            "ai/feature-interactions",
            200,
            data=interaction_data,
            timeout=45
        )
        
        if success:
            print(f"   Analysis ID: {response.get('analysis_id', 'Unknown')}")
            print(f"   Feature Pairs Analyzed: {len(response.get('pairwise_interactions', []))}")
            print(f"   Significant Interactions: {len(response.get('significant_interactions', []))}")
            print(f"   Interaction Strength: {response.get('max_interaction_strength', 0):.3f}")
        return success

    def test_enhanced_explainable_ai_transparency_assessment(self):
        """Test Enhanced Explainable AI - Transparency Assessment"""
        explanation_id = getattr(self, 'explanation_id', 'test_explanation_001')
        
        success, response = self.run_test(
            "Enhanced Explainable AI - Transparency Assessment",
            "GET",
            f"ai/transparency-assessment/{explanation_id}",
            200
        )
        
        if success:
            print(f"   Explanation ID: {response.get('explanation_id', 'Unknown')}")
            print(f"   Transparency Score: {response.get('transparency_score', 0):.2f}")
            print(f"   Interpretability Level: {response.get('interpretability_level', 'Unknown')}")
            print(f"   Clinical Relevance: {response.get('clinical_relevance', 0):.2f}")
            print(f"   Explanation Fidelity: {response.get('explanation_fidelity', 0):.2f}")
        return success

    def run_critical_priority_features_tests(self):
        """Run tests for the three Critical Priority Features"""
        
        print("üöÄ Starting Critical Priority Features Testing Suite")
        print("=" * 80)
        
        # Living Evidence Engine System Tests
        print("\nüî¨ LIVING EVIDENCE ENGINE SYSTEM TESTS")
        print("-" * 50)
        self.test_living_evidence_engine_protocol_mapping()
        self.test_living_evidence_engine_living_reviews()
        self.test_living_evidence_engine_protocol_retrieval()
        self.test_living_evidence_engine_alerts()
        
        # Advanced Differential Diagnosis System Tests
        print("\nüß† ADVANCED DIFFERENTIAL DIAGNOSIS SYSTEM TESTS")
        print("-" * 50)
        self.test_advanced_differential_diagnosis_comprehensive()
        self.test_advanced_differential_diagnosis_retrieval()
        self.test_advanced_differential_diagnosis_engine_status()
        
        # Enhanced Explainable AI System Tests
        print("\nüéØ ENHANCED EXPLAINABLE AI SYSTEM TESTS")
        print("-" * 50)
        self.test_enhanced_explainable_ai_explanation()
        self.test_enhanced_explainable_ai_retrieval()
        self.test_enhanced_explainable_ai_visual_breakdown()
        self.test_enhanced_explainable_ai_feature_interactions()
        self.test_enhanced_explainable_ai_transparency_assessment()
        
        # Results Summary
        print("\n" + "=" * 80)
        print("üèÅ CRITICAL PRIORITY FEATURES TESTING COMPLETE")
        print("=" * 80)
        print(f"üìä Tests Run: {self.tests_run}")
        print(f"‚úÖ Tests Passed: {self.tests_passed}")
        print(f"‚ùå Tests Failed: {self.tests_run - self.tests_passed}")
        print(f"üìà Success Rate: {(self.tests_passed/self.tests_run)*100:.1f}%")
        
        return self.tests_passed, self.tests_run

    # ========== CORE MEDICAL AI FEATURES TESTING ==========
    
    def test_differential_diagnosis_generation(self):
        """Test POST /api/analyze-patient endpoint with comprehensive patient data"""
        if not self.patient_id:
            print("‚ùå No patient ID available for differential diagnosis testing")
            return False

        # Use the specific patient profile from the review request
        patient_data = {
            "demographics": {
                "name": "Margaret Thompson",
                "age": "65",
                "gender": "Female",
                "occupation": "Retired Teacher",
                "insurance": "Medicare"
            },
            "chief_complaint": "Chronic knee pain and stiffness",
            "history_present_illness": "65-year-old female with progressive bilateral knee pain over 2 years. Pain worse with activity, morning stiffness lasting 45 minutes. Failed conservative management including NSAIDs, physical therapy, and corticosteroid injections. Seeking regenerative medicine alternatives to knee replacement surgery.",
            "past_medical_history": ["Osteoarthritis", "Hypertension", "Type 2 Diabetes"],
            "medications": ["Metformin", "Lisinopril", "Ibuprofen"],
            "allergies": ["NKDA"],
            "vital_signs": {
                "temperature": "98.4",
                "blood_pressure": "135/85",
                "heart_rate": "78",
                "respiratory_rate": "16",
                "oxygen_saturation": "97",
                "weight": "165",
                "height": "5'4\""
            },
            "symptoms": ["chronic knee pain", "morning stiffness", "decreased mobility", "joint space narrowing"],
            "lab_results": {
                "inflammatory_markers": {
                    "CRP": "3.2 mg/L",
                    "ESR": "22 mm/hr"
                },
                "metabolic_panel": {
                    "glucose": "145 mg/dL",
                    "HbA1c": "7.1%"
                }
            },
            "imaging_data": [
                {
                    "type": "X-ray",
                    "location": "bilateral knees",
                    "findings": "Knee osteoarthritis with joint space narrowing",
                    "date": "2024-01-15"
                }
            ]
        }

        print("   Testing differential diagnosis generation with comprehensive patient data...")
        print("   This may take 30-60 seconds for AI processing...")
        
        success, response = self.run_test(
            "CORE FEATURE: Differential Diagnosis Generation",
            "POST",
            f"patients/{self.patient_id}/analyze",
            200,
            data=patient_data,
            timeout=90
        )
        
        if success:
            diagnostic_results = response.get('diagnostic_results', [])
            print(f"   ‚úÖ Differential Diagnoses Generated: {len(diagnostic_results)}")
            
            if diagnostic_results:
                # Test probability rankings
                for i, result in enumerate(diagnostic_results[:3]):
                    diagnosis = result.get('diagnosis', 'Unknown')
                    confidence = result.get('confidence_score', 0)
                    reasoning = result.get('reasoning', '')
                    targets = result.get('regenerative_targets', [])
                    mechanisms = result.get('mechanisms_involved', [])
                    
                    print(f"   Diagnosis {i+1}: {diagnosis}")
                    print(f"   Confidence Score: {confidence:.2f} (Range: 0.0-1.0)")
                    print(f"   Regenerative Targets: {len(targets)}")
                    print(f"   Mechanisms Involved: {len(mechanisms)}")
                    print(f"   Reasoning Quality: {'Good' if len(reasoning) > 50 else 'Basic'}")
                
                # Verify confidence scores are in valid range
                valid_scores = all(0.0 <= result.get('confidence_score', 0) <= 1.0 for result in diagnostic_results)
                print(f"   ‚úÖ Confidence Scores Valid (0.0-1.0): {valid_scores}")
                
                # Check for multi-modal data integration
                multi_modal_integration = any('multi' in str(result).lower() for result in diagnostic_results)
                print(f"   Multi-modal Integration: {'Detected' if multi_modal_integration else 'Basic'}")
                
        return success

    def test_shap_lime_explainable_ai(self):
        """Test GET /api/protocols/{protocol_id}/explanation endpoint"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for explainable AI testing")
            return False

        print("   Testing SHAP/LIME explainable AI system...")
        success, response = self.run_test(
            "CORE FEATURE: SHAP/LIME Explainable AI System",
            "POST",
            f"protocols/{self.protocol_id}/explanation",
            200,
            timeout=60
        )
        
        if success:
            explanation = response.get('explanation', {})
            print(f"   ‚úÖ Explanation Generated: {response.get('status', 'unknown')}")
            print(f"   Explanation Type: {explanation.get('explanation_type', 'unknown')}")
            print(f"   Explanation Confidence: {explanation.get('explanation_confidence', 0):.2f}")
            print(f"   Transparency Score: {explanation.get('overall_transparency_score', 0):.2f}")
            
            # Test SHAP-style feature importance
            feature_importance = explanation.get('feature_importance', {})
            print(f"   Feature Importance Factors: {len(feature_importance)}")
            
            if feature_importance:
                # Check key features
                key_features = ['age', 'diagnosis_confidence', 'symptom_severity', 'medical_history']
                available_features = [f for f in key_features if f in feature_importance]
                print(f"   Key Features Available: {len(available_features)}/{len(key_features)}")
                print(f"   Available Features: {', '.join(available_features)}")
                
                # Check feature explanations
                for feature in available_features[:3]:
                    feature_data = feature_importance[feature]
                    importance = feature_data.get('importance_score', 0)
                    contribution = feature_data.get('contribution', 'unknown')
                    print(f"   {feature}: Importance={importance:.2f}, Contribution={contribution}")
            
            # Test SHAP explanation structure
            shap_explanation = explanation.get('shap_explanation', {})
            if shap_explanation:
                print(f"   ‚úÖ SHAP Explanation Available")
                print(f"   Base Value: {shap_explanation.get('base_value', 0):.2f}")
                print(f"   Final Prediction: {shap_explanation.get('final_prediction', 0):.2f}")
                
                feature_contributions = shap_explanation.get('feature_contributions', [])
                print(f"   Feature Contributions: {len(feature_contributions)}")
                
                if feature_contributions:
                    top_contribution = feature_contributions[0]
                    print(f"   Top Contributing Feature: {top_contribution.get('feature', 'unknown')}")
                    print(f"   Contribution Value: {top_contribution.get('contribution', 0):.2f}")
            
            # Test therapy selection reasoning
            therapy_reasoning = explanation.get('therapy_selection_reasoning', [])
            print(f"   Therapy Selection Reasoning: {len(therapy_reasoning)} therapies explained")
            
            if therapy_reasoning:
                first_therapy = therapy_reasoning[0]
                print(f"   First Therapy: {first_therapy.get('therapy', 'unknown')}")
                selection_factors = first_therapy.get('selection_factors', [])
                print(f"   Selection Factors: {len(selection_factors)}")
                
        return success

    def test_safety_alerts_contraindication_checking(self):
        """Test contraindication detection and safety alerts for high-risk patients"""
        if not self.patient_id:
            print("‚ùå No patient ID available for safety testing")
            return False

        # Create high-risk patient data with contraindications
        high_risk_patient_data = {
            "demographics": {
                "name": "Robert Wilson",
                "age": "72",
                "gender": "Male",
                "occupation": "Retired",
                "insurance": "Medicare"
            },
            "chief_complaint": "Severe knee osteoarthritis with diabetes complications",
            "history_present_illness": "72-year-old male with severe bilateral knee osteoarthritis and poorly controlled diabetes. Recent history of infection. Seeking regenerative medicine options but has multiple risk factors.",
            "past_medical_history": ["Osteoarthritis", "Type 2 Diabetes", "Recent infection", "Hypertension", "Chronic kidney disease"],
            "medications": ["Metformin", "Insulin", "Prednisone", "Lisinopril", "Warfarin"],
            "allergies": ["Penicillin"],
            "vital_signs": {
                "temperature": "99.2",
                "blood_pressure": "145/95",
                "heart_rate": "88",
                "respiratory_rate": "18",
                "oxygen_saturation": "95",
                "weight": "210",
                "height": "5'10\""
            },
            "symptoms": ["severe knee pain", "swelling", "limited mobility", "recent fever"],
            "lab_results": {
                "inflammatory_markers": {
                    "CRP": "8.5 mg/L",  # Elevated
                    "ESR": "45 mm/hr"   # Elevated
                },
                "metabolic_panel": {
                    "glucose": "220 mg/dL",  # High
                    "HbA1c": "9.2%",         # Poor control
                    "creatinine": "1.8 mg/dL" # Elevated
                }
            }
        }

        print("   Testing safety alerts and contraindication checking...")
        print("   This may take 30-60 seconds for comprehensive safety analysis...")
        
        success, response = self.run_test(
            "CORE FEATURE: Safety Alerts & Contraindication Checking",
            "POST",
            f"patients/{self.patient_id}/analyze",
            200,
            data=high_risk_patient_data,
            timeout=90
        )
        
        if success:
            diagnostic_results = response.get('diagnostic_results', [])
            print(f"   ‚úÖ Safety Analysis Completed: {len(diagnostic_results)} results")
            
            # Check for contraindication detection
            contraindications_found = []
            safety_alerts = []
            
            for result in diagnostic_results:
                reasoning = result.get('reasoning', '').lower()
                evidence = ' '.join(result.get('supporting_evidence', [])).lower()
                
                # Look for contraindication keywords
                contraindication_keywords = ['contraindication', 'risk', 'caution', 'avoid', 'infection', 'diabetes', 'steroid']
                for keyword in contraindication_keywords:
                    if keyword in reasoning or keyword in evidence:
                        contraindications_found.append(keyword)
                
                # Look for safety alerts
                safety_keywords = ['safety', 'alert', 'warning', 'high risk', 'complication']
                for keyword in safety_keywords:
                    if keyword in reasoning or keyword in evidence:
                        safety_alerts.append(keyword)
            
            print(f"   Contraindications Detected: {len(set(contraindications_found))}")
            print(f"   Safety Alerts Generated: {len(set(safety_alerts))}")
            
            if contraindications_found:
                print(f"   Key Contraindications: {', '.join(list(set(contraindications_found))[:3])}")
            
            # Test medication interaction warnings
            medication_warnings = []
            for result in diagnostic_results:
                reasoning = result.get('reasoning', '').lower()
                if any(med in reasoning for med in ['prednisone', 'steroid', 'warfarin', 'insulin']):
                    medication_warnings.append("Medication interaction detected")
            
            print(f"   Medication Interaction Warnings: {len(medication_warnings)}")
            
            # Check for diabetes considerations
            diabetes_considerations = []
            for result in diagnostic_results:
                reasoning = result.get('reasoning', '').lower()
                if 'diabetes' in reasoning or 'glucose' in reasoning:
                    diabetes_considerations.append("Diabetes consideration noted")
            
            print(f"   ‚úÖ Diabetes Considerations for Regenerative Medicine: {len(diabetes_considerations)}")
            
        return success

    def test_comprehensive_patient_analysis_workflow(self):
        """Test the complete patient ‚Üí analysis ‚Üí protocol ‚Üí explanation workflow"""
        print("   Testing complete medical AI workflow...")
        
        # Step 1: Create patient with comprehensive data
        workflow_patient_data = {
            "demographics": {
                "name": "Dr. Jennifer Adams",
                "age": "58",
                "gender": "Female",
                "occupation": "Orthopedic Surgeon",
                "insurance": "Self-pay"
            },
            "chief_complaint": "Bilateral knee osteoarthritis seeking regenerative alternatives",
            "history_present_illness": "58-year-old female orthopedic surgeon with progressive bilateral knee pain over 3 years. Pain worse with activity, morning stiffness lasting 30 minutes. Failed conservative management. Seeking regenerative medicine options to continue surgical practice.",
            "past_medical_history": ["Osteoarthritis", "Hypertension", "Hypothyroidism"],
            "medications": ["Lisinopril 10mg daily", "Levothyroxine 75mcg daily", "Ibuprofen PRN"],
            "allergies": ["NKDA"],
            "vital_signs": {
                "temperature": "98.6",
                "blood_pressure": "128/82",
                "heart_rate": "72",
                "respiratory_rate": "16",
                "oxygen_saturation": "98",
                "weight": "145",
                "height": "5'6\""
            },
            "symptoms": ["bilateral knee pain", "morning stiffness", "decreased mobility", "functional limitation"],
            "lab_results": {
                "inflammatory_markers": {
                    "CRP": "2.1 mg/L",
                    "ESR": "18 mm/hr"
                },
                "complete_blood_count": {
                    "WBC": "6.2 K/uL",
                    "RBC": "4.5 M/uL",
                    "platelets": "285 K/uL"
                }
            },
            "imaging_data": [
                {
                    "type": "X-ray",
                    "location": "bilateral knees",
                    "findings": "Grade 2-3 osteoarthritis with joint space narrowing and osteophyte formation",
                    "date": "2024-01-15"
                }
            ]
        }

        # Step 1: Create workflow patient
        workflow_success, workflow_response = self.run_test(
            "WORKFLOW STEP 1: Create Comprehensive Patient",
            "POST",
            "patients",
            200,
            data=workflow_patient_data
        )
        
        if not workflow_success:
            return False
            
        workflow_patient_id = workflow_response.get('patient_id')
        print(f"   Workflow Patient ID: {workflow_patient_id}")

        # Step 2: Analyze patient data
        analysis_success, analysis_response = self.run_test(
            "WORKFLOW STEP 2: Comprehensive Patient Analysis",
            "POST",
            f"patients/{workflow_patient_id}/analyze",
            200,
            data={},
            timeout=90
        )
        
        if not analysis_success:
            return False
            
        diagnostic_results = analysis_response.get('diagnostic_results', [])
        print(f"   Analysis Results: {len(diagnostic_results)} diagnoses")

        # Step 3: Generate protocol
        protocol_data = {
            "patient_id": workflow_patient_id,
            "school_of_thought": "ai_optimized"
        }
        
        protocol_success, protocol_response = self.run_test(
            "WORKFLOW STEP 3: Generate AI-Optimized Protocol",
            "POST",
            "protocols/generate",
            200,
            data=protocol_data,
            timeout=90
        )
        
        if not protocol_success:
            return False
            
        workflow_protocol_id = protocol_response.get('protocol_id')
        print(f"   Protocol Generated: {workflow_protocol_id}")
        print(f"   Protocol Steps: {len(protocol_response.get('protocol_steps', []))}")

        # Step 4: Generate explanation
        explanation_success, explanation_response = self.run_test(
            "WORKFLOW STEP 4: Generate SHAP/LIME Explanation",
            "POST",
            f"protocols/{workflow_protocol_id}/explanation",
            200,
            timeout=60
        )
        
        if not explanation_success:
            return False
            
        explanation = explanation_response.get('explanation', {})
        print(f"   Explanation Generated: {explanation_response.get('status', 'unknown')}")
        print(f"   Feature Importance Factors: {len(explanation.get('feature_importance', {}))}")

        # Step 5: Verify data persistence
        persistence_success, persistence_response = self.run_test(
            "WORKFLOW STEP 5: Verify Data Persistence",
            "GET",
            f"patients/{workflow_patient_id}",
            200
        )
        
        if persistence_success:
            print(f"   ‚úÖ Data Persistence Verified")
            print(f"   Patient Retrieved: {persistence_response.get('demographics', {}).get('name', 'Unknown')}")
        
        # Overall workflow success
        workflow_complete = all([workflow_success, analysis_success, protocol_success, explanation_success, persistence_success])
        print(f"   ‚úÖ COMPLETE WORKFLOW SUCCESS: {workflow_complete}")
        
        return workflow_complete

    def test_protocol_safety_validation(self):
        """Test protocol safety validation for high-risk patients"""
        if not self.patient_id:
            print("‚ùå No patient ID available for protocol safety testing")
            return False

        # Test with high-risk patient scenario
        high_risk_protocol_data = {
            "patient_id": self.patient_id,
            "school_of_thought": "traditional_autologous"
        }

        print("   Testing protocol safety validation...")
        success, response = self.run_test(
            "CORE FEATURE: Protocol Safety Validation",
            "POST",
            "protocols/generate",
            200,
            data=high_risk_protocol_data,
            timeout=90
        )
        
        if success:
            protocol = response
            contraindications = protocol.get('contraindications', [])
            legal_warnings = protocol.get('legal_warnings', [])
            
            print(f"   ‚úÖ Safety Validation Completed")
            print(f"   Contraindications Identified: {len(contraindications)}")
            print(f"   Legal Warnings Generated: {len(legal_warnings)}")
            
            if contraindications:
                print(f"   Key Contraindications: {', '.join(contraindications[:2])}")
            
            if legal_warnings:
                print(f"   Legal Warnings: {', '.join(legal_warnings[:2])}")
            
            # Check for diabetes-specific safety considerations
            diabetes_safety = any('diabetes' in str(item).lower() for item in contraindications + legal_warnings)
            print(f"   Diabetes Safety Considerations: {'Detected' if diabetes_safety else 'Not specific'}")
            
            # Check confidence score adjustment for risk
            confidence = protocol.get('confidence_score', 0)
            print(f"   Risk-Adjusted Confidence: {confidence:.2f}")
            
        return success

    def run_core_medical_ai_tests(self):
        """Run the CORE MEDICAL AI FEATURES tests as requested"""
        print("üöÄ Starting CORE MEDICAL AI FEATURES Testing Suite")
        print("=" * 80)
        print("Testing the most critical features for medical AI system reliability")
        print("and practitioner trust in regenerative medicine applications.")
        print("=" * 80)
        
        # First ensure we have a patient for testing
        print("\nüìã SETUP: Creating Test Patient")
        print("-" * 50)
        patient_created = self.test_create_patient()
        
        if not patient_created:
            print("‚ùå CRITICAL: Cannot proceed without patient data")
            return False
        
        # Core Medical AI Features Tests
        print("\nüß† CORE MEDICAL AI FEATURES TESTS")
        print("-" * 50)
        
        # Test 1: Differential Diagnosis Generation
        test1_success = self.test_differential_diagnosis_generation()
        
        # Test 2: SHAP/LIME Explainable AI System  
        test2_success = self.test_shap_lime_explainable_ai()
        
        # Test 3: Safety Alerts & Contraindication Checking
        test3_success = self.test_safety_alerts_contraindication_checking()
        
        # Test 4: Comprehensive Patient Analysis Workflow
        test4_success = self.test_comprehensive_patient_analysis_workflow()
        
        # Test 5: Protocol Safety Validation
        test5_success = self.test_protocol_safety_validation()
        
        # Summary of Core Medical AI Features
        print("\n" + "=" * 80)
        print("üéØ CORE MEDICAL AI FEATURES TESTING SUMMARY")
        print("=" * 80)
        
        core_tests = [
            ("Differential Diagnosis Generation", test1_success),
            ("SHAP/LIME Explainable AI System", test2_success),
            ("Safety Alerts & Contraindication Checking", test3_success),
            ("Comprehensive Patient Analysis Workflow", test4_success),
            ("Protocol Safety Validation", test5_success)
        ]
        
        passed_core_tests = sum(1 for _, success in core_tests if success)
        total_core_tests = len(core_tests)
        
        print(f"Core Medical AI Features Tests: {passed_core_tests}/{total_core_tests} PASSED")
        print(f"Core Features Success Rate: {(passed_core_tests / total_core_tests * 100):.1f}%")
        
        for test_name, success in core_tests:
            status = "‚úÖ PASSED" if success else "‚ùå FAILED"
            print(f"   {status}: {test_name}")
        
        if passed_core_tests == total_core_tests:
            print("\nüéâ ALL CORE MEDICAL AI FEATURES ARE FUNCTIONAL!")
            print("The medical AI system is ready for practitioner use.")
        else:
            print(f"\n‚ö†Ô∏è  {total_core_tests - passed_core_tests} core features failed.")
            print("These are critical for medical AI system reliability.")
        
        return passed_core_tests == total_core_tests

    # ========== COMPREHENSIVE GAP FIX TESTING ==========

    def run_comprehensive_gap_fix_tests(self):
        """Run comprehensive tests for all gap fixes implemented"""
        print("\n" + "="*80)
        print("üéØ COMPREHENSIVE GAP FIX TESTING - VERIFYING 100% FUNCTIONALITY")
        print("="*80)
        
        # Test GAP 1 FIX: FILE PROCESSING WORKFLOW
        print("\nüìÅ GAP 1 FIX: FILE PROCESSING WORKFLOW TESTING")
        print("-" * 60)
        gap1_tests = [
            self.test_gap1_file_retrieval_categorization,
            self.test_gap1_file_reprocessing_ai_integration,
            self.test_gap1_file_upload_analysis_protocol_workflow,
            self.test_gap1_multimodal_file_integration
        ]
        gap1_passed = sum([test() for test in gap1_tests])
        print(f"üìÅ GAP 1 RESULTS: {gap1_passed}/{len(gap1_tests)} tests passed")
        
        # Test GAP 2 FIX: OUTCOME TRACKING SYSTEM
        print("\nüìä GAP 2 FIX: OUTCOME TRACKING SYSTEM TESTING")
        print("-" * 60)
        gap2_tests = [
            self.test_gap2_outcome_recording_calculations,
            self.test_gap2_outcome_retrieval_statistics,
            self.test_gap2_comprehensive_analytics,
            self.test_gap2_dashboard_analytics_real_data
        ]
        gap2_passed = sum([test() for test in gap2_tests])
        print(f"üìä GAP 2 RESULTS: {gap2_passed}/{len(gap2_tests)} tests passed")
        
        # Test GAP 3: PROTOCOL ENHANCEMENT STATUS CHECK
        print("\nüß¨ GAP 3: PROTOCOL ENHANCEMENT STATUS CHECK")
        print("-" * 60)
        gap3_tests = [
            self.test_gap3_protocol_generation_quality,
            self.test_gap3_evidence_citations_detail,
            self.test_gap3_clinical_sophistication_check,
            self.test_gap3_john_hudson_standards_compliance
        ]
        gap3_passed = sum([test() for test in gap3_tests])
        print(f"üß¨ GAP 3 RESULTS: {gap3_passed}/{len(gap3_tests)} tests passed")
        
        # Test WORKFLOW RELIABILITY VERIFICATION
        print("\n‚ö° WORKFLOW RELIABILITY VERIFICATION")
        print("-" * 60)
        reliability_tests = [
            self.test_protocol_generation_button_reliability,
            self.test_backend_connectivity_operations,
            self.test_error_handling_recovery,
            self.test_data_persistence_session_management
        ]
        reliability_passed = sum([test() for test in reliability_tests])
        print(f"‚ö° RELIABILITY RESULTS: {reliability_passed}/{len(reliability_tests)} tests passed")
        
        # Test INTEGRATION TESTING
        print("\nüîÑ INTEGRATION TESTING")
        print("-" * 60)
        integration_tests = [
            self.test_complete_patient_workflow,
            self.test_cross_platform_data_flow,
            self.test_dashboard_updates_new_activity,
            self.test_endpoints_seamless_integration
        ]
        integration_passed = sum([test() for test in integration_tests])
        print(f"üîÑ INTEGRATION RESULTS: {integration_passed}/{len(integration_tests)} tests passed")
        
        # Calculate overall results
        total_tests = len(gap1_tests) + len(gap2_tests) + len(gap3_tests) + len(reliability_tests) + len(integration_tests)
        total_passed = gap1_passed + gap2_passed + gap3_passed + reliability_passed + integration_passed
        
        print("\n" + "="*80)
        print(f"üéØ COMPREHENSIVE GAP FIX TESTING RESULTS: {total_passed}/{total_tests} ({total_passed/total_tests*100:.1f}%)")
        print("="*80)
        
        # Determine if 100% functionality achieved
        success_rate = total_passed / total_tests
        if success_rate >= 0.95:
            print("‚úÖ SUCCESS: 100% FUNCTIONALITY ACHIEVED - ALL GAPS FIXED!")
        elif success_rate >= 0.85:
            print("‚ö†Ô∏è  NEAR SUCCESS: Most gaps fixed, minor issues remain")
        else:
            print("‚ùå GAPS REMAIN: Significant functionality gaps still need attention")
        
        return success_rate >= 0.95

    # ========== GAP 1 FIX: FILE PROCESSING WORKFLOW TESTS ==========

    def test_gap1_file_retrieval_categorization(self):
        """Test GET /api/patients/{patient_id}/files - File retrieval and categorization"""
        if not self.patient_id:
            print("‚ùå No patient ID available for file retrieval testing")
            return False

        success, response = self.run_test(
            "GAP 1: File Retrieval & Categorization",
            "GET",
            f"patients/{self.patient_id}/files",
            200
        )
        
        if success:
            files_by_category = response.get('files_by_category', {})
            total_files = response.get('total_files', 0)
            categories = response.get('categories_present', [])
            
            print(f"   Total Files Retrieved: {total_files}")
            print(f"   Categories Present: {len(categories)} - {', '.join(categories)}")
            print(f"   File Categorization Working: {len(files_by_category) > 0}")
            
            # Check for expected categories
            expected_categories = ['chart', 'genetics', 'imaging', 'labs']
            found_categories = [cat for cat in expected_categories if cat in files_by_category]
            print(f"   Expected Categories Found: {len(found_categories)}/{len(expected_categories)}")
            
            # Verify file structure
            for category, files in files_by_category.items():
                if files:
                    sample_file = files[0]
                    has_required_fields = all(field in sample_file for field in ['filename', 'file_type', 'processing_status'])
                    print(f"   Category '{category}': {len(files)} files, structure valid: {has_required_fields}")
        
        return success

    def test_gap1_file_reprocessing_ai_integration(self):
        """Test POST /api/patients/{patient_id}/files/process-all - File reprocessing and AI integration"""
        if not self.patient_id:
            print("‚ùå No patient ID available for file reprocessing testing")
            return False

        print("   This may take 30-45 seconds for comprehensive file reprocessing...")
        success, response = self.run_test(
            "GAP 1: File Reprocessing & AI Integration",
            "POST",
            f"patients/{self.patient_id}/files/process-all",
            200,
            data={},
            timeout=60
        )
        
        if success:
            status = response.get('status', 'unknown')
            files_processed = response.get('files_processed', 0)
            categories_processed = response.get('categories_processed', [])
            analysis_updated = response.get('analysis_updated', False)
            
            print(f"   Processing Status: {status}")
            print(f"   Files Processed: {files_processed}")
            print(f"   Categories Processed: {len(categories_processed)} - {', '.join(categories_processed)}")
            print(f"   AI Analysis Updated: {analysis_updated}")
            
            # Verify reprocessing worked
            if status == 'files_reprocessed' and files_processed > 0:
                print(f"   ‚úÖ File reprocessing successful with AI integration")
            elif status == 'no_files':
                print(f"   ‚ö†Ô∏è  No files found for patient - expected if no files uploaded")
            else:
                print(f"   ‚ùå File reprocessing may have issues")
        
        return success

    def test_gap1_file_upload_analysis_protocol_workflow(self):
        """Test complete file upload ‚Üí analysis ‚Üí protocol integration workflow"""
        if not self.patient_id:
            print("‚ùå No patient ID available for workflow testing")
            return False

        # Step 1: Upload a comprehensive test file
        print("   Step 1: Uploading test file...")
        upload_success = self.test_file_upload_comprehensive_test()
        
        if not upload_success:
            print("   ‚ùå File upload failed - workflow cannot continue")
            return False
        
        # Step 2: Trigger file analysis
        print("   Step 2: Triggering comprehensive file analysis...")
        analysis_success, analysis_response = self.run_test(
            "Workflow Step 2: File Analysis",
            "GET",
            f"files/comprehensive-analysis/{self.patient_id}",
            200,
            timeout=45
        )
        
        if not analysis_success:
            print("   ‚ùå File analysis failed")
            return False
        
        # Step 3: Generate protocol using file insights
        print("   Step 3: Generating protocol with file integration...")
        protocol_success, protocol_response = self.run_test(
            "Workflow Step 3: File-Based Protocol Generation",
            "POST",
            f"protocols/generate-from-files?patient_id={self.patient_id}&school_of_thought=ai_optimized",
            200,
            data={},
            timeout=90
        )
        
        if protocol_success:
            protocol = protocol_response.get('protocol', {})
            files_used = protocol_response.get('file_insights_used', 0)
            enhancement_confidence = protocol_response.get('enhancement_confidence', 0)
            
            print(f"   Protocol Generated: {protocol.get('protocol_id', 'Unknown')}")
            print(f"   Files Integrated: {files_used}")
            print(f"   Enhancement Confidence: {enhancement_confidence:.2f}")
            print(f"   ‚úÖ Complete workflow successful: Upload ‚Üí Analysis ‚Üí Protocol")
            
            return True
        else:
            print("   ‚ùå Protocol generation with file integration failed")
            return False

    def test_gap1_multimodal_file_integration(self):
        """Test multi-modal file integration with protocol generation"""
        if not self.patient_id:
            print("‚ùå No patient ID available for multi-modal testing")
            return False

        # Upload multiple file types to test multi-modal integration
        print("   Uploading multiple file types for multi-modal testing...")
        
        file_types_uploaded = 0
        
        # Upload genetic data
        if self.test_file_upload_genetic_data():
            file_types_uploaded += 1
            print("   ‚úÖ Genetic data uploaded")
        
        # Upload lab results
        if self.test_file_upload_lab_results():
            file_types_uploaded += 1
            print("   ‚úÖ Lab results uploaded")
        
        # Upload DICOM imaging
        if self.test_file_upload_dicom_imaging():
            file_types_uploaded += 1
            print("   ‚úÖ DICOM imaging uploaded")
        
        print(f"   Multi-modal files uploaded: {file_types_uploaded}/3")
        
        if file_types_uploaded == 0:
            print("   ‚ùå No files uploaded - multi-modal test cannot proceed")
            return False
        
        # Test multi-modal analysis
        print("   Testing multi-modal analysis integration...")
        success, response = self.run_test(
            "Multi-Modal File Integration Analysis",
            "GET",
            f"files/comprehensive-analysis/{self.patient_id}",
            200,
            timeout=45
        )
        
        if success:
            analysis = response.get('comprehensive_analysis', {})
            multi_modal_insights = analysis.get('multi_modal_insights', {})
            confidence_level = analysis.get('confidence_level', 0)
            
            print(f"   Multi-Modal Insights Generated: {len(multi_modal_insights) > 0}")
            print(f"   Analysis Confidence: {confidence_level:.2f}")
            print(f"   ‚úÖ Multi-modal integration working")
            
            return True
        else:
            print("   ‚ùå Multi-modal analysis failed")
            return False

    def test_file_upload_comprehensive_test(self):
        """Upload a comprehensive test file for workflow testing"""
        if not self.patient_id:
            return False

        # Create comprehensive test data
        comprehensive_data = {
            "patient_id": self.patient_id,
            "test_type": "comprehensive_regenerative_assessment",
            "clinical_data": {
                "chief_complaint": "Bilateral knee osteoarthritis with functional limitation",
                "pain_scale": 7,
                "functional_assessment": "Moderate limitation in daily activities",
                "previous_treatments": ["NSAIDs", "Physical therapy", "Corticosteroid injections"]
            },
            "biomarkers": {
                "inflammatory_markers": {
                    "CRP": "3.2 mg/L",
                    "ESR": "22 mm/hr",
                    "IL-6": "4.1 pg/mL"
                },
                "regenerative_markers": {
                    "PDGF": "52 pg/mL",
                    "VEGF": "145 pg/mL",
                    "IGF-1": "195 ng/mL"
                }
            },
            "imaging_findings": {
                "modality": "MRI",
                "findings": "Grade 2-3 osteoarthritis with cartilage thinning and bone marrow edema",
                "regenerative_targets": ["Articular cartilage", "Subchondral bone", "Synovial membrane"]
            }
        }
        
        import json
        test_data = json.dumps(comprehensive_data).encode()
        
        files = {
            'file': ('comprehensive_assessment.json', test_data, 'application/json')
        }
        data = {
            'patient_id': self.patient_id,
            'file_category': 'chart'
        }
        
        upload_headers = {'Authorization': 'Bearer demo-token'}
        
        try:
            import requests
            response = requests.post(
                f"{self.api_url}/files/upload",
                files=files,
                data=data,
                headers=upload_headers,
                timeout=60
            )
            
            return response.status_code == 200
        except:
            return False

    # ========== GAP 2 FIX: OUTCOME TRACKING SYSTEM TESTS ==========

    def test_gap2_outcome_recording_calculations(self):
        """Test POST /api/patients/{patient_id}/outcomes - Outcome recording with calculations"""
        if not self.patient_id or not self.protocol_id:
            print("‚ùå No patient/protocol ID available for outcome recording testing")
            return False

        # Create comprehensive outcome data with calculations
        outcome_data = {
            "protocol_id": self.protocol_id,
            "patient_id": self.patient_id,
            "followup_date": datetime.utcnow().isoformat(),
            "measurements": {
                "pain_scale_baseline": 8,
                "pain_scale_current": 3,
                "pain_improvement_percentage": 62.5,
                "range_of_motion_baseline": 90,
                "range_of_motion_current": 135,
                "rom_improvement_degrees": 45,
                "functional_score_baseline": 45,
                "functional_score_current": 85,
                "functional_improvement_percentage": 88.9,
                "quality_of_life_score": 8.5
            },
            "calculated_outcomes": {
                "overall_improvement_score": 85.2,
                "treatment_success_indicator": True,
                "time_to_improvement_weeks": 4,
                "sustained_benefit_probability": 0.87
            },
            "practitioner_notes": "Excellent response to PRP treatment. Patient reports significant pain reduction and improved mobility. No adverse events. Recommend maintenance protocol in 6 months.",
            "patient_reported_outcomes": {
                "pain_improvement": "Much better - from 8/10 to 3/10",
                "activity_level": "Significantly improved - can walk stairs without pain",
                "satisfaction": "Very satisfied with results",
                "would_recommend": True,
                "return_to_activities": "90% return to normal activities"
            },
            "adverse_events": [],
            "satisfaction_score": 9,
            "follow_up_required": True,
            "next_assessment_date": (datetime.utcnow() + timedelta(weeks=12)).isoformat()
        }

        success, response = self.run_test(
            "GAP 2: Outcome Recording with Calculations",
            "POST",
            f"patients/{self.patient_id}/outcomes",
            200,
            data=outcome_data
        )
        
        if success:
            outcome_id = response.get('outcome_id', 'Unknown')
            calculated_metrics = response.get('calculated_metrics', {})
            improvement_score = response.get('improvement_score', 0)
            success_indicator = response.get('treatment_success', False)
            
            print(f"   Outcome ID: {outcome_id}")
            print(f"   Improvement Score: {improvement_score:.1f}%")
            print(f"   Treatment Success: {success_indicator}")
            print(f"   Calculated Metrics: {len(calculated_metrics)} metrics")
            print(f"   ‚úÖ Outcome recording with calculations working")
            
            # Store outcome ID for later tests
            self.outcome_id = outcome_id
            
            return True
        else:
            print("   ‚ùå Outcome recording failed")
            return False

    def test_gap2_outcome_retrieval_statistics(self):
        """Test GET /api/patients/{patient_id}/outcomes - Outcome retrieval and statistics"""
        if not self.patient_id:
            print("‚ùå No patient ID available for outcome retrieval testing")
            return False

        success, response = self.run_test(
            "GAP 2: Outcome Retrieval & Statistics",
            "GET",
            f"patients/{self.patient_id}/outcomes",
            200
        )
        
        if success:
            outcomes = response.get('outcomes', [])
            statistics = response.get('statistics', {})
            patient_id = response.get('patient_id', 'Unknown')
            
            print(f"   Patient ID: {patient_id}")
            print(f"   Outcomes Retrieved: {len(outcomes)}")
            
            if statistics:
                avg_improvement = statistics.get('average_improvement_score', 0)
                success_rate = statistics.get('treatment_success_rate', 0)
                total_followups = statistics.get('total_followups', 0)
                
                print(f"   Average Improvement Score: {avg_improvement:.1f}%")
                print(f"   Treatment Success Rate: {success_rate:.1f}%")
                print(f"   Total Follow-ups: {total_followups}")
                print(f"   ‚úÖ Outcome statistics calculation working")
            
            if outcomes:
                latest_outcome = outcomes[0]
                measurements = latest_outcome.get('measurements', {})
                calculated_outcomes = latest_outcome.get('calculated_outcomes', {})
                
                print(f"   Latest Outcome Measurements: {len(measurements)} metrics")
                print(f"   Calculated Outcomes: {len(calculated_outcomes)} calculations")
            
            return True
        else:
            print("   ‚ùå Outcome retrieval failed")
            return False

    def test_gap2_comprehensive_analytics(self):
        """Test GET /api/analytics/outcomes - Comprehensive analytics"""
        success, response = self.run_test(
            "GAP 2: Comprehensive Outcome Analytics",
            "GET",
            "analytics/outcomes",
            200,
            timeout=30
        )
        
        if success:
            analytics = response.get('analytics', {})
            summary_stats = response.get('summary_stats', {})
            therapy_performance = response.get('therapy_performance', {})
            trends = response.get('trends', {})
            
            print(f"   Analytics Categories: {len(analytics)} categories")
            
            if summary_stats:
                total_outcomes = summary_stats.get('total_outcomes_tracked', 0)
                avg_success_rate = summary_stats.get('overall_success_rate', 0)
                avg_improvement = summary_stats.get('average_improvement_score', 0)
                
                print(f"   Total Outcomes Tracked: {total_outcomes}")
                print(f"   Overall Success Rate: {avg_success_rate:.1f}%")
                print(f"   Average Improvement: {avg_improvement:.1f}%")
            
            if therapy_performance:
                therapies_analyzed = len(therapy_performance)
                print(f"   Therapies Analyzed: {therapies_analyzed}")
                
                # Show top performing therapy
                if therapy_performance:
                    top_therapy = max(therapy_performance.items(), key=lambda x: x[1].get('success_rate', 0))
                    therapy_name, performance = top_therapy
                    print(f"   Top Therapy: {therapy_name} ({performance.get('success_rate', 0):.1f}% success)")
            
            if trends:
                trend_categories = len(trends)
                print(f"   Trend Categories: {trend_categories}")
            
            print(f"   ‚úÖ Comprehensive analytics working")
            return True
        else:
            print("   ‚ùå Comprehensive analytics failed")
            return False

    def test_gap2_dashboard_analytics_real_data(self):
        """Test updated dashboard analytics with real outcome data"""
        success, response = self.run_test(
            "GAP 2: Dashboard Analytics with Real Data",
            "GET",
            "analytics/dashboard",
            200
        )
        
        if success:
            summary_stats = response.get('summary_stats', {})
            recent_activities = response.get('recent_activities', [])
            platform_insights = response.get('platform_insights', {})
            
            # Check for real outcome data integration
            outcomes_tracked = summary_stats.get('outcomes_tracked', 0)
            success_rate = platform_insights.get('protocol_success_rate', 0)
            
            print(f"   Outcomes Tracked: {outcomes_tracked}")
            print(f"   Protocol Success Rate: {success_rate}%")
            print(f"   Recent Activities: {len(recent_activities)}")
            
            # Check if dashboard shows dynamic data
            total_patients = summary_stats.get('total_patients', 0)
            protocols_generated = summary_stats.get('protocols_generated', 0)
            
            print(f"   Total Patients: {total_patients}")
            print(f"   Protocols Generated: {protocols_generated}")
            
            # Verify real-time updates
            if outcomes_tracked > 0:
                print(f"   ‚úÖ Dashboard showing real outcome data")
            else:
                print(f"   ‚ö†Ô∏è  Dashboard may not be showing real outcome data yet")
            
            return True
        else:
            print("   ‚ùå Dashboard analytics failed")
            return False

    # ========== CRITICAL FIXES VERIFICATION TESTS ==========

    def test_file_reprocessing_api_fix(self):
        """Test the critical fix for file reprocessing API parameter issue"""
        if not self.patient_id:
            print("‚ùå No patient ID available for file reprocessing testing")
            return False

        print("   Testing the CRITICAL FIX for RegenerativeMedicineAI.analyze_patient_data() parameter mismatch...")
        success, response = self.run_test(
            "CRITICAL FIX: File Reprocessing API Parameter Issue",
            "POST",
            f"patients/{self.patient_id}/files/process-all",
            200,
            data={},
            timeout=60
        )
        
        if success:
            print(f"   ‚úÖ FIXED: File reprocessing API now works without 500 errors")
            print(f"   Status: {response.get('status', 'unknown')}")
            print(f"   Files Processed: {response.get('files_processed', 0)}")
            print(f"   Analysis Updated: {response.get('analysis_updated', False)}")
            print(f"   Categories Processed: {response.get('categories_processed', [])}")
        else:
            print(f"   ‚ùå STILL BROKEN: File reprocessing API still has parameter issues")
        return success

    def test_dashboard_analytics_fix(self):
        """Test the critical fix for dashboard analytics 500 error"""
        print("   Testing the CRITICAL FIX for dashboard analytics 500 error...")
        success, response = self.run_test(
            "CRITICAL FIX: Dashboard Analytics 500 Error",
            "GET",
            "analytics/dashboard",
            200,
            timeout=30
        )
        
        if success:
            print(f"   ‚úÖ FIXED: Dashboard analytics now works without 500 errors")
            stats = response.get('summary_stats', {})
            print(f"   Total Patients: {stats.get('total_patients', 0)}")
            print(f"   Protocols Generated: {stats.get('protocols_generated', 0)}")
            print(f"   Outcomes Tracked: {stats.get('outcomes_tracked', 0)}")
            print(f"   Recent Activities: {len(response.get('recent_activities', []))}")
            print(f"   Platform Insights: {len(response.get('platform_insights', {}))}")
        else:
            print(f"   ‚ùå STILL BROKEN: Dashboard analytics still returns 500 errors")
        return success

    def test_outcome_tracking_comprehensive(self):
        """Test comprehensive outcome tracking system functionality"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for outcome tracking testing")
            return False

        # Test 1: Submit outcome
        outcome_data = {
            "protocol_id": self.protocol_id,
            "patient_id": self.patient_id,
            "followup_date": datetime.utcnow().isoformat(),
            "measurements": {
                "pain_scale_before": 8,
                "pain_scale_after": 3,
                "range_of_motion_before": 45,
                "range_of_motion_after": 85,
                "functional_score": 88,
                "improvement_percentage": 70
            },
            "practitioner_notes": "Excellent response to regenerative therapy. Patient reports significant pain reduction and improved mobility.",
            "patient_reported_outcomes": {
                "pain_improvement": "70%",
                "activity_level": "much improved",
                "satisfaction": "very satisfied",
                "return_to_activities": "yes"
            },
            "adverse_events": [],
            "satisfaction_score": 9
        }

        print("   Testing comprehensive outcome tracking workflow...")
        success1, response1 = self.run_test(
            "Outcome Recording with Calculations",
            "POST",
            "outcomes",
            200,
            data=outcome_data
        )
        
        outcome_id = None
        if success1:
            outcome_id = response1.get('outcome_id')
            print(f"   ‚úÖ Outcome recorded successfully: {outcome_id}")
            print(f"   Pain Reduction: {response1.get('measurements', {}).get('pain_reduction_percentage', 0)}%")
            print(f"   Functional Improvement: {response1.get('measurements', {}).get('functional_improvement', 0)}%")

        # Test 2: Get outcomes for patient
        success2, response2 = self.run_test(
            "Outcome Retrieval & Statistics",
            "GET",
            f"patients/{self.patient_id}/outcomes",
            200
        )
        
        if success2:
            outcomes = response2.get('outcomes', [])
            stats = response2.get('statistics', {})
            print(f"   ‚úÖ Retrieved {len(outcomes)} outcomes")
            print(f"   Average Pain Reduction: {stats.get('avg_pain_reduction', 0)}%")
            print(f"   Success Rate: {stats.get('success_rate', 0)}%")

        # Test 3: Get comprehensive analytics
        success3, response3 = self.run_test(
            "Comprehensive Analytics",
            "GET",
            "analytics/outcomes",
            200
        )
        
        if success3:
            analytics = response3.get('analytics', {})
            print(f"   ‚úÖ Analytics generated successfully")
            print(f"   Total Outcomes Tracked: {analytics.get('total_outcomes', 0)}")
            print(f"   Overall Success Rate: {analytics.get('overall_success_rate', 0)}%")
            print(f"   Average Pain Reduction: {analytics.get('avg_pain_reduction', 0)}%")
            print(f"   Patient Satisfaction: {analytics.get('avg_satisfaction', 0)}/10")

        # Test 4: Dashboard integration (this was failing before)
        success4, response4 = self.run_test(
            "Dashboard Analytics Integration",
            "GET",
            "analytics/dashboard",
            200
        )
        
        if success4:
            dashboard_stats = response4.get('summary_stats', {})
            print(f"   ‚úÖ Dashboard integration working")
            print(f"   Dashboard Outcomes: {dashboard_stats.get('outcomes_tracked', 0)}")
        else:
            print(f"   ‚ùå Dashboard integration still failing")

        return success1 and success2 and success3 and success4

    def test_complete_workflow_validation(self):
        """Test complete patient workflow: Create ‚Üí Upload Files ‚Üí Analysis ‚Üí Protocol ‚Üí Outcomes"""
        print("   Testing complete end-to-end workflow validation...")
        
        # Step 1: Create patient (already done in setup)
        if not self.patient_id:
            print("   ‚ùå No patient available for workflow testing")
            return False
        
        workflow_success = True
        
        # Step 2: Upload files (simulate multiple file types)
        print("   Step 2: Multi-modal file upload...")
        file_upload_success = (
            self.test_file_upload_patient_chart() and
            self.test_file_upload_genetic_data() and
            self.test_file_upload_lab_results()
        )
        
        if file_upload_success:
            print("   ‚úÖ Multi-modal file upload successful")
        else:
            print("   ‚ùå File upload workflow has issues")
            workflow_success = False
        
        # Step 3: Comprehensive analysis
        print("   Step 3: AI analysis with file integration...")
        analysis_success, analysis_response = self.run_test(
            "AI Analysis with File Integration",
            "POST",
            f"patients/{self.patient_id}/analyze",
            200,
            timeout=90
        )
        
        if analysis_success:
            print("   ‚úÖ AI analysis completed successfully")
            diagnostic_results = analysis_response.get('diagnostic_results', [])
            print(f"   Generated {len(diagnostic_results)} diagnostic results")
        else:
            print("   ‚ùå AI analysis failed")
            workflow_success = False
        
        # Step 4: Protocol generation
        print("   Step 4: Evidence-based protocol generation...")
        protocol_success, protocol_response = self.run_test(
            "Evidence-Based Protocol Generation",
            "POST",
            "protocols/generate",
            200,
            data={
                "patient_id": self.patient_id,
                "school_of_thought": "ai_optimized"
            },
            timeout=90
        )
        
        if protocol_success:
            print("   ‚úÖ Protocol generation successful")
            self.protocol_id = protocol_response.get('protocol_id')
            print(f"   Protocol confidence: {protocol_response.get('confidence_score', 0):.2f}")
        else:
            print("   ‚ùå Protocol generation failed")
            workflow_success = False
        
        # Step 5: Outcome tracking
        print("   Step 5: Outcome tracking and analytics...")
        if self.protocol_id:
            outcome_success = self.test_outcome_tracking_comprehensive()
            if outcome_success:
                print("   ‚úÖ Outcome tracking workflow successful")
            else:
                print("   ‚ùå Outcome tracking workflow failed")
                workflow_success = False
        else:
            print("   ‚ùå No protocol ID for outcome testing")
            workflow_success = False
        
        # Step 6: Dashboard updates
        print("   Step 6: Dashboard real-time updates...")
        dashboard_success, dashboard_response = self.run_test(
            "Dashboard Updates with New Activity",
            "GET",
            "analytics/dashboard",
            200
        )
        
        if dashboard_success:
            print("   ‚úÖ Dashboard updates working")
            stats = dashboard_response.get('summary_stats', {})
            print(f"   Real-time metrics: {stats.get('total_patients', 0)} patients, {stats.get('protocols_generated', 0)} protocols")
        else:
            print("   ‚ùå Dashboard updates failing")
            workflow_success = False
        
        return workflow_success

    def test_final_functionality_assessment(self):
        """Final comprehensive functionality assessment"""
        print("   Conducting final 100% functionality assessment...")
        
        assessment_results = {
            "file_processing": False,
            "outcome_tracking": False,
            "protocol_generation": False,
            "dashboard_analytics": False,
            "workflow_integration": False
        }
        
        # Test 1: File processing workflow
        print("   Assessment 1: File processing workflow...")
        file_success = self.test_file_reprocessing_api_fix()
        assessment_results["file_processing"] = file_success
        
        # Test 2: Outcome tracking system
        print("   Assessment 2: Outcome tracking system...")
        outcome_success = self.test_outcome_tracking_comprehensive()
        assessment_results["outcome_tracking"] = outcome_success
        
        # Test 3: Protocol generation reliability
        print("   Assessment 3: Protocol generation reliability...")
        protocol_tests = [
            self.test_generate_protocol_ai_optimized(),
            self.test_generate_protocol_traditional(),
            self.test_generate_protocol_biologics()
        ]
        protocol_success = sum(protocol_tests) >= 2  # At least 2/3 must pass
        assessment_results["protocol_generation"] = protocol_success
        
        # Test 4: Dashboard analytics
        print("   Assessment 4: Dashboard analytics...")
        dashboard_success = self.test_dashboard_analytics_fix()
        assessment_results["dashboard_analytics"] = dashboard_success
        
        # Test 5: Complete workflow integration
        print("   Assessment 5: Complete workflow integration...")
        workflow_success = self.test_complete_workflow_validation()
        assessment_results["workflow_integration"] = workflow_success
        
        # Calculate overall functionality percentage
        total_systems = len(assessment_results)
        working_systems = sum(assessment_results.values())
        functionality_percentage = (working_systems / total_systems) * 100
        
        print(f"\n   üìä FINAL FUNCTIONALITY ASSESSMENT:")
        print(f"   File Processing: {'‚úÖ' if assessment_results['file_processing'] else '‚ùå'}")
        print(f"   Outcome Tracking: {'‚úÖ' if assessment_results['outcome_tracking'] else '‚ùå'}")
        print(f"   Protocol Generation: {'‚úÖ' if assessment_results['protocol_generation'] else '‚ùå'}")
        print(f"   Dashboard Analytics: {'‚úÖ' if assessment_results['dashboard_analytics'] else '‚ùå'}")
        print(f"   Workflow Integration: {'‚úÖ' if assessment_results['workflow_integration'] else '‚ùå'}")
        print(f"   OVERALL FUNCTIONALITY: {functionality_percentage:.1f}%")
        
        return functionality_percentage >= 100.0

    def run_critical_fixes_verification(self):
        """Run focused testing on the two critical fixes"""
        print("üöÄ CRITICAL FIXES VERIFICATION - Testing Two Key Issues")
        print("=" * 80)
        
        # Setup: Create patient for testing
        print("\nüìã SETUP: Creating Test Patient")
        print("-" * 50)
        patient_created = self.test_create_patient()
        
        if not patient_created:
            print("‚ùå Cannot proceed without patient - setup failed")
            return
        
        # Upload some files for testing
        print("\nüìÅ SETUP: Uploading Test Files")
        print("-" * 50)
        self.test_file_upload_patient_chart()
        self.test_file_upload_genetic_data()
        self.test_file_upload_lab_results()
        
        # Generate a protocol for outcome testing
        print("\nüìã SETUP: Generating Test Protocol")
        print("-" * 50)
        self.test_generate_protocol_ai_optimized()
        
        # CRITICAL FIX TESTS
        print("\nüîß CRITICAL FIX 1: File Reprocessing API Parameter Issue")
        print("-" * 50)
        fix1_success = self.test_file_reprocessing_api_fix()
        
        print("\nüîß CRITICAL FIX 2: Dashboard Analytics 500 Error")
        print("-" * 50)
        fix2_success = self.test_dashboard_analytics_fix()
        
        print("\nüìä COMPREHENSIVE WORKFLOW VALIDATION")
        print("-" * 50)
        workflow_success = self.test_complete_workflow_validation()
        
        print("\nüéØ FINAL FUNCTIONALITY ASSESSMENT")
        print("-" * 50)
        final_assessment = self.test_final_functionality_assessment()
        
        # Final Results
        print("\n" + "=" * 80)
        print("üéØ CRITICAL FIXES VERIFICATION RESULTS")
        print("=" * 80)
        print(f"Total Tests Run: {self.tests_run}")
        print(f"Tests Passed: {self.tests_passed}")
        print(f"Tests Failed: {self.tests_run - self.tests_passed}")
        print(f"Success Rate: {(self.tests_passed / self.tests_run * 100):.1f}%")
        
        print(f"\nüîß CRITICAL FIXES STATUS:")
        print(f"Fix 1 - File Reprocessing API: {'‚úÖ RESOLVED' if fix1_success else '‚ùå STILL BROKEN'}")
        print(f"Fix 2 - Dashboard Analytics: {'‚úÖ RESOLVED' if fix2_success else '‚ùå STILL BROKEN'}")
        print(f"Complete Workflow: {'‚úÖ FUNCTIONAL' if workflow_success else '‚ùå ISSUES REMAIN'}")
        print(f"Final Assessment: {'‚úÖ 100% FUNCTIONALITY' if final_assessment else '‚ùå < 100% FUNCTIONALITY'}")
        
        if fix1_success and fix2_success and workflow_success and final_assessment:
            print("\nüéâ SUCCESS: All critical fixes verified and platform achieves 100% functionality!")
        elif fix1_success and fix2_success:
            print("\n‚úÖ GOOD: Critical fixes resolved but some workflow issues remain")
        else:
            print("\n‚ùå CRITICAL: Major issues still exist that prevent 100% functionality")
        
        print("=" * 80)

    # ========== PHASE 2: AI CLINICAL INTELLIGENCE TESTING ==========

    def test_clinical_intelligence_status(self):
        """Test Phase 2: AI Clinical Intelligence system status"""
        success, response = self.run_test(
            "Phase 2: AI Clinical Intelligence Status",
            "GET",
            "ai/clinical-intelligence-status",
            200
        )
        
        if success:
            print(f"   Phase: {response.get('phase', 'Unknown')}")
            print(f"   Overall Status: {response.get('overall_status', 'Unknown')}")
            
            component_status = response.get('component_status', {})
            print(f"   Visual Explainable AI: {component_status.get('visual_explainable_ai', {}).get('status', 'unknown')}")
            print(f"   Comparative Analytics: {component_status.get('comparative_effectiveness_analytics', {}).get('status', 'unknown')}")
            print(f"   Risk Assessment: {component_status.get('personalized_risk_assessment', {}).get('status', 'unknown')}")
            
            usage_stats = response.get('usage_statistics', {})
            print(f"   Visual Explanations: {usage_stats.get('visual_explanations_generated', 0)}")
            print(f"   Treatment Comparisons: {usage_stats.get('treatment_comparisons_performed', 0)}")
            print(f"   Risk Assessments: {usage_stats.get('risk_assessments_completed', 0)}")
            
            capabilities = response.get('capabilities', [])
            print(f"   Capabilities Available: {len(capabilities)}")
        return success

    def test_visual_explainable_ai_generation(self):
        """Test Visual Explainable AI system with SHAP/LIME analysis"""
        if not self.patient_id:
            print("‚ùå No patient ID available for visual explanation testing")
            return False

        # Sample prediction data for osteoarthritis patient
        prediction_data = {
            "model_prediction": {
                "primary_diagnosis": "Osteoarthritis bilateral knee",
                "confidence_score": 0.92,
                "treatment_recommendation": "PRP therapy",
                "success_probability": 0.85
            },
            "feature_values": {
                "age": 58,
                "symptom_duration_months": 36,
                "pain_scale": 7,
                "functional_limitation": 0.6,
                "inflammatory_markers": 0.3,
                "previous_treatments": 3,
                "activity_level": 0.4
            }
        }

        patient_data = {
            "patient_id": self.patient_id,
            "demographics": {
                "age": 58,
                "gender": "Female",
                "occupation": "Physician"
            },
            "medical_history": ["Osteoarthritis", "Hypertension"],
            "current_symptoms": ["bilateral knee pain", "morning stiffness"]
        }

        print("   This may take 30-45 seconds for SHAP/LIME analysis...")
        success, response = self.run_test(
            "Generate Visual AI Explanation (SHAP/LIME)",
            "POST",
            "ai/visual-explanation",
            200,
            data={
                "prediction_data": prediction_data,
                "patient_data": patient_data
            },
            timeout=60
        )
        
        if success:
            visual_explanation = response.get('visual_explanation', {})
            print(f"   Explanation ID: {visual_explanation.get('explanation_id', 'Unknown')}")
            print(f"   Explanation Type: {visual_explanation.get('explanation_type', 'Unknown')}")
            
            feature_importance = visual_explanation.get('feature_importance_analysis', {})
            print(f"   Feature Factors Analyzed: {len(feature_importance.get('feature_contributions', []))}")
            print(f"   Base Value: {feature_importance.get('base_value', 0):.2f}")
            print(f"   Final Prediction: {feature_importance.get('final_prediction', 0):.2f}")
            
            clinical_interpretation = visual_explanation.get('clinical_interpretation', {})
            print(f"   Transparency Score: {clinical_interpretation.get('transparency_score', 0):.2f}")
            print(f"   Explanation Confidence: {clinical_interpretation.get('explanation_confidence', 0):.2f}")
            
            # Store explanation ID for retrieval test
            self.explanation_id = visual_explanation.get('explanation_id')
        return success

    def test_get_visual_explanation(self):
        """Test retrieving stored visual explanation"""
        if not hasattr(self, 'explanation_id') or not self.explanation_id:
            print("‚ùå No explanation ID available for retrieval testing")
            return False

        success, response = self.run_test(
            "Get Visual Explanation Details",
            "GET",
            f"ai/visual-explanation/{self.explanation_id}",
            200
        )
        
        if success:
            print(f"   Retrieved Explanation ID: {response.get('explanation_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Generated At: {response.get('generated_at', 'Unknown')}")
            
            feature_importance = response.get('feature_importance_analysis', {})
            contributions = feature_importance.get('feature_contributions', [])
            if contributions:
                top_feature = contributions[0]
                print(f"   Top Feature: {top_feature.get('feature_name', 'Unknown')} (importance: {top_feature.get('importance_score', 0):.3f})")
        return success

    def test_treatment_comparison_analysis(self):
        """Test Comparative Effectiveness Analytics with multiple treatments"""
        comparison_request = {
            "condition": "osteoarthritis",
            "treatments": ["PRP", "BMAC", "stem_cells"],
            "patient_profile": {
                "age": 58,
                "severity": "moderate",
                "previous_treatments": ["NSAIDs", "physical_therapy"],
                "comorbidities": ["hypertension"]
            },
            "comparison_criteria": [
                "efficacy", "safety", "cost_effectiveness", "durability"
            ],
            "time_horizon": "12_months"
        }

        print("   This may take 30-45 seconds for comprehensive treatment analysis...")
        success, response = self.run_test(
            "Comparative Treatment Effectiveness Analysis",
            "POST",
            "analytics/treatment-comparison",
            200,
            data=comparison_request,
            timeout=60
        )
        
        if success:
            comparison_report = response.get('comparison_report', {})
            print(f"   Comparison ID: {comparison_report.get('comparison_id', 'Unknown')}")
            print(f"   Condition: {comparison_report.get('condition', 'Unknown')}")
            print(f"   Treatments Compared: {len(comparison_report.get('treatments_analyzed', []))}")
            
            head_to_head = comparison_report.get('head_to_head_analysis', {})
            print(f"   Pairwise Comparisons: {len(head_to_head.get('pairwise_comparisons', []))}")
            
            cost_effectiveness = comparison_report.get('cost_effectiveness_analysis', {})
            print(f"   Cost-Effectiveness Calculated: {cost_effectiveness.get('analysis_completed', False)}")
            
            network_meta = comparison_report.get('network_meta_analysis', {})
            print(f"   Network Meta-Analysis: {network_meta.get('analysis_type', 'Unknown')}")
            
            treatment_ranking = comparison_report.get('treatment_ranking', {})
            rankings = treatment_ranking.get('ranked_treatments', [])
            if rankings:
                top_treatment = rankings[0]
                print(f"   Top Ranked Treatment: {top_treatment.get('treatment', 'Unknown')} (score: {top_treatment.get('overall_score', 0):.2f})")
            
            # Store comparison ID for retrieval test
            self.comparison_id = comparison_report.get('comparison_id')
        return success

    def test_get_treatment_comparison(self):
        """Test retrieving stored treatment comparison analysis"""
        if not hasattr(self, 'comparison_id') or not self.comparison_id:
            print("‚ùå No comparison ID available for retrieval testing")
            return False

        success, response = self.run_test(
            "Get Treatment Comparison Details",
            "GET",
            f"analytics/treatment-comparison/{self.comparison_id}",
            200
        )
        
        if success:
            print(f"   Retrieved Comparison ID: {response.get('comparison_id', 'Unknown')}")
            print(f"   Analysis Date: {response.get('analysis_date', 'Unknown')}")
            print(f"   Treatments: {', '.join(response.get('treatments_analyzed', []))}")
            
            recommendations = response.get('clinical_recommendations', {})
            print(f"   Recommendations Generated: {len(recommendations.get('treatment_recommendations', []))}")
        return success

    def test_treatment_effectiveness_data(self):
        """Test getting treatment effectiveness data for specific condition"""
        success, response = self.run_test(
            "Treatment Effectiveness Data - Osteoarthritis",
            "GET",
            "analytics/treatment-effectiveness/osteoarthritis?treatment=PRP&time_horizon=6_months",
            200
        )
        
        if success:
            print(f"   Condition: {response.get('condition', 'Unknown')}")
            print(f"   Time Horizon: {response.get('time_horizon', 'Unknown')}")
            
            effectiveness_data = response.get('effectiveness_data', {})
            print(f"   Effectiveness Data Available: {len(effectiveness_data)}")
            
            if effectiveness_data:
                # Show sample effectiveness metrics
                for treatment, data in list(effectiveness_data.items())[:2]:
                    print(f"   {treatment}: Success Rate {data.get('success_rate', 0):.1%}, Cost ${data.get('average_cost', 0):,}")
        return success

    def test_comprehensive_risk_assessment(self):
        """Test Personalized Risk Assessment with comprehensive patient data"""
        if not self.patient_id:
            print("‚ùå No patient ID available for risk assessment testing")
            return False

        patient_data = {
            "patient_id": self.patient_id,
            "demographics": {
                "age": 58,
                "gender": "Female",
                "bmi": 26.5,
                "occupation": "Physician"
            },
            "medical_history": {
                "conditions": ["Osteoarthritis", "Hypertension", "Hypothyroidism"],
                "surgeries": [],
                "allergies": ["NKDA"]
            },
            "current_medications": ["Lisinopril", "Levothyroxine", "Ibuprofen PRN"],
            "lifestyle_factors": {
                "smoking_status": "never",
                "alcohol_consumption": "moderate",
                "exercise_level": "low_due_to_pain",
                "stress_level": "moderate"
            },
            "laboratory_results": {
                "inflammatory_markers": {
                    "CRP": 2.1,
                    "ESR": 18
                },
                "metabolic_panel": {
                    "glucose": 95,
                    "HbA1c": 5.4
                }
            },
            "functional_assessments": {
                "pain_scale": 7,
                "mobility_score": 0.6,
                "quality_of_life": 0.5
            }
        }

        treatment_plan = {
            "treatment_type": "PRP_therapy",
            "treatment_details": {
                "injection_sites": ["bilateral_knees"],
                "planned_sessions": 3,
                "interval_weeks": 4
            },
            "adjunct_therapies": ["physical_therapy", "activity_modification"]
        }

        print("   This may take 30-45 seconds for comprehensive risk analysis...")
        success, response = self.run_test(
            "Comprehensive Personalized Risk Assessment",
            "POST",
            "ai/risk-assessment",
            200,
            data={
                "patient_data": patient_data,
                "treatment_plan": treatment_plan
            },
            timeout=60
        )
        
        if success:
            risk_assessment = response.get('risk_assessment', {})
            print(f"   Assessment ID: {risk_assessment.get('assessment_id', 'Unknown')}")
            
            individual_risks = risk_assessment.get('individual_risk_assessments', {})
            print(f"   Risk Categories Assessed: {len(individual_risks)}")
            
            treatment_success = individual_risks.get('treatment_success', {})
            print(f"   Treatment Success Probability: {treatment_success.get('predicted_success_probability', 0):.1%}")
            
            adverse_events = individual_risks.get('adverse_events', {})
            print(f"   Overall Adverse Event Risk: {adverse_events.get('overall_adverse_event_risk', 0):.1%}")
            
            overall_stratification = risk_assessment.get('overall_risk_stratification', {})
            print(f"   Risk Category: {overall_stratification.get('overall_risk_category', 'Unknown')}")
            print(f"   Risk-Benefit Ratio: {overall_stratification.get('risk_benefit_ratio', 0):.1f}")
            
            monitoring_plan = risk_assessment.get('personalized_monitoring_plan', {})
            print(f"   Monitoring Intervals: {len(monitoring_plan.get('monitoring_schedule', []))}")
            
            # Store assessment ID for retrieval test
            self.assessment_id = risk_assessment.get('assessment_id')
        return success

    def test_get_risk_assessment(self):
        """Test retrieving stored risk assessment"""
        if not hasattr(self, 'assessment_id') or not self.assessment_id:
            print("‚ùå No assessment ID available for retrieval testing")
            return False

        success, response = self.run_test(
            "Get Risk Assessment Details",
            "GET",
            f"ai/risk-assessment/{self.assessment_id}",
            200
        )
        
        if success:
            print(f"   Retrieved Assessment ID: {response.get('assessment_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Assessment Date: {response.get('assessment_date', 'Unknown')}")
            
            risk_factors = response.get('risk_factors_identified', [])
            print(f"   Risk Factors Identified: {len(risk_factors)}")
            
            protective_factors = response.get('protective_factors_identified', [])
            print(f"   Protective Factors: {len(protective_factors)}")
        return success

    def test_patient_cohort_risk_stratification(self):
        """Test risk stratification for patient cohort"""
        # Create sample patient cohort for testing
        patient_cohort = [
            {
                "patient_id": f"test_patient_{i}",
                "demographics": {"age": 45 + i*5, "gender": "Female" if i%2 else "Male", "bmi": 24 + i},
                "medical_history": {"conditions": ["Osteoarthritis"]},
                "functional_assessments": {"pain_scale": 5 + i, "mobility_score": 0.8 - i*0.1}
            }
            for i in range(5)  # Test with 5 patients
        ]

        success, response = self.run_test(
            "Patient Cohort Risk Stratification",
            "POST",
            "ai/risk-stratification",
            200,
            data={
                "patient_cohort": patient_cohort,
                "treatment_type": "PRP_therapy"
            },
            timeout=90
        )
        
        if success:
            stratification_results = response.get('stratification_results', [])
            print(f"   Cohort Size: {response.get('cohort_size', 0)}")
            print(f"   Successful Assessments: {response.get('successful_assessments', 0)}")
            print(f"   Treatment Type: {response.get('treatment_type', 'Unknown')}")
            print(f"   Ranking Criteria: {response.get('ranking_criteria', 'Unknown')}")
            
            if stratification_results:
                top_candidate = stratification_results[0]
                print(f"   Top Candidate: {top_candidate.get('patient_id', 'Unknown')}")
                print(f"   Risk Category: {top_candidate.get('overall_risk_category', 'Unknown')}")
                print(f"   Success Probability: {top_candidate.get('treatment_success_probability', 0):.1%}")
                print(f"   Risk-Benefit Ratio: {top_candidate.get('risk_benefit_ratio', 0):.1f}")
        return success

    def run_phase2_ai_clinical_intelligence_tests(self):
        """Run comprehensive Phase 2: AI Clinical Intelligence tests"""
        print("üöÄ Starting Phase 2: AI Clinical Intelligence Test Suite")
        print("=" * 80)
        print("Testing world-class Visual Explainable AI, Comparative Effectiveness")
        print("Analytics, and Personalized Risk Assessment systems.")
        print("=" * 80)
        
        # Setup: Create patient for testing
        print("\nüìã SETUP: Creating Test Patient")
        print("-" * 50)
        patient_created = self.test_create_patient()
        
        if not patient_created:
            print("‚ùå Cannot proceed without patient - setup failed")
            return False
        
        # Phase 2 AI Clinical Intelligence Tests
        print("\nüéØ PHASE 2: AI CLINICAL INTELLIGENCE TESTS")
        print("-" * 50)
        
        # Test 1: System Status
        test1_success = self.test_clinical_intelligence_status()
        
        # Test 2: Visual Explainable AI Generation
        test2_success = self.test_visual_explainable_ai_generation()
        
        # Test 3: Get Visual Explanation
        test3_success = self.test_get_visual_explanation()
        
        # Test 4: Treatment Comparison Analysis
        test4_success = self.test_treatment_comparison_analysis()
        
        # Test 5: Get Treatment Comparison
        test5_success = self.test_get_treatment_comparison()
        
        # Test 6: Treatment Effectiveness Data
        test6_success = self.test_treatment_effectiveness_data()
        
        # Test 7: Comprehensive Risk Assessment
        test7_success = self.test_comprehensive_risk_assessment()
        
        # Test 8: Get Risk Assessment
        test8_success = self.test_get_risk_assessment()
        
        # Test 9: Patient Cohort Risk Stratification
        test9_success = self.test_patient_cohort_risk_stratification()
        
        # Summary of Phase 2 Tests
        print("\n" + "=" * 80)
        print("üéØ PHASE 2: AI CLINICAL INTELLIGENCE TESTING SUMMARY")
        print("=" * 80)
        
        phase2_tests = [
            ("Clinical Intelligence Status", test1_success),
            ("Visual Explainable AI Generation", test2_success),
            ("Get Visual Explanation", test3_success),
            ("Treatment Comparison Analysis", test4_success),
            ("Get Treatment Comparison", test5_success),
            ("Treatment Effectiveness Data", test6_success),
            ("Comprehensive Risk Assessment", test7_success),
            ("Get Risk Assessment", test8_success),
            ("Patient Cohort Risk Stratification", test9_success)
        ]
        
        passed_phase2_tests = sum(1 for _, success in phase2_tests if success)
        total_phase2_tests = len(phase2_tests)
        
        print(f"Phase 2 AI Clinical Intelligence Tests: {passed_phase2_tests}/{total_phase2_tests} PASSED")
        print(f"Phase 2 Success Rate: {(passed_phase2_tests / total_phase2_tests * 100):.1f}%")
        
        for test_name, success in phase2_tests:
            status = "‚úÖ PASSED" if success else "‚ùå FAILED"
            print(f"   {status}: {test_name}")
        
        if passed_phase2_tests == total_phase2_tests:
            print("\nüéâ ALL PHASE 2: AI CLINICAL INTELLIGENCE FEATURES ARE FUNCTIONAL!")
            print("The world-class AI clinical intelligence system is ready for production use.")
        else:
            print(f"\n‚ö†Ô∏è  {total_phase2_tests - passed_phase2_tests} Phase 2 features failed.")
            print("These are critical for world-class AI clinical intelligence.")
        
        return passed_phase2_tests == total_phase2_tests

    def run_critical_priority_features_tests(self):
        """Run tests for the three newly implemented Critical Priority features"""
        
        print("\nüî• CRITICAL PRIORITY FEATURES TESTING")
        print("=" * 80)
        print("Testing the three newly implemented 'Critical Priority' features:")
        print("1. Living Evidence Engine System")
        print("2. Advanced Differential Diagnosis System") 
        print("3. Enhanced Explainable AI System")
        print("=" * 80)
        
        # First ensure we have a patient and protocol for testing
        print("\nüìã SETUP: Creating Test Patient and Protocol")
        print("-" * 50)
        patient_created = self.test_create_patient()
        
        if patient_created:
            # Generate a protocol for testing
            protocol_data = {
                "patient_id": self.patient_id,
                "school_of_thought": "ai_optimized"
            }
            
            protocol_success, protocol_response = self.run_test(
                "Setup: Generate Protocol for Testing",
                "POST",
                "protocols/generate",
                200,
                data=protocol_data,
                timeout=90
            )
            
            if protocol_success:
                self.protocol_id = protocol_response.get('protocol_id')
                print(f"   ‚úÖ Test protocol created: {self.protocol_id}")
            else:
                print("   ‚ö†Ô∏è  Protocol creation failed - some tests may be limited")
        
        # Living Evidence Engine System Tests
        print("\nüß¨ LIVING EVIDENCE ENGINE SYSTEM TESTS")
        print("-" * 60)
        self.test_living_evidence_engine_living_map()
        self.test_living_evidence_engine_freshness_analysis()
        self.test_living_evidence_engine_update_mapping()
        self.test_living_evidence_engine_validate_links()
        
        # Advanced Differential Diagnosis System Tests
        print("\nü©∫ ADVANCED DIFFERENTIAL DIAGNOSIS SYSTEM TESTS")
        print("-" * 60)
        self.test_advanced_differential_diagnosis_comprehensive()
        self.test_advanced_differential_diagnosis_confidence_analysis()
        self.test_advanced_differential_diagnosis_tree()
        self.test_advanced_differential_diagnosis_precision_assessment()
        
        # Enhanced Explainable AI System Tests
        print("\nü§ñ ENHANCED EXPLAINABLE AI SYSTEM TESTS")
        print("-" * 60)
        self.test_enhanced_explainable_ai_explanation()
        self.test_enhanced_explainable_ai_visual_breakdown()
        self.test_enhanced_explainable_ai_feature_interactions()
        self.test_enhanced_explainable_ai_transparency_assessment()

    def run_phase3_global_knowledge_engine_tests(self):
        """Run comprehensive Phase 3: Global Knowledge Engine tests"""
        print("üåç Starting Phase 3: Global Knowledge Engine Test Suite")
        print("=" * 80)
        print("Testing Global Regulatory Intelligence, International Protocol Library,")
        print("and Community Collaboration Platform systems.")
        print("=" * 80)
        
        # Setup: Create patient for testing if needed
        if not self.patient_id:
            print("\nüìã SETUP: Creating Test Patient")
            print("-" * 50)
            patient_created = self.test_create_patient()
            
            if not patient_created:
                print("‚ùå Cannot proceed without patient - setup failed")
                return False
        
        # Phase 3 Global Knowledge Engine Tests
        print("\nüåç PHASE 3: GLOBAL KNOWLEDGE ENGINE TESTS")
        print("-" * 50)
        
        # Test 1: System Status
        test1_success = self.test_phase3_global_knowledge_system_status()
        
        # Test 2-6: Global Regulatory Intelligence
        test2_success = self.test_regulatory_treatment_status_prp()
        test3_success = self.test_regulatory_treatment_status_bmac()
        test4_success = self.test_regulatory_treatment_status_stem_cells()
        test5_success = self.test_regulatory_country_specific_us()
        test6_success = self.test_cross_jurisdictional_comparison()
        
        # Test 7-9: International Protocol Library
        test7_success = self.test_international_protocol_search_osteoarthritis()
        test8_success = self.test_international_protocol_search_multiple_traditions()
        test9_success = self.test_international_protocol_integration_levels()
        
        # Test 10-15: Community & Collaboration Platform
        test10_success = self.test_peer_consultation_emergency()
        test11_success = self.test_peer_consultation_routine()
        test12_success = self.test_share_protocol_public()
        test13_success = self.test_share_protocol_professional()
        test14_success = self.test_community_insights_collective_intelligence()
        test15_success = self.test_community_insights_therapy_comparison()
        
        # Summary of Phase 3 Tests
        print("\n" + "=" * 80)
        print("üåç PHASE 3: GLOBAL KNOWLEDGE ENGINE TESTING SUMMARY")
        print("=" * 80)
        
        phase3_tests = [
            ("Global Knowledge System Status", test1_success),
            ("Regulatory Treatment Status - PRP", test2_success),
            ("Regulatory Treatment Status - BMAC", test3_success),
            ("Regulatory Treatment Status - Stem Cells", test4_success),
            ("Regulatory Country Specific - US", test5_success),
            ("Cross-Jurisdictional Comparison", test6_success),
            ("International Protocol Search - Osteoarthritis", test7_success),
            ("International Protocol Search - Multiple Traditions", test8_success),
            ("International Protocol Integration Levels", test9_success),
            ("Peer Consultation - Emergency", test10_success),
            ("Peer Consultation - Routine", test11_success),
            ("Share Protocol - Public", test12_success),
            ("Share Protocol - Professional", test13_success),
            ("Community Insights - Collective Intelligence", test14_success),
            ("Community Insights - Therapy Comparison", test15_success)
        ]
        
        passed_phase3_tests = sum(1 for _, success in phase3_tests if success)
        total_phase3_tests = len(phase3_tests)
        
        print(f"Phase 3 Global Knowledge Engine Tests: {passed_phase3_tests}/{total_phase3_tests} PASSED")
        print(f"Phase 3 Success Rate: {(passed_phase3_tests / total_phase3_tests * 100):.1f}%")
        
        for test_name, success in phase3_tests:
            status = "‚úÖ PASSED" if success else "‚ùå FAILED"
            print(f"   {status}: {test_name}")
        
        if passed_phase3_tests == total_phase3_tests:
            print("\nüéâ ALL PHASE 3: GLOBAL KNOWLEDGE ENGINE FEATURES ARE FUNCTIONAL!")
            print("The world-class global knowledge engine system is ready for production use.")
        else:
            print(f"\n‚ö†Ô∏è  {total_phase3_tests - passed_phase3_tests} Phase 3 features failed.")
            print("These are critical for world-class global knowledge capabilities.")
        
        return passed_phase3_tests == total_phase3_tests

def main():
    print("üß¨ RegenMed AI Pro - Comprehensive Backend API Testing")
    print("Advanced Regenerative Medicine Knowledge Platform v2.0")
    print("Testing All Advanced AI Features & Services")
    print("=" * 70)
    
    # Initialize tester
    tester = RegenMedAIProTester()
    
    # Test with existing patient first
    print("\nüîç Testing with existing patient: c458d177-712c-4eb9-8fd3-5f5e41fe7b71 (Sarah Chen)")
    tester.patient_id = "c458d177-712c-4eb9-8fd3-5f5e41fe7b71"
    
    # Define comprehensive test suite with advanced features
    tests = [
        # Phase 1: System Health & Status
        ("System Health Check", tester.test_health_check),
        ("Root API Endpoint", tester.test_root_endpoint),
        ("Advanced System Status", tester.test_advanced_system_status),
        
        # Phase 2: Core Functionality with existing patient
        ("Therapy Database Access", tester.test_therapies_database),
        ("Get Existing Patient Details", tester.test_get_patient),
        ("AI Patient Analysis", tester.test_analyze_patient),
        
        # Phase 3: File Upload and Processing System
        ("File Upload - Patient Chart", tester.test_file_upload_patient_chart),
        ("File Upload - Genetic Data", tester.test_file_upload_genetic_data),
        ("File Upload - DICOM Imaging", tester.test_file_upload_dicom_imaging),
        ("File Upload - Lab Results", tester.test_file_upload_lab_results),
        ("Get Patient Files", tester.test_get_patient_files),
        ("Comprehensive File Analysis", tester.test_comprehensive_patient_analysis),
        ("File-Based Protocol Generation", tester.test_file_based_protocol_generation),
        
        # Phase 4: Protocol Generation
        ("Generate Protocol - Traditional", tester.test_generate_protocol_traditional),
        ("Generate Protocol - Biologics", tester.test_generate_protocol_biologics),
        ("Generate Protocol - AI Optimized", tester.test_generate_protocol_ai_optimized),
        ("Generate Protocol - Experimental", tester.test_generate_protocol_experimental),
        ("Get Protocol Details", tester.test_get_protocol),
        ("Approve Protocol", tester.test_approve_protocol),
        
        # Phase 5: Advanced AI Features - Federated Learning
        ("Federated Learning - Register Clinic", tester.test_federated_register_clinic),
        ("Federated Learning - Global Model Status", tester.test_federated_global_model_status),
        
        # Phase 6: Literature Integration
        ("Literature - Latest Updates", tester.test_literature_latest_updates),
        ("Literature - Search Database", tester.test_literature_search),
        
        # Phase 6.1: Google Scholar Integration System (NEW)
        ("Google Scholar - Basic Search", tester.test_google_scholar_search_basic),
        ("Google Scholar - Stem Cell Therapy", tester.test_google_scholar_search_stem_cell),
        ("Google Scholar - Year Filter", tester.test_google_scholar_search_with_year_filter),
        ("Google Scholar - Error Handling", tester.test_google_scholar_error_handling),
        ("Multi-Source - Comprehensive Search", tester.test_multi_source_search_comprehensive),
        ("Multi-Source - BMAC Rotator Cuff", tester.test_multi_source_search_bmac),
        ("Multi-Source - Deduplication Test", tester.test_multi_source_deduplication),
        ("Literature - Evidence Extraction", tester.test_literature_integration_evidence_extraction),
        
        # Phase 6.5: Evidence Synthesis System (NEW)
        ("Evidence Synthesis - System Status", tester.test_evidence_synthesis_status),
        ("Evidence Synthesis - Osteoarthritis Protocol", tester.test_evidence_synthesis_osteoarthritis),
        ("Evidence Synthesis - Rotator Cuff Protocol", tester.test_evidence_synthesis_rotator_cuff),
        ("Evidence Synthesis - Error Handling (Missing Condition)", tester.test_evidence_synthesis_missing_condition),
        ("Evidence Synthesis - Invalid Condition", tester.test_evidence_synthesis_invalid_condition),
        
        # Phase 6.7: ClinicalTrials.gov API Integration (NEW)
        ("Clinical Trials - Search Osteoarthritis", tester.test_clinical_trials_search_osteoarthritis),
        ("Clinical Trials - Search Rotator Cuff + Stem Cell", tester.test_clinical_trials_search_rotator_cuff_stem_cell),
        ("Clinical Trials - Search Knee Pain + PRP", tester.test_clinical_trials_search_knee_pain_prp),
        ("Clinical Trials - JSON API Parsing Quality", tester.test_clinical_trials_json_api_parsing),
        ("Clinical Trials - Intervention Categorization", tester.test_clinical_trials_intervention_categorization),
        ("Clinical Trials - Relevance Scoring Algorithm", tester.test_clinical_trials_relevance_scoring),
        ("Clinical Trials - Patient Matching (Osteoarthritis)", tester.test_clinical_trials_patient_matching_osteoarthritis),
        ("Clinical Trials - Patient Matching (Shoulder + BMAC)", tester.test_clinical_trials_patient_matching_shoulder_bmac),
        ("Clinical Trials - Match Scoring Algorithm", tester.test_clinical_trials_match_scoring_algorithm),
        ("Clinical Trials - Database Storage & Indexing", tester.test_clinical_trials_database_storage),
        ("Clinical Trials - Error Handling", tester.test_clinical_trials_error_handling),
        
        # Phase 7: ML Prediction & DICOM Processing
        ("ML Model Performance", tester.test_prediction_model_performance),
        ("ML Treatment Outcome Prediction", tester.test_treatment_outcome_prediction),
        ("DICOM Analysis (AI-Powered)", tester.test_dicom_analysis_simulation),
        ("Imaging Analysis History", tester.test_imaging_analysis_history),
        
        # Phase 8: Analytics & Outcomes
        ("Dashboard Analytics", tester.test_dashboard_analytics),
        ("Submit Outcome Data", tester.test_submit_outcome),
        
        # Phase 9: Create new patient for completeness
        ("Create New Patient Record", tester.test_create_patient),
        ("List All Patients", tester.test_list_patients),
    ]
    
    print(f"\nRunning {len(tests)} comprehensive API tests...")
    print("Including all advanced AI features: Federated Learning, Literature Integration, ML Prediction, DICOM Processing")
    print("\n")
    
    for test_name, test_func in tests:
        try:
            test_func()
        except Exception as e:
            print(f"‚ùå {test_name} failed with exception: {str(e)}")
    
    # Print final results
    print("\n" + "=" * 70)
    print(f"üìä Test Results: {tester.tests_passed}/{tester.tests_run} tests passed")
    print(f"üéØ Success Rate: {(tester.tests_passed/tester.tests_run)*100:.1f}%")
    
    if tester.tests_passed == tester.tests_run:
        print("üéâ All tests passed! RegenMed AI Pro Backend is fully functional.")
        print("‚úÖ All advanced AI services operational:")
        print("   ‚Ä¢ Federated Learning System")
        print("   ‚Ä¢ PubMed Literature Integration") 
        print("   ‚Ä¢ AI-Powered DICOM Processing")
        print("   ‚Ä¢ ML Outcome Prediction Models")
        print("‚úÖ Ready for comprehensive frontend integration testing.")
        return 0
    elif tester.tests_passed >= tester.tests_run * 0.8:
        print("‚ö†Ô∏è  Most tests passed. Minor issues detected - proceeding to frontend testing.")
        print("üîß Some advanced features may need attention but core functionality works.")
        return 0
    else:
        print("‚ùå Significant backend issues detected. Frontend testing may be impacted.")
        print("üö® Advanced AI services may not be fully operational.")
        return 1

    # ========== CRITICAL FIXES VERIFICATION TESTS ==========

    def test_file_reprocessing_api_fix(self):
        """Test the critical fix for file reprocessing API parameter issue"""
        if not self.patient_id:
            print("‚ùå No patient ID available for file reprocessing testing")
            return False

        print("   Testing the CRITICAL FIX for RegenerativeMedicineAI.analyze_patient_data() parameter mismatch...")
        success, response = self.run_test(
            "CRITICAL FIX: File Reprocessing API Parameter Issue",
            "POST",
            f"patients/{self.patient_id}/files/process-all",
            200,
            data={},
            timeout=60
        )
        
        if success:
            print(f"   ‚úÖ FIXED: File reprocessing API now works without 500 errors")
            print(f"   Status: {response.get('status', 'unknown')}")
            print(f"   Files Processed: {response.get('files_processed', 0)}")
            print(f"   Analysis Updated: {response.get('analysis_updated', False)}")
            print(f"   Categories Processed: {response.get('categories_processed', [])}")
        else:
            print(f"   ‚ùå STILL BROKEN: File reprocessing API still has parameter issues")
        return success

    def test_dashboard_analytics_fix(self):
        """Test the critical fix for dashboard analytics 500 error"""
        print("   Testing the CRITICAL FIX for dashboard analytics 500 error...")
        success, response = self.run_test(
            "CRITICAL FIX: Dashboard Analytics 500 Error",
            "GET",
            "analytics/dashboard",
            200,
            timeout=30
        )
        
        if success:
            print(f"   ‚úÖ FIXED: Dashboard analytics now works without 500 errors")
            stats = response.get('summary_stats', {})
            print(f"   Total Patients: {stats.get('total_patients', 0)}")
            print(f"   Protocols Generated: {stats.get('protocols_generated', 0)}")
            print(f"   Outcomes Tracked: {stats.get('outcomes_tracked', 0)}")
            print(f"   Recent Activities: {len(response.get('recent_activities', []))}")
            print(f"   Platform Insights: {len(response.get('platform_insights', {}))}")
        else:
            print(f"   ‚ùå STILL BROKEN: Dashboard analytics still returns 500 errors")
        return success

    def test_outcome_tracking_comprehensive(self):
        """Test comprehensive outcome tracking system functionality"""
        if not self.protocol_id:
            print("‚ùå No protocol ID available for outcome tracking testing")
            return False

        # Test 1: Submit outcome
        outcome_data = {
            "protocol_id": self.protocol_id,
            "patient_id": self.patient_id,
            "followup_date": datetime.utcnow().isoformat(),
            "measurements": {
                "pain_scale_before": 8,
                "pain_scale_after": 3,
                "range_of_motion_before": 45,
                "range_of_motion_after": 85,
                "functional_score": 88,
                "improvement_percentage": 70
            },
            "practitioner_notes": "Excellent response to regenerative therapy. Patient reports significant pain reduction and improved mobility.",
            "patient_reported_outcomes": {
                "pain_improvement": "70%",
                "activity_level": "much improved",
                "satisfaction": "very satisfied",
                "return_to_activities": "yes"
            },
            "adverse_events": [],
            "satisfaction_score": 9
        }

        print("   Testing comprehensive outcome tracking workflow...")
        success1, response1 = self.run_test(
            "Outcome Recording with Calculations",
            "POST",
            "outcomes",
            200,
            data=outcome_data
        )
        
        outcome_id = None
        if success1:
            outcome_id = response1.get('outcome_id')
            print(f"   ‚úÖ Outcome recorded successfully: {outcome_id}")
            print(f"   Pain Reduction: {response1.get('measurements', {}).get('pain_reduction_percentage', 0)}%")
            print(f"   Functional Improvement: {response1.get('measurements', {}).get('functional_improvement', 0)}%")

        # Test 2: Get outcomes for patient
        success2, response2 = self.run_test(
            "Outcome Retrieval & Statistics",
            "GET",
            f"patients/{self.patient_id}/outcomes",
            200
        )
        
        if success2:
            outcomes = response2.get('outcomes', [])
            stats = response2.get('statistics', {})
            print(f"   ‚úÖ Retrieved {len(outcomes)} outcomes")
            print(f"   Average Pain Reduction: {stats.get('avg_pain_reduction', 0)}%")
            print(f"   Success Rate: {stats.get('success_rate', 0)}%")

        # Test 3: Get comprehensive analytics
        success3, response3 = self.run_test(
            "Comprehensive Analytics",
            "GET",
            "analytics/outcomes",
            200
        )
        
        if success3:
            analytics = response3.get('analytics', {})
            print(f"   ‚úÖ Analytics generated successfully")
            print(f"   Total Outcomes Tracked: {analytics.get('total_outcomes', 0)}")
            print(f"   Overall Success Rate: {analytics.get('overall_success_rate', 0)}%")
            print(f"   Average Pain Reduction: {analytics.get('avg_pain_reduction', 0)}%")
            print(f"   Patient Satisfaction: {analytics.get('avg_satisfaction', 0)}/10")

        # Test 4: Dashboard integration (this was failing before)
        success4, response4 = self.run_test(
            "Dashboard Analytics Integration",
            "GET",
            "analytics/dashboard",
            200
        )
        
        if success4:
            dashboard_stats = response4.get('summary_stats', {})
            print(f"   ‚úÖ Dashboard integration working")
            print(f"   Dashboard Outcomes: {dashboard_stats.get('outcomes_tracked', 0)}")
        else:
            print(f"   ‚ùå Dashboard integration still failing")

        return success1 and success2 and success3 and success4

    def test_complete_workflow_validation(self):
        """Test complete patient workflow: Create ‚Üí Upload Files ‚Üí Analysis ‚Üí Protocol ‚Üí Outcomes"""
        print("   Testing complete end-to-end workflow validation...")
        
        # Step 1: Create patient (already done in setup)
        if not self.patient_id:
            print("   ‚ùå No patient available for workflow testing")
            return False
        
        workflow_success = True
        
        # Step 2: Upload files (simulate multiple file types)
        print("   Step 2: Multi-modal file upload...")
        file_upload_success = (
            self.test_file_upload_patient_chart() and
            self.test_file_upload_genetic_data() and
            self.test_file_upload_lab_results()
        )
        
        if file_upload_success:
            print("   ‚úÖ Multi-modal file upload successful")
        else:
            print("   ‚ùå File upload workflow has issues")
            workflow_success = False
        
        # Step 3: Comprehensive analysis
        print("   Step 3: AI analysis with file integration...")
        analysis_success, analysis_response = self.run_test(
            "AI Analysis with File Integration",
            "POST",
            f"patients/{self.patient_id}/analyze",
            200,
            timeout=90
        )
        
        if analysis_success:
            print("   ‚úÖ AI analysis completed successfully")
            diagnostic_results = analysis_response.get('diagnostic_results', [])
            print(f"   Generated {len(diagnostic_results)} diagnostic results")
        else:
            print("   ‚ùå AI analysis failed")
            workflow_success = False
        
        # Step 4: Protocol generation
        print("   Step 4: Evidence-based protocol generation...")
        protocol_success, protocol_response = self.run_test(
            "Evidence-Based Protocol Generation",
            "POST",
            "protocols/generate",
            200,
            data={
                "patient_id": self.patient_id,
                "school_of_thought": "ai_optimized"
            },
            timeout=90
        )
        
        if protocol_success:
            print("   ‚úÖ Protocol generation successful")
            self.protocol_id = protocol_response.get('protocol_id')
            print(f"   Protocol confidence: {protocol_response.get('confidence_score', 0):.2f}")
        else:
            print("   ‚ùå Protocol generation failed")
            workflow_success = False
        
        # Step 5: Outcome tracking
        print("   Step 5: Outcome tracking and analytics...")
        if self.protocol_id:
            outcome_success = self.test_outcome_tracking_comprehensive()
            if outcome_success:
                print("   ‚úÖ Outcome tracking workflow successful")
            else:
                print("   ‚ùå Outcome tracking workflow failed")
                workflow_success = False
        else:
            print("   ‚ùå No protocol ID for outcome testing")
            workflow_success = False
        
        # Step 6: Dashboard updates
        print("   Step 6: Dashboard real-time updates...")
        dashboard_success, dashboard_response = self.run_test(
            "Dashboard Updates with New Activity",
            "GET",
            "analytics/dashboard",
            200
        )
        
        if dashboard_success:
            print("   ‚úÖ Dashboard updates working")
            stats = dashboard_response.get('summary_stats', {})
            print(f"   Real-time metrics: {stats.get('total_patients', 0)} patients, {stats.get('protocols_generated', 0)} protocols")
        else:
            print("   ‚ùå Dashboard updates failing")
            workflow_success = False
        
        return workflow_success

    def test_final_functionality_assessment(self):
        """Final comprehensive functionality assessment"""
        print("   Conducting final 100% functionality assessment...")
        
        assessment_results = {
            "file_processing": False,
            "outcome_tracking": False,
            "protocol_generation": False,
            "dashboard_analytics": False,
            "workflow_integration": False
        }
        
        # Test 1: File processing workflow
        print("   Assessment 1: File processing workflow...")
        file_success = self.test_file_reprocessing_api_fix()
        assessment_results["file_processing"] = file_success
        
        # Test 2: Outcome tracking system
        print("   Assessment 2: Outcome tracking system...")
        outcome_success = self.test_outcome_tracking_comprehensive()
        assessment_results["outcome_tracking"] = outcome_success
        
        # Test 3: Protocol generation reliability
        print("   Assessment 3: Protocol generation reliability...")
        protocol_tests = [
            self.test_generate_protocol_ai_optimized(),
            self.test_generate_protocol_traditional(),
            self.test_generate_protocol_biologics()
        ]
        protocol_success = sum(protocol_tests) >= 2  # At least 2/3 must pass
        assessment_results["protocol_generation"] = protocol_success
        
        # Test 4: Dashboard analytics
        print("   Assessment 4: Dashboard analytics...")
        dashboard_success = self.test_dashboard_analytics_fix()
        assessment_results["dashboard_analytics"] = dashboard_success
        
        # Test 5: Complete workflow integration
        print("   Assessment 5: Complete workflow integration...")
        workflow_success = self.test_complete_workflow_validation()
        assessment_results["workflow_integration"] = workflow_success
        
        # Calculate overall functionality percentage
        total_systems = len(assessment_results)
        working_systems = sum(assessment_results.values())
        functionality_percentage = (working_systems / total_systems) * 100
        
        print(f"\n   üìä FINAL FUNCTIONALITY ASSESSMENT:")
        print(f"   File Processing: {'‚úÖ' if assessment_results['file_processing'] else '‚ùå'}")
        print(f"   Outcome Tracking: {'‚úÖ' if assessment_results['outcome_tracking'] else '‚ùå'}")
        print(f"   Protocol Generation: {'‚úÖ' if assessment_results['protocol_generation'] else '‚ùå'}")
        print(f"   Dashboard Analytics: {'‚úÖ' if assessment_results['dashboard_analytics'] else '‚ùå'}")
        print(f"   Workflow Integration: {'‚úÖ' if assessment_results['workflow_integration'] else '‚ùå'}")
        print(f"   OVERALL FUNCTIONALITY: {functionality_percentage:.1f}%")
        
        return functionality_percentage >= 100.0

    def run_critical_fixes_verification(self):
        """Run focused testing on the two critical fixes"""
        print("üöÄ CRITICAL FIXES VERIFICATION - Testing Two Key Issues")
        print("=" * 80)
        
        # Setup: Create patient for testing
        print("\nüìã SETUP: Creating Test Patient")
        print("-" * 50)
        patient_created = self.test_create_patient()
        
        if not patient_created:
            print("‚ùå Cannot proceed without patient - setup failed")
            return
        
        # Upload some files for testing
        print("\nüìÅ SETUP: Uploading Test Files")
        print("-" * 50)
        self.test_file_upload_patient_chart()
        self.test_file_upload_genetic_data()
        self.test_file_upload_lab_results()
        
        # Generate a protocol for outcome testing
        print("\nüìã SETUP: Generating Test Protocol")
        print("-" * 50)
        self.test_generate_protocol_ai_optimized()
        
        # CRITICAL FIX TESTS
        print("\nüîß CRITICAL FIX 1: File Reprocessing API Parameter Issue")
        print("-" * 50)
        fix1_success = self.test_file_reprocessing_api_fix()
        
        print("\nüîß CRITICAL FIX 2: Dashboard Analytics 500 Error")
        print("-" * 50)
        fix2_success = self.test_dashboard_analytics_fix()
        
        print("\nüìä COMPREHENSIVE WORKFLOW VALIDATION")
        print("-" * 50)
        workflow_success = self.test_complete_workflow_validation()
        
        print("\nüéØ FINAL FUNCTIONALITY ASSESSMENT")
        print("-" * 50)
        final_assessment = self.test_final_functionality_assessment()
        
        # Final Results
        print("\n" + "=" * 80)
        print("üéØ CRITICAL FIXES VERIFICATION RESULTS")
        print("=" * 80)
        print(f"Total Tests Run: {self.tests_run}")
        print(f"Tests Passed: {self.tests_passed}")
        print(f"Tests Failed: {self.tests_run - self.tests_passed}")
        print(f"Success Rate: {(self.tests_passed / self.tests_run * 100):.1f}%")
        
        print(f"\nüîß CRITICAL FIXES STATUS:")
        print(f"Fix 1 - File Reprocessing API: {'‚úÖ RESOLVED' if fix1_success else '‚ùå STILL BROKEN'}")
        print(f"Fix 2 - Dashboard Analytics: {'‚úÖ RESOLVED' if fix2_success else '‚ùå STILL BROKEN'}")
        print(f"Complete Workflow: {'‚úÖ FUNCTIONAL' if workflow_success else '‚ùå ISSUES REMAIN'}")
        print(f"Final Assessment: {'‚úÖ 100% FUNCTIONALITY' if final_assessment else '‚ùå < 100% FUNCTIONALITY'}")
        
        if fix1_success and fix2_success and workflow_success and final_assessment:
            print("\nüéâ SUCCESS: All critical fixes verified and platform achieves 100% functionality!")
        elif fix1_success and fix2_success:
            print("\n‚úÖ GOOD: Critical fixes resolved but some workflow issues remain")
        else:
            print("\n‚ùå CRITICAL: Major issues still exist that prevent 100% functionality")
        
        print("=" * 80)

    # ========== PHASE 2: AI CLINICAL INTELLIGENCE TESTING ==========

    def test_clinical_intelligence_status(self):
        """Test Phase 2: AI Clinical Intelligence system status"""
        success, response = self.run_test(
            "Phase 2: AI Clinical Intelligence Status",
            "GET",
            "ai/clinical-intelligence-status",
            200
        )
        
        if success:
            print(f"   Phase: {response.get('phase', 'Unknown')}")
            print(f"   Overall Status: {response.get('overall_status', 'Unknown')}")
            
            component_status = response.get('component_status', {})
            print(f"   Visual Explainable AI: {component_status.get('visual_explainable_ai', {}).get('status', 'unknown')}")
            print(f"   Comparative Analytics: {component_status.get('comparative_effectiveness_analytics', {}).get('status', 'unknown')}")
            print(f"   Risk Assessment: {component_status.get('personalized_risk_assessment', {}).get('status', 'unknown')}")
            
            usage_stats = response.get('usage_statistics', {})
            print(f"   Visual Explanations: {usage_stats.get('visual_explanations_generated', 0)}")
            print(f"   Treatment Comparisons: {usage_stats.get('treatment_comparisons_performed', 0)}")
            print(f"   Risk Assessments: {usage_stats.get('risk_assessments_completed', 0)}")
            
            capabilities = response.get('capabilities', [])
            print(f"   Capabilities Available: {len(capabilities)}")
        return success

    def test_visual_explainable_ai_generation(self):
        """Test Visual Explainable AI system with SHAP/LIME analysis"""
        if not self.patient_id:
            print("‚ùå No patient ID available for visual explanation testing")
            return False

        # Sample prediction data for osteoarthritis patient
        prediction_data = {
            "model_prediction": {
                "primary_diagnosis": "Osteoarthritis bilateral knee",
                "confidence_score": 0.92,
                "treatment_recommendation": "PRP therapy",
                "success_probability": 0.85
            },
            "feature_values": {
                "age": 58,
                "symptom_duration_months": 36,
                "pain_scale": 7,
                "functional_limitation": 0.6,
                "inflammatory_markers": 0.3,
                "previous_treatments": 3,
                "activity_level": 0.4
            }
        }

        patient_data = {
            "patient_id": self.patient_id,
            "demographics": {
                "age": 58,
                "gender": "Female",
                "occupation": "Physician"
            },
            "medical_history": ["Osteoarthritis", "Hypertension"],
            "current_symptoms": ["bilateral knee pain", "morning stiffness"]
        }

        print("   This may take 30-45 seconds for SHAP/LIME analysis...")
        success, response = self.run_test(
            "Generate Visual AI Explanation (SHAP/LIME)",
            "POST",
            "ai/visual-explanation",
            200,
            data={
                "prediction_data": prediction_data,
                "patient_data": patient_data
            },
            timeout=60
        )
        
        if success:
            visual_explanation = response.get('visual_explanation', {})
            print(f"   Explanation ID: {visual_explanation.get('explanation_id', 'Unknown')}")
            print(f"   Explanation Type: {visual_explanation.get('explanation_type', 'Unknown')}")
            
            feature_importance = visual_explanation.get('feature_importance_analysis', {})
            print(f"   Feature Factors Analyzed: {len(feature_importance.get('feature_contributions', []))}")
            print(f"   Base Value: {feature_importance.get('base_value', 0):.2f}")
            print(f"   Final Prediction: {feature_importance.get('final_prediction', 0):.2f}")
            
            clinical_interpretation = visual_explanation.get('clinical_interpretation', {})
            print(f"   Transparency Score: {clinical_interpretation.get('transparency_score', 0):.2f}")
            print(f"   Explanation Confidence: {clinical_interpretation.get('explanation_confidence', 0):.2f}")
            
            # Store explanation ID for retrieval test
            self.explanation_id = visual_explanation.get('explanation_id')
        return success

    def test_get_visual_explanation(self):
        """Test retrieving stored visual explanation"""
        if not hasattr(self, 'explanation_id') or not self.explanation_id:
            print("‚ùå No explanation ID available for retrieval testing")
            return False

        success, response = self.run_test(
            "Get Visual Explanation Details",
            "GET",
            f"ai/visual-explanation/{self.explanation_id}",
            200
        )
        
        if success:
            print(f"   Retrieved Explanation ID: {response.get('explanation_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Generated At: {response.get('generated_at', 'Unknown')}")
            
            feature_importance = response.get('feature_importance_analysis', {})
            contributions = feature_importance.get('feature_contributions', [])
            if contributions:
                top_feature = contributions[0]
                print(f"   Top Feature: {top_feature.get('feature_name', 'Unknown')} (importance: {top_feature.get('importance_score', 0):.3f})")
        return success

    def test_treatment_comparison_analysis(self):
        """Test Comparative Effectiveness Analytics with multiple treatments"""
        comparison_request = {
            "condition": "osteoarthritis",
            "treatments": ["PRP", "BMAC", "stem_cells"],
            "patient_profile": {
                "age": 58,
                "severity": "moderate",
                "previous_treatments": ["NSAIDs", "physical_therapy"],
                "comorbidities": ["hypertension"]
            },
            "comparison_criteria": [
                "efficacy", "safety", "cost_effectiveness", "durability"
            ],
            "time_horizon": "12_months"
        }

        print("   This may take 30-45 seconds for comprehensive treatment analysis...")
        success, response = self.run_test(
            "Comparative Treatment Effectiveness Analysis",
            "POST",
            "analytics/treatment-comparison",
            200,
            data=comparison_request,
            timeout=60
        )
        
        if success:
            comparison_report = response.get('comparison_report', {})
            print(f"   Comparison ID: {comparison_report.get('comparison_id', 'Unknown')}")
            print(f"   Condition: {comparison_report.get('condition', 'Unknown')}")
            print(f"   Treatments Compared: {len(comparison_report.get('treatments_analyzed', []))}")
            
            head_to_head = comparison_report.get('head_to_head_analysis', {})
            print(f"   Pairwise Comparisons: {len(head_to_head.get('pairwise_comparisons', []))}")
            
            cost_effectiveness = comparison_report.get('cost_effectiveness_analysis', {})
            print(f"   Cost-Effectiveness Calculated: {cost_effectiveness.get('analysis_completed', False)}")
            
            network_meta = comparison_report.get('network_meta_analysis', {})
            print(f"   Network Meta-Analysis: {network_meta.get('analysis_type', 'Unknown')}")
            
            treatment_ranking = comparison_report.get('treatment_ranking', {})
            rankings = treatment_ranking.get('ranked_treatments', [])
            if rankings:
                top_treatment = rankings[0]
                print(f"   Top Ranked Treatment: {top_treatment.get('treatment', 'Unknown')} (score: {top_treatment.get('overall_score', 0):.2f})")
            
            # Store comparison ID for retrieval test
            self.comparison_id = comparison_report.get('comparison_id')
        return success

    def test_get_treatment_comparison(self):
        """Test retrieving stored treatment comparison analysis"""
        if not hasattr(self, 'comparison_id') or not self.comparison_id:
            print("‚ùå No comparison ID available for retrieval testing")
            return False

        success, response = self.run_test(
            "Get Treatment Comparison Details",
            "GET",
            f"analytics/treatment-comparison/{self.comparison_id}",
            200
        )
        
        if success:
            print(f"   Retrieved Comparison ID: {response.get('comparison_id', 'Unknown')}")
            print(f"   Analysis Date: {response.get('analysis_date', 'Unknown')}")
            print(f"   Treatments: {', '.join(response.get('treatments_analyzed', []))}")
            
            recommendations = response.get('clinical_recommendations', {})
            print(f"   Recommendations Generated: {len(recommendations.get('treatment_recommendations', []))}")
        return success

    def test_treatment_effectiveness_data(self):
        """Test getting treatment effectiveness data for specific condition"""
        success, response = self.run_test(
            "Treatment Effectiveness Data - Osteoarthritis",
            "GET",
            "analytics/treatment-effectiveness/osteoarthritis?treatment=PRP&time_horizon=6_months",
            200
        )
        
        if success:
            print(f"   Condition: {response.get('condition', 'Unknown')}")
            print(f"   Time Horizon: {response.get('time_horizon', 'Unknown')}")
            
            effectiveness_data = response.get('effectiveness_data', {})
            print(f"   Effectiveness Data Available: {len(effectiveness_data)}")
            
            if effectiveness_data:
                # Show sample effectiveness metrics
                for treatment, data in list(effectiveness_data.items())[:2]:
                    print(f"   {treatment}: Success Rate {data.get('success_rate', 0):.1%}, Cost ${data.get('average_cost', 0):,}")
        return success

    def test_comprehensive_risk_assessment(self):
        """Test Personalized Risk Assessment with comprehensive patient data"""
        if not self.patient_id:
            print("‚ùå No patient ID available for risk assessment testing")
            return False

        patient_data = {
            "patient_id": self.patient_id,
            "demographics": {
                "age": 58,
                "gender": "Female",
                "bmi": 26.5,
                "occupation": "Physician"
            },
            "medical_history": {
                "conditions": ["Osteoarthritis", "Hypertension", "Hypothyroidism"],
                "surgeries": [],
                "allergies": ["NKDA"]
            },
            "current_medications": ["Lisinopril", "Levothyroxine", "Ibuprofen PRN"],
            "lifestyle_factors": {
                "smoking_status": "never",
                "alcohol_consumption": "moderate",
                "exercise_level": "low_due_to_pain",
                "stress_level": "moderate"
            },
            "laboratory_results": {
                "inflammatory_markers": {
                    "CRP": 2.1,
                    "ESR": 18
                },
                "metabolic_panel": {
                    "glucose": 95,
                    "HbA1c": 5.4
                }
            },
            "functional_assessments": {
                "pain_scale": 7,
                "mobility_score": 0.6,
                "quality_of_life": 0.5
            }
        }

        treatment_plan = {
            "treatment_type": "PRP_therapy",
            "treatment_details": {
                "injection_sites": ["bilateral_knees"],
                "planned_sessions": 3,
                "interval_weeks": 4
            },
            "adjunct_therapies": ["physical_therapy", "activity_modification"]
        }

        print("   This may take 30-45 seconds for comprehensive risk analysis...")
        success, response = self.run_test(
            "Comprehensive Personalized Risk Assessment",
            "POST",
            "ai/risk-assessment",
            200,
            data={
                "patient_data": patient_data,
                "treatment_plan": treatment_plan
            },
            timeout=60
        )
        
        if success:
            risk_assessment = response.get('risk_assessment', {})
            print(f"   Assessment ID: {risk_assessment.get('assessment_id', 'Unknown')}")
            
            individual_risks = risk_assessment.get('individual_risk_assessments', {})
            print(f"   Risk Categories Assessed: {len(individual_risks)}")
            
            treatment_success = individual_risks.get('treatment_success', {})
            print(f"   Treatment Success Probability: {treatment_success.get('predicted_success_probability', 0):.1%}")
            
            adverse_events = individual_risks.get('adverse_events', {})
            print(f"   Overall Adverse Event Risk: {adverse_events.get('overall_adverse_event_risk', 0):.1%}")
            
            overall_stratification = risk_assessment.get('overall_risk_stratification', {})
            print(f"   Risk Category: {overall_stratification.get('overall_risk_category', 'Unknown')}")
            print(f"   Risk-Benefit Ratio: {overall_stratification.get('risk_benefit_ratio', 0):.1f}")
            
            monitoring_plan = risk_assessment.get('personalized_monitoring_plan', {})
            print(f"   Monitoring Intervals: {len(monitoring_plan.get('monitoring_schedule', []))}")
            
            # Store assessment ID for retrieval test
            self.assessment_id = risk_assessment.get('assessment_id')
        return success

    def test_get_risk_assessment(self):
        """Test retrieving stored risk assessment"""
        if not hasattr(self, 'assessment_id') or not self.assessment_id:
            print("‚ùå No assessment ID available for retrieval testing")
            return False

        success, response = self.run_test(
            "Get Risk Assessment Details",
            "GET",
            f"ai/risk-assessment/{self.assessment_id}",
            200
        )
        
        if success:
            print(f"   Retrieved Assessment ID: {response.get('assessment_id', 'Unknown')}")
            print(f"   Patient ID: {response.get('patient_id', 'Unknown')}")
            print(f"   Assessment Date: {response.get('assessment_date', 'Unknown')}")
            
            risk_factors = response.get('risk_factors_identified', [])
            print(f"   Risk Factors Identified: {len(risk_factors)}")
            
            protective_factors = response.get('protective_factors_identified', [])
            print(f"   Protective Factors: {len(protective_factors)}")
        return success

    def test_patient_cohort_risk_stratification(self):
        """Test risk stratification for patient cohort"""
        # Create sample patient cohort for testing
        patient_cohort = [
            {
                "patient_id": f"test_patient_{i}",
                "demographics": {"age": 45 + i*5, "gender": "Female" if i%2 else "Male", "bmi": 24 + i},
                "medical_history": {"conditions": ["Osteoarthritis"]},
                "functional_assessments": {"pain_scale": 5 + i, "mobility_score": 0.8 - i*0.1}
            }
            for i in range(5)  # Test with 5 patients
        ]

        success, response = self.run_test(
            "Patient Cohort Risk Stratification",
            "POST",
            "ai/risk-stratification",
            200,
            data={
                "patient_cohort": patient_cohort,
                "treatment_type": "PRP_therapy"
            },
            timeout=90
        )
        
        if success:
            stratification_results = response.get('stratification_results', [])
            print(f"   Cohort Size: {response.get('cohort_size', 0)}")
            print(f"   Successful Assessments: {response.get('successful_assessments', 0)}")
            print(f"   Treatment Type: {response.get('treatment_type', 'Unknown')}")
            print(f"   Ranking Criteria: {response.get('ranking_criteria', 'Unknown')}")
            
            if stratification_results:
                top_candidate = stratification_results[0]
                print(f"   Top Candidate: {top_candidate.get('patient_id', 'Unknown')}")
                print(f"   Risk Category: {top_candidate.get('overall_risk_category', 'Unknown')}")
                print(f"   Success Probability: {top_candidate.get('treatment_success_probability', 0):.1%}")
                print(f"   Risk-Benefit Ratio: {top_candidate.get('risk_benefit_ratio', 0):.1f}")
        return success

    def run_phase2_ai_clinical_intelligence_tests(self):
        """Run comprehensive Phase 2: AI Clinical Intelligence tests"""
        print("üöÄ Starting Phase 2: AI Clinical Intelligence Test Suite")
        print("=" * 80)
        print("Testing world-class Visual Explainable AI, Comparative Effectiveness")
        print("Analytics, and Personalized Risk Assessment systems.")
        print("=" * 80)
        
        # Setup: Create patient for testing
        print("\nüìã SETUP: Creating Test Patient")
        print("-" * 50)
        patient_created = self.test_create_patient()
        
        if not patient_created:
            print("‚ùå Cannot proceed without patient - setup failed")
            return False
        
        # Phase 2 AI Clinical Intelligence Tests
        print("\nüéØ PHASE 2: AI CLINICAL INTELLIGENCE TESTS")
        print("-" * 50)
        
        # Test 1: System Status
        test1_success = self.test_clinical_intelligence_status()
        
        # Test 2: Visual Explainable AI Generation
        test2_success = self.test_visual_explainable_ai_generation()
        
        # Test 3: Get Visual Explanation
        test3_success = self.test_get_visual_explanation()
        
        # Test 4: Treatment Comparison Analysis
        test4_success = self.test_treatment_comparison_analysis()
        
        # Test 5: Get Treatment Comparison
        test5_success = self.test_get_treatment_comparison()
        
        # Test 6: Treatment Effectiveness Data
        test6_success = self.test_treatment_effectiveness_data()
        
        # Test 7: Comprehensive Risk Assessment
        test7_success = self.test_comprehensive_risk_assessment()
        
        # Test 8: Get Risk Assessment
        test8_success = self.test_get_risk_assessment()
        
        # Test 9: Patient Cohort Risk Stratification
        test9_success = self.test_patient_cohort_risk_stratification()
        
        # Summary of Phase 2 Tests
        print("\n" + "=" * 80)
        print("üéØ PHASE 2: AI CLINICAL INTELLIGENCE TESTING SUMMARY")
        print("=" * 80)
        
        phase2_tests = [
            ("Clinical Intelligence Status", test1_success),
            ("Visual Explainable AI Generation", test2_success),
            ("Get Visual Explanation", test3_success),
            ("Treatment Comparison Analysis", test4_success),
            ("Get Treatment Comparison", test5_success),
            ("Treatment Effectiveness Data", test6_success),
            ("Comprehensive Risk Assessment", test7_success),
            ("Get Risk Assessment", test8_success),
            ("Patient Cohort Risk Stratification", test9_success)
        ]
        
        passed_phase2_tests = sum(1 for _, success in phase2_tests if success)
        total_phase2_tests = len(phase2_tests)
        
        print(f"Phase 2 AI Clinical Intelligence Tests: {passed_phase2_tests}/{total_phase2_tests} PASSED")
        print(f"Phase 2 Success Rate: {(passed_phase2_tests / total_phase2_tests * 100):.1f}%")
        
        for test_name, success in phase2_tests:
            status = "‚úÖ PASSED" if success else "‚ùå FAILED"
            print(f"   {status}: {test_name}")
        
        if passed_phase2_tests == total_phase2_tests:
            print("\nüéâ ALL PHASE 2: AI CLINICAL INTELLIGENCE FEATURES ARE FUNCTIONAL!")
            print("The world-class AI clinical intelligence system is ready for production use.")
        else:
            print(f"\n‚ö†Ô∏è  {total_phase2_tests - passed_phase2_tests} Phase 2 features failed.")
            print("These are critical for world-class AI clinical intelligence.")
        
        return passed_phase2_tests == total_phase2_tests

    def run_phase3_focused_tests(self):
        """Run Phase 3: Global Knowledge Engine focused testing with routing fix validation"""
        print("üåç PHASE 3: GLOBAL KNOWLEDGE ENGINE - ROUTING FIX VALIDATION")
        print("=" * 80)
        print("Testing FINALLY FIXED Phase 3 with routing issue resolved")
        print("Focus: International Protocol Library routing fix + complete Phase 3 validation")
        print("=" * 80)
        
        # Phase 3 System Status
        print("\nüîç PHASE 3 SYSTEM STATUS")
        print("-" * 50)
        self.test_phase3_global_knowledge_system_status()
        
        # International Protocol Library - ROUTING FIX TESTING
        print("\nüåê INTERNATIONAL PROTOCOL LIBRARY - ROUTING FIX TESTING")
        print("-" * 50)
        print("Testing the FIXED routing issue - duplicate endpoint removed, proper positioning")
        self.test_international_protocol_search_osteoarthritis_fix()
        self.test_international_protocol_search_rotator_cuff_fix()
        self.test_international_protocol_search_chronic_pain_fix()
        self.test_international_protocol_multiple_traditions()
        self.test_international_protocol_integration_filter()
        
        # Global Regulatory Intelligence (was working previously)
        print("\nüèõÔ∏è GLOBAL REGULATORY INTELLIGENCE SYSTEM")
        print("-" * 50)
        self.test_global_regulatory_intelligence_treatment_status()
        self.test_global_regulatory_intelligence_country_specific()
        self.test_global_regulatory_intelligence_cross_jurisdictional()
        
        # Community Collaboration Platform (peer consultation fixed)
        print("\nüë• COMMUNITY COLLABORATION PLATFORM")
        print("-" * 50)
        self.test_community_peer_consultation_fix()
        self.test_community_peer_consultation_minimal()
        self.test_community_insights_collective_intelligence()
        self.test_community_insights_therapy_comparison()
        
        # Overall Phase 3 System Integration
        print("\nüîó PHASE 3 SYSTEM INTEGRATION VALIDATION")
        print("-" * 50)
        self.test_phase3_complete_workflow()
        
        # Final Phase 3 Results
        print("\n" + "=" * 80)
        print("üéØ PHASE 3: GLOBAL KNOWLEDGE ENGINE TEST RESULTS")
        print("=" * 80)
        print(f"Total Tests Run: {self.tests_run}")
        print(f"Tests Passed: {self.tests_passed}")
        print(f"Tests Failed: {self.tests_run - self.tests_passed}")
        success_rate = (self.tests_passed / self.tests_run * 100) if self.tests_run > 0 else 0
        print(f"Success Rate: {success_rate:.1f}%")
        
        # Phase 3 specific assessment
        if success_rate >= 80:
            print("üéâ PHASE 3 SUCCESS! Global Knowledge Engine is fully operational!")
            print("‚úÖ International Protocol Library routing fix SUCCESSFUL")
            print("‚úÖ Community Collaboration Platform operational")
            print("‚úÖ Global Regulatory Intelligence functional")
            print("üåü World-class global knowledge capabilities achieved!")
        elif success_rate >= 60:
            print("‚ö†Ô∏è  PHASE 3 PARTIAL SUCCESS: Most components working, minor issues remain")
        else:
            print("‚ùå PHASE 3 NEEDS WORK: Significant issues in global knowledge systems")
        
        return success_rate / 100

    def test_international_protocol_search_osteoarthritis_fix(self):
        """Test FIXED International Protocol Library - osteoarthritis search (routing fix)"""
        success, response = self.run_test(
            "ROUTING FIX: International Protocol Search - Osteoarthritis",
            "GET",
            "protocols/international-search?condition=osteoarthritis&max_results=10",
            200,
            timeout=45
        )
        
        if success:
            print(f"   ‚úÖ ROUTING FIX SUCCESSFUL - No more 404 errors!")
            print(f"   Condition: {response.get('condition', 'unknown')}")
            print(f"   Search Status: {response.get('status', 'unknown')}")
            
            protocols = response.get('protocols', [])
            print(f"   Protocols Found: {len(protocols)}")
            
            if protocols:
                first_protocol = protocols[0]
                print(f"   First Protocol: {first_protocol.get('protocol_name', 'Unknown')}")
                print(f"   Medical Tradition: {first_protocol.get('medical_tradition', 'Unknown')}")
                print(f"   Country: {first_protocol.get('country', 'Unknown')}")
                print(f"   Integration Level: {first_protocol.get('integration_level', 'Unknown')}")
                print(f"   Evidence Level: {first_protocol.get('evidence_level', 'Unknown')}")
            
            search_metadata = response.get('search_metadata', {})
            if search_metadata:
                print(f"   Search Timestamp: {search_metadata.get('search_timestamp', 'Unknown')}")
                print(f"   Total Available: {search_metadata.get('total_available_protocols', 0)}")
        else:
            print("   ‚ùå ROUTING FIX FAILED - Still getting errors!")
        
        return success

    def test_international_protocol_search_rotator_cuff_fix(self):
        """Test FIXED International Protocol Library - rotator cuff search (routing fix)"""
        success, response = self.run_test(
            "ROUTING FIX: International Protocol Search - Rotator Cuff",
            "GET",
            "protocols/international-search?condition=rotator_cuff&max_results=8",
            200,
            timeout=45
        )
        
        if success:
            print(f"   ‚úÖ ROUTING FIX SUCCESSFUL for rotator cuff!")
            print(f"   Condition: {response.get('condition', 'unknown')}")
            protocols = response.get('protocols', [])
            print(f"   Protocols Found: {len(protocols)}")
            
            # Check for rotator cuff specific protocols
            rotator_protocols = [p for p in protocols if 'rotator' in p.get('protocol_name', '').lower() or 'shoulder' in p.get('protocol_name', '').lower()]
            print(f"   Rotator Cuff Specific: {len(rotator_protocols)}")
            
            if protocols:
                sample = protocols[0]
                print(f"   Sample Protocol: {sample.get('protocol_name', 'Unknown')}")
                print(f"   Medical Tradition: {sample.get('medical_tradition', 'Unknown')}")
        else:
            print("   ‚ùå ROUTING FIX FAILED for rotator cuff!")
        
        return success

    def test_international_protocol_search_chronic_pain_fix(self):
        """Test FIXED International Protocol Library - chronic pain search (routing fix)"""
        success, response = self.run_test(
            "ROUTING FIX: International Protocol Search - Chronic Pain",
            "GET",
            "protocols/international-search?condition=chronic_pain&max_results=12",
            200,
            timeout=45
        )
        
        if success:
            print(f"   ‚úÖ ROUTING FIX SUCCESSFUL for chronic pain!")
            print(f"   Condition: {response.get('condition', 'unknown')}")
            protocols = response.get('protocols', [])
            print(f"   Protocols Found: {len(protocols)}")
            
            # Check for pain management protocols
            pain_protocols = [p for p in protocols if 'pain' in p.get('protocol_name', '').lower()]
            print(f"   Pain Management Specific: {len(pain_protocols)}")
            
            if protocols:
                sample = protocols[0]
                print(f"   Sample Protocol: {sample.get('protocol_name', 'Unknown')}")
                print(f"   Target Conditions: {sample.get('target_conditions', [])}")
        else:
            print("   ‚ùå ROUTING FIX FAILED for chronic pain!")
        
        return success

    def test_global_regulatory_intelligence_treatment_status(self):
        """Test Global Regulatory Intelligence - treatment status queries"""
        success, response = self.run_test(
            "Global Regulatory Intelligence - Treatment Status",
            "GET",
            "regulatory/treatment-status?treatment=PRP&jurisdiction=global",
            200,
            timeout=30
        )
        
        if success:
            print(f"   Treatment: {response.get('treatment', 'unknown')}")
            print(f"   Jurisdiction: {response.get('jurisdiction', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
            
            regulatory_data = response.get('regulatory_data', {})
            if regulatory_data:
                print(f"   Approval Status: {regulatory_data.get('approval_status', 'unknown')}")
                print(f"   Regulatory Category: {regulatory_data.get('regulatory_category', 'unknown')}")
        
        return success

    def test_global_regulatory_intelligence_country_specific(self):
        """Test Global Regulatory Intelligence - country-specific queries"""
        success, response = self.run_test(
            "Global Regulatory Intelligence - Country Specific",
            "GET",
            "regulatory/country-status?country=US&treatment=BMAC",
            200,
            timeout=30
        )
        
        if success:
            print(f"   Country: {response.get('country', 'unknown')}")
            print(f"   Treatment: {response.get('treatment', 'unknown')}")
            
            country_data = response.get('country_regulatory_data', {})
            if country_data:
                print(f"   Legal Status: {country_data.get('legal_status', 'unknown')}")
                print(f"   Regulatory Body: {country_data.get('regulatory_body', 'unknown')}")
        
        return success

    def test_global_regulatory_intelligence_cross_jurisdictional(self):
        """Test Global Regulatory Intelligence - cross-jurisdictional comparison"""
        success, response = self.run_test(
            "Global Regulatory Intelligence - Cross-Jurisdictional",
            "GET",
            "regulatory/cross-jurisdictional?treatment=stem_cells&countries=US,EU,Canada,Australia",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Treatment: {response.get('treatment', 'unknown')}")
            
            jurisdictional_comparison = response.get('jurisdictional_comparison', {})
            if jurisdictional_comparison:
                countries_analyzed = len(jurisdictional_comparison.get('countries', []))
                print(f"   Countries Analyzed: {countries_analyzed}")
                
                harmonization = jurisdictional_comparison.get('harmonization_assessment', {})
                if harmonization:
                    print(f"   Harmonization Level: {harmonization.get('harmonization_level', 'unknown')}")
        
        return success

    def test_community_peer_consultation_fix(self):
        """Test FIXED Community Collaboration Platform - peer consultation with optional case_summary"""
        consultation_data = {
            "consultation_type": "regenerative_medicine_case",
            "patient_demographics": {
                "age": 58,
                "gender": "Female",
                "condition": "Bilateral knee osteoarthritis"
            },
            "clinical_question": "Seeking advice on optimal PRP protocol for 58-year-old female physician with bilateral knee osteoarthritis. Patient has failed conservative management and wants to avoid knee replacement. What are your experiences with PRP vs BMAC for this patient profile?",
            "urgency_level": "routine",
            "expertise_sought": ["regenerative_medicine", "orthopedics", "sports_medicine"],
            "anonymized": True
        }

        success, response = self.run_test(
            "FIXED: Community Peer Consultation - Optional case_summary",
            "POST",
            "community/peer-consultation",
            200,
            data=consultation_data,
            timeout=30
        )
        
        if success:
            print(f"   ‚úÖ PEER CONSULTATION FIX SUCCESSFUL!")
            print(f"   Consultation ID: {response.get('consultation_id', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
            print(f"   Consultation Type: {response.get('consultation_type', 'unknown')}")
            print(f"   Urgency Level: {response.get('urgency_level', 'unknown')}")
            print(f"   Expertise Areas: {len(response.get('expertise_sought', []))}")
            
            matching_experts = response.get('matching_experts', [])
            print(f"   Matching Experts Found: {len(matching_experts)}")
        else:
            print("   ‚ùå PEER CONSULTATION FIX FAILED!")
        
        return success

    def test_community_peer_consultation_minimal(self):
        """Test Community Collaboration Platform - minimal data consultation"""
        minimal_consultation = {
            "consultation_type": "quick_question",
            "clinical_question": "What's the optimal PRP concentration for knee osteoarthritis in elderly patients?",
            "urgency_level": "routine"
        }

        success, response = self.run_test(
            "Community Peer Consultation - Minimal Data",
            "POST",
            "community/peer-consultation",
            200,
            data=minimal_consultation,
            timeout=30
        )
        
        if success:
            print(f"   ‚úÖ MINIMAL CONSULTATION SUCCESSFUL!")
            print(f"   Consultation ID: {response.get('consultation_id', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
        
        return success

    def test_phase3_complete_workflow(self):
        """Test complete Phase 3 workflow integration"""
        success, response = self.run_test(
            "Phase 3: Complete Global Knowledge Workflow",
            "GET",
            "global-knowledge/system-status",
            200
        )
        
        if success:
            services = response.get('services', {})
            
            # Check all three major Phase 3 components
            regulatory_status = services.get('global_regulatory_intelligence', {}).get('status', 'unknown')
            protocol_status = services.get('international_protocol_library', {}).get('status', 'unknown')
            community_status = services.get('community_collaboration_platform', {}).get('status', 'unknown')
            
            print(f"   Global Regulatory Intelligence: {regulatory_status}")
            print(f"   International Protocol Library: {protocol_status}")
            print(f"   Community Collaboration Platform: {community_status}")
            
            # Calculate Phase 3 component success
            working_components = sum(1 for status in [regulatory_status, protocol_status, community_status] if status == 'active')
            phase3_success_rate = (working_components / 3) * 100
            
            print(f"   Phase 3 Component Success Rate: {phase3_success_rate:.1f}%")
            
            if phase3_success_rate >= 80:
                print("   üéâ PHASE 3 COMPLETE SUCCESS - All major systems operational!")
            elif phase3_success_rate >= 60:
                print("   ‚ö†Ô∏è  PHASE 3 PARTIAL SUCCESS - Most systems working")
            else:
                print("   ‚ùå PHASE 3 NEEDS WORK - Multiple system issues")
        
        return success

def main_phase3():
    """Main function to run Phase 3: Global Knowledge Engine tests"""
    print("üåç RegenMed AI Pro - Phase 3: Global Knowledge Engine Testing")
    print("Testing newly implemented world-class global knowledge engine features")
    print("=" * 70)
    
    tester = RegenMedAIProTester()
    success = tester.run_phase3_global_knowledge_engine_tests()
    
    if success:
        print("\nüéâ SUCCESS: All Phase 3: Global Knowledge Engine features are functional!")
        return 0
    else:
        print("\n‚ùå FAILURE: Some Phase 3 features need attention.")
        return 1

def main():
    """Main function to run Phase 2: AI Clinical Intelligence tests"""
    print("üß¨ RegenMed AI Pro - Phase 2: AI Clinical Intelligence Testing")
    print("Testing newly implemented world-class AI clinical intelligence features")
    print("=" * 70)
    
    tester = RegenMedAIProTester()
    success = tester.run_phase2_ai_clinical_intelligence_tests()
    
    if success:
        print("\nüéâ SUCCESS: All Phase 2: AI Clinical Intelligence features are functional!")
        return 0
    else:
        print("\n‚ùå FAILURE: Some Phase 2 features need attention.")
        return 1

    def run_phase3_focused_tests(self):
        """Run Phase 3: Global Knowledge Engine focused testing with routing fix validation"""
        print("üåç PHASE 3: GLOBAL KNOWLEDGE ENGINE - ROUTING FIX VALIDATION")
        print("=" * 80)
        print("Testing FINALLY FIXED Phase 3 with routing issue resolved")
        print("Focus: International Protocol Library routing fix + complete Phase 3 validation")
        print("=" * 80)
        
        # Phase 3 System Status
        print("\nüîç PHASE 3 SYSTEM STATUS")
        print("-" * 50)
        self.test_phase3_global_knowledge_system_status()
        
        # International Protocol Library - ROUTING FIX TESTING
        print("\nüåê INTERNATIONAL PROTOCOL LIBRARY - ROUTING FIX TESTING")
        print("-" * 50)
        print("Testing the FIXED routing issue - duplicate endpoint removed, proper positioning")
        self.test_international_protocol_search_osteoarthritis_fix()
        self.test_international_protocol_search_rotator_cuff_fix()
        self.test_international_protocol_search_chronic_pain_fix()
        self.test_international_protocol_multiple_traditions()
        self.test_international_protocol_integration_filter()
        
        # Global Regulatory Intelligence (was working previously)
        print("\nüèõÔ∏è GLOBAL REGULATORY INTELLIGENCE SYSTEM")
        print("-" * 50)
        self.test_global_regulatory_intelligence_treatment_status()
        self.test_global_regulatory_intelligence_country_specific()
        self.test_global_regulatory_intelligence_cross_jurisdictional()
        
        # Community Collaboration Platform (peer consultation fixed)
        print("\nüë• COMMUNITY COLLABORATION PLATFORM")
        print("-" * 50)
        self.test_community_peer_consultation_fix()
        self.test_community_peer_consultation_minimal()
        self.test_community_insights_collective_intelligence()
        self.test_community_insights_therapy_comparison()
        
        # Overall Phase 3 System Integration
        print("\nüîó PHASE 3 SYSTEM INTEGRATION VALIDATION")
        print("-" * 50)
        self.test_phase3_complete_workflow()
        
        # Final Phase 3 Results
        print("\n" + "=" * 80)
        print("üéØ PHASE 3: GLOBAL KNOWLEDGE ENGINE TEST RESULTS")
        print("=" * 80)
        print(f"Total Tests Run: {self.tests_run}")
        print(f"Tests Passed: {self.tests_passed}")
        print(f"Tests Failed: {self.tests_run - self.tests_passed}")
        success_rate = (self.tests_passed / self.tests_run * 100) if self.tests_run > 0 else 0
        print(f"Success Rate: {success_rate:.1f}%")
        
        # Phase 3 specific assessment
        if success_rate >= 80:
            print("üéâ PHASE 3 SUCCESS! Global Knowledge Engine is fully operational!")
            print("‚úÖ International Protocol Library routing fix SUCCESSFUL")
            print("‚úÖ Community Collaboration Platform operational")
            print("‚úÖ Global Regulatory Intelligence functional")
            print("üåü World-class global knowledge capabilities achieved!")
        elif success_rate >= 60:
            print("‚ö†Ô∏è  PHASE 3 PARTIAL SUCCESS: Most components working, minor issues remain")
        else:
            print("‚ùå PHASE 3 NEEDS WORK: Significant issues in global knowledge systems")
        
        return success_rate / 100

    def test_international_protocol_search_osteoarthritis_fix(self):
        """Test FIXED International Protocol Library - osteoarthritis search (routing fix)"""
        success, response = self.run_test(
            "ROUTING FIX: International Protocol Search - Osteoarthritis",
            "GET",
            "protocols/international-search?condition=osteoarthritis&max_results=10",
            200,
            timeout=45
        )
        
        if success:
            print(f"   ‚úÖ ROUTING FIX SUCCESSFUL - No more 404 errors!")
            print(f"   Condition: {response.get('condition', 'unknown')}")
            print(f"   Search Status: {response.get('status', 'unknown')}")
            
            protocols = response.get('protocols', [])
            print(f"   Protocols Found: {len(protocols)}")
            
            if protocols:
                first_protocol = protocols[0]
                print(f"   First Protocol: {first_protocol.get('protocol_name', 'Unknown')}")
                print(f"   Medical Tradition: {first_protocol.get('medical_tradition', 'Unknown')}")
                print(f"   Country: {first_protocol.get('country', 'Unknown')}")
                print(f"   Integration Level: {first_protocol.get('integration_level', 'Unknown')}")
                print(f"   Evidence Level: {first_protocol.get('evidence_level', 'Unknown')}")
            
            search_metadata = response.get('search_metadata', {})
            if search_metadata:
                print(f"   Search Timestamp: {search_metadata.get('search_timestamp', 'Unknown')}")
                print(f"   Total Available: {search_metadata.get('total_available_protocols', 0)}")
        else:
            print("   ‚ùå ROUTING FIX FAILED - Still getting errors!")
        
        return success

    def test_international_protocol_search_rotator_cuff_fix(self):
        """Test FIXED International Protocol Library - rotator cuff search (routing fix)"""
        success, response = self.run_test(
            "ROUTING FIX: International Protocol Search - Rotator Cuff",
            "GET",
            "protocols/international-search?condition=rotator_cuff&max_results=8",
            200,
            timeout=45
        )
        
        if success:
            print(f"   ‚úÖ ROUTING FIX SUCCESSFUL for rotator cuff!")
            print(f"   Condition: {response.get('condition', 'unknown')}")
            protocols = response.get('protocols', [])
            print(f"   Protocols Found: {len(protocols)}")
            
            # Check for rotator cuff specific protocols
            rotator_protocols = [p for p in protocols if 'rotator' in p.get('protocol_name', '').lower() or 'shoulder' in p.get('protocol_name', '').lower()]
            print(f"   Rotator Cuff Specific: {len(rotator_protocols)}")
            
            if protocols:
                sample = protocols[0]
                print(f"   Sample Protocol: {sample.get('protocol_name', 'Unknown')}")
                print(f"   Medical Tradition: {sample.get('medical_tradition', 'Unknown')}")
        else:
            print("   ‚ùå ROUTING FIX FAILED for rotator cuff!")
        
        return success

    def test_international_protocol_search_chronic_pain_fix(self):
        """Test FIXED International Protocol Library - chronic pain search (routing fix)"""
        success, response = self.run_test(
            "ROUTING FIX: International Protocol Search - Chronic Pain",
            "GET",
            "protocols/international-search?condition=chronic_pain&max_results=12",
            200,
            timeout=45
        )
        
        if success:
            print(f"   ‚úÖ ROUTING FIX SUCCESSFUL for chronic pain!")
            print(f"   Condition: {response.get('condition', 'unknown')}")
            protocols = response.get('protocols', [])
            print(f"   Protocols Found: {len(protocols)}")
            
            # Check for pain management protocols
            pain_protocols = [p for p in protocols if 'pain' in p.get('protocol_name', '').lower()]
            print(f"   Pain Management Specific: {len(pain_protocols)}")
            
            if protocols:
                sample = protocols[0]
                print(f"   Sample Protocol: {sample.get('protocol_name', 'Unknown')}")
                print(f"   Target Conditions: {sample.get('target_conditions', [])}")
        else:
            print("   ‚ùå ROUTING FIX FAILED for chronic pain!")
        
        return success

    def test_global_regulatory_intelligence_treatment_status(self):
        """Test Global Regulatory Intelligence - treatment status queries"""
        success, response = self.run_test(
            "Global Regulatory Intelligence - Treatment Status",
            "GET",
            "regulatory/treatment-status?treatment=PRP&jurisdiction=global",
            200,
            timeout=30
        )
        
        if success:
            print(f"   Treatment: {response.get('treatment', 'unknown')}")
            print(f"   Jurisdiction: {response.get('jurisdiction', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
            
            regulatory_data = response.get('regulatory_data', {})
            if regulatory_data:
                print(f"   Approval Status: {regulatory_data.get('approval_status', 'unknown')}")
                print(f"   Regulatory Category: {regulatory_data.get('regulatory_category', 'unknown')}")
        
        return success

    def test_global_regulatory_intelligence_country_specific(self):
        """Test Global Regulatory Intelligence - country-specific queries"""
        success, response = self.run_test(
            "Global Regulatory Intelligence - Country Specific",
            "GET",
            "regulatory/country-status?country=US&treatment=BMAC",
            200,
            timeout=30
        )
        
        if success:
            print(f"   Country: {response.get('country', 'unknown')}")
            print(f"   Treatment: {response.get('treatment', 'unknown')}")
            
            country_data = response.get('country_regulatory_data', {})
            if country_data:
                print(f"   Legal Status: {country_data.get('legal_status', 'unknown')}")
                print(f"   Regulatory Body: {country_data.get('regulatory_body', 'unknown')}")
        
        return success

    def test_global_regulatory_intelligence_cross_jurisdictional(self):
        """Test Global Regulatory Intelligence - cross-jurisdictional comparison"""
        success, response = self.run_test(
            "Global Regulatory Intelligence - Cross-Jurisdictional",
            "GET",
            "regulatory/cross-jurisdictional?treatment=stem_cells&countries=US,EU,Canada,Australia",
            200,
            timeout=45
        )
        
        if success:
            print(f"   Treatment: {response.get('treatment', 'unknown')}")
            
            jurisdictional_comparison = response.get('jurisdictional_comparison', {})
            if jurisdictional_comparison:
                countries_analyzed = len(jurisdictional_comparison.get('countries', []))
                print(f"   Countries Analyzed: {countries_analyzed}")
                
                harmonization = jurisdictional_comparison.get('harmonization_assessment', {})
                if harmonization:
                    print(f"   Harmonization Level: {harmonization.get('harmonization_level', 'unknown')}")
        
        return success

    def test_community_peer_consultation_fix(self):
        """Test FIXED Community Collaboration Platform - peer consultation with optional case_summary"""
        consultation_data = {
            "consultation_type": "regenerative_medicine_case",
            "patient_demographics": {
                "age": 58,
                "gender": "Female",
                "condition": "Bilateral knee osteoarthritis"
            },
            "clinical_question": "Seeking advice on optimal PRP protocol for 58-year-old female physician with bilateral knee osteoarthritis. Patient has failed conservative management and wants to avoid knee replacement. What are your experiences with PRP vs BMAC for this patient profile?",
            "urgency_level": "routine",
            "expertise_sought": ["regenerative_medicine", "orthopedics", "sports_medicine"],
            "anonymized": True
        }

        success, response = self.run_test(
            "FIXED: Community Peer Consultation - Optional case_summary",
            "POST",
            "community/peer-consultation",
            200,
            data=consultation_data,
            timeout=30
        )
        
        if success:
            print(f"   ‚úÖ PEER CONSULTATION FIX SUCCESSFUL!")
            print(f"   Consultation ID: {response.get('consultation_id', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
            print(f"   Consultation Type: {response.get('consultation_type', 'unknown')}")
            print(f"   Urgency Level: {response.get('urgency_level', 'unknown')}")
            print(f"   Expertise Areas: {len(response.get('expertise_sought', []))}")
            
            matching_experts = response.get('matching_experts', [])
            print(f"   Matching Experts Found: {len(matching_experts)}")
        else:
            print("   ‚ùå PEER CONSULTATION FIX FAILED!")
        
        return success

    def test_community_peer_consultation_minimal(self):
        """Test Community Collaboration Platform - minimal data consultation"""
        minimal_consultation = {
            "consultation_type": "quick_question",
            "clinical_question": "What's the optimal PRP concentration for knee osteoarthritis in elderly patients?",
            "urgency_level": "routine"
        }

        success, response = self.run_test(
            "Community Peer Consultation - Minimal Data",
            "POST",
            "community/peer-consultation",
            200,
            data=minimal_consultation,
            timeout=30
        )
        
        if success:
            print(f"   ‚úÖ MINIMAL CONSULTATION SUCCESSFUL!")
            print(f"   Consultation ID: {response.get('consultation_id', 'unknown')}")
            print(f"   Status: {response.get('status', 'unknown')}")
        
        return success

    def test_phase3_complete_workflow(self):
        """Test complete Phase 3 workflow integration"""
        success, response = self.run_test(
            "Phase 3: Complete Global Knowledge Workflow",
            "GET",
            "global-knowledge/system-status",
            200
        )
        
        if success:
            services = response.get('services', {})
            
            # Check all three major Phase 3 components
            regulatory_status = services.get('global_regulatory_intelligence', {}).get('status', 'unknown')
            protocol_status = services.get('international_protocol_library', {}).get('status', 'unknown')
            community_status = services.get('community_collaboration_platform', {}).get('status', 'unknown')
            
            print(f"   Global Regulatory Intelligence: {regulatory_status}")
            print(f"   International Protocol Library: {protocol_status}")
            print(f"   Community Collaboration Platform: {community_status}")
            
            # Calculate Phase 3 component success
            working_components = sum(1 for status in [regulatory_status, protocol_status, community_status] if status == 'active')
            phase3_success_rate = (working_components / 3) * 100
            
            print(f"   Phase 3 Component Success Rate: {phase3_success_rate:.1f}%")
            
            if phase3_success_rate >= 80:
                print("   üéâ PHASE 3 COMPLETE SUCCESS - All major systems operational!")
            elif phase3_success_rate >= 60:
                print("   ‚ö†Ô∏è  PHASE 3 PARTIAL SUCCESS - Most systems working")
            else:
                print("   ‚ùå PHASE 3 NEEDS WORK - Multiple system issues")
        
        return success

    def run_critical_priority_features_tests(self):
        """Run tests for the three newly implemented Critical Priority features"""
        
        print("\nüî• CRITICAL PRIORITY FEATURES TESTING")
        print("=" * 80)
        print("Testing the three newly implemented 'Critical Priority' features:")
        print("1. Living Evidence Engine System")
        print("2. Advanced Differential Diagnosis System") 
        print("3. Enhanced Explainable AI System")
        print("=" * 80)
        
        # First ensure we have a patient and protocol for testing
        print("\nüìã SETUP: Creating Test Patient and Protocol")
        print("-" * 50)
        patient_created = self.test_create_patient()
        
        if patient_created:
            # Generate a protocol for testing
            protocol_data = {
                "patient_id": self.patient_id,
                "school_of_thought": "ai_optimized"
            }
            
            protocol_success, protocol_response = self.run_test(
                "Setup: Generate Protocol for Testing",
                "POST",
                "protocols/generate",
                200,
                data=protocol_data,
                timeout=90
            )
            
            if protocol_success:
                self.protocol_id = protocol_response.get('protocol_id')
                print(f"   ‚úÖ Test protocol created: {self.protocol_id}")
            else:
                print("   ‚ö†Ô∏è  Protocol creation failed - some tests may be limited")
        
        # Living Evidence Engine System Tests
        print("\nüß¨ LIVING EVIDENCE ENGINE SYSTEM TESTS")
        print("-" * 60)
        self.test_living_evidence_engine_living_map()
        self.test_living_evidence_engine_freshness_analysis()
        self.test_living_evidence_engine_update_mapping()
        self.test_living_evidence_engine_validate_links()
        
        # Advanced Differential Diagnosis System Tests
        print("\nü©∫ ADVANCED DIFFERENTIAL DIAGNOSIS SYSTEM TESTS")
        print("-" * 60)
        self.test_advanced_differential_diagnosis_comprehensive()
        self.test_advanced_differential_diagnosis_confidence_analysis()
        self.test_advanced_differential_diagnosis_tree()
        self.test_advanced_differential_diagnosis_precision_assessment()
        
        # Enhanced Explainable AI System Tests
        print("\nü§ñ ENHANCED EXPLAINABLE AI SYSTEM TESTS")
        print("-" * 60)
        self.test_enhanced_explainable_ai_explanation()
        self.test_enhanced_explainable_ai_visual_breakdown()
        self.test_enhanced_explainable_ai_feature_interactions()
        self.test_enhanced_explainable_ai_transparency_assessment()

    def run_critical_priority_tests(self):
        """Run CRITICAL PRIORITY FEATURES testing as requested in review"""
        print("üöÄ Starting CRITICAL PRIORITY FEATURES Testing Suite")
        print("=" * 80)
        print("Testing three Critical Priority systems after comprehensive fixes:")
        print("1. Living Evidence Engine System (should be 100% functional)")
        print("2. Advanced Differential Diagnosis System (recently fixed)")
        print("3. Enhanced Explainable AI System (comprehensive implementation)")
        print("=" * 80)
        
        # Setup: Create patient for testing
        print("\nüîß SETUP: Creating test patient for Critical Priority testing")
        print("-" * 60)
        self.test_create_patient()
        
        if not self.patient_id:
            print("‚ùå CRITICAL ERROR: Cannot proceed without patient ID")
            return False
        
        # Setup: Generate protocol for testing
        print("\nüîß SETUP: Generating protocol for Critical Priority testing")
        print("-" * 60)
        self.test_generate_protocol_ai_optimized()
        
        # CRITICAL PRIORITY SYSTEM 1: Living Evidence Engine System
        print("\n‚≠ê CRITICAL PRIORITY SYSTEM 1: LIVING EVIDENCE ENGINE SYSTEM")
        print("-" * 60)
        print("Expected: 100% functional (4/4 endpoints working)")
        
        living_evidence_tests = [
            self.test_living_evidence_engine_protocol_mapping,
            self.test_living_evidence_engine_living_reviews,
            self.test_living_evidence_engine_protocol_retrieval,
            self.test_living_evidence_engine_alerts
        ]
        
        living_evidence_passed = 0
        for test in living_evidence_tests:
            if test():
                living_evidence_passed += 1
        
        print(f"\nüìä Living Evidence Engine Results: {living_evidence_passed}/{len(living_evidence_tests)} tests passed")
        
        # CRITICAL PRIORITY SYSTEM 2: Advanced Differential Diagnosis System
        print("\n‚≠ê CRITICAL PRIORITY SYSTEM 2: ADVANCED DIFFERENTIAL DIAGNOSIS SYSTEM")
        print("-" * 60)
        print("Expected: Fixed AttributeError issues, all 3 endpoints working")
        
        differential_diagnosis_tests = [
            self.test_advanced_differential_diagnosis_comprehensive_differential,
            self.test_advanced_differential_diagnosis_engine_status,
            self.test_advanced_differential_diagnosis_retrieval
        ]
        
        differential_diagnosis_passed = 0
        for test in differential_diagnosis_tests:
            if test():
                differential_diagnosis_passed += 1
        
        print(f"\nüìä Advanced Differential Diagnosis Results: {differential_diagnosis_passed}/{len(differential_diagnosis_tests)} tests passed")
        
        # CRITICAL PRIORITY SYSTEM 3: Enhanced Explainable AI System
        print("\n‚≠ê CRITICAL PRIORITY SYSTEM 3: ENHANCED EXPLAINABLE AI SYSTEM")
        print("-" * 60)
        print("Expected: All 5 endpoints functional with comprehensive implementation")
        
        explainable_ai_tests = [
            self.test_enhanced_explainable_ai_enhanced_explanation,
            self.test_enhanced_explainable_ai_explanation_retrieval,
            self.test_enhanced_explainable_ai_visual_breakdown,
            self.test_enhanced_explainable_ai_feature_interactions,
            self.test_enhanced_explainable_ai_transparency_assessment
        ]
        
        explainable_ai_passed = 0
        for test in explainable_ai_tests:
            if test():
                explainable_ai_passed += 1
        
        print(f"\nüìä Enhanced Explainable AI Results: {explainable_ai_passed}/{len(explainable_ai_tests)} tests passed")
        
        # FINAL CRITICAL PRIORITY RESULTS
        total_critical_tests = len(living_evidence_tests) + len(differential_diagnosis_tests) + len(explainable_ai_tests)
        total_critical_passed = living_evidence_passed + differential_diagnosis_passed + explainable_ai_passed
        
        print("\n" + "=" * 80)
        print("üèÅ CRITICAL PRIORITY FEATURES TESTING COMPLETE")
        print("=" * 80)
        print(f"‚≠ê Living Evidence Engine: {living_evidence_passed}/{len(living_evidence_tests)} ({'‚úÖ WORKING' if living_evidence_passed == len(living_evidence_tests) else '‚ùå ISSUES'})")
        print(f"‚≠ê Advanced Differential Diagnosis: {differential_diagnosis_passed}/{len(differential_diagnosis_tests)} ({'‚úÖ WORKING' if differential_diagnosis_passed == len(differential_diagnosis_tests) else '‚ùå ISSUES'})")
        print(f"‚≠ê Enhanced Explainable AI: {explainable_ai_passed}/{len(explainable_ai_tests)} ({'‚úÖ WORKING' if explainable_ai_passed == len(explainable_ai_tests) else '‚ùå ISSUES'})")
        print("-" * 80)
        print(f"üìä OVERALL CRITICAL PRIORITY SUCCESS: {total_critical_passed}/{total_critical_tests} ({(total_critical_passed / total_critical_tests * 100):.1f}%)")
        
        if total_critical_passed == total_critical_tests:
            print("üéâ ALL CRITICAL PRIORITY FEATURES WORKING! Step 1 completion confirmed.")
        else:
            print("‚ö†Ô∏è  Some Critical Priority features have issues. Check output above for details.")
        
        return total_critical_passed == total_critical_tests

    def run_advanced_differential_diagnosis_tests_only(self):
        """Run ONLY Advanced Differential Diagnosis System tests after troubleshoot agent fixes"""
        print("üöÄ Testing ONLY Advanced Differential Diagnosis System After Troubleshoot Agent Fixes")
        print("=" * 80)
        print("üéØ FOCUS: Testing the three specific endpoints mentioned in review request:")
        print("   1. POST /api/diagnosis/comprehensive-differential")
        print("   2. GET /api/diagnosis/engine-status") 
        print("   3. GET /api/diagnosis/{diagnosis_id}")
        print("=" * 80)
        
        # First create a patient for testing
        print("\nüë• SETUP: Creating Patient for Diagnosis Testing")
        print("-" * 50)
        patient_created = self.test_create_patient()
        
        if not patient_created:
            print("‚ùå CRITICAL: Could not create patient - cannot proceed with diagnosis tests")
            return False
        
        # Now run the specific Advanced Differential Diagnosis tests
        print("\nüß† ADVANCED DIFFERENTIAL DIAGNOSIS SYSTEM TESTS")
        print("-" * 50)
        
        # Test 1: Comprehensive differential diagnosis generation
        test1_success = self.test_advanced_differential_diagnosis_comprehensive_differential()
        
        # Test 2: Engine status check
        test2_success = self.test_advanced_differential_diagnosis_engine_status()
        
        # Test 3: Diagnosis retrieval by ID
        test3_success = self.test_advanced_differential_diagnosis_retrieval()
        
        # Additional test with different medical scenario
        test4_success = self.test_advanced_differential_diagnosis_chronic_pain_case()
        
        # Final Results
        print("\n" + "=" * 80)
        print("üèÅ ADVANCED DIFFERENTIAL DIAGNOSIS TESTING COMPLETE")
        print(f"‚úÖ Tests Passed: {self.tests_passed}/{self.tests_run}")
        print(f"üìä Success Rate: {(self.tests_passed/self.tests_run)*100:.1f}%")
        
        # Specific results for the three critical endpoints
        print("\nüéØ CRITICAL ENDPOINT RESULTS:")
        print(f"   1. POST /api/diagnosis/comprehensive-differential: {'‚úÖ PASSED' if test1_success else '‚ùå FAILED'}")
        print(f"   2. GET /api/diagnosis/engine-status: {'‚úÖ PASSED' if test2_success else '‚ùå FAILED'}")
        print(f"   3. GET /api/diagnosis/{'{diagnosis_id}'}: {'‚úÖ PASSED' if test3_success else '‚ùå FAILED'}")
        print(f"   4. Additional chronic pain test: {'‚úÖ PASSED' if test4_success else '‚ùå FAILED'}")
        
        all_critical_passed = test1_success and test2_success and test3_success
        
        if all_critical_passed:
            print("\nüéâ SUCCESS: All critical Advanced Differential Diagnosis endpoints are working!")
            print("‚úÖ The troubleshoot agent fixes have resolved the AttributeError issues")
            print("‚úÖ Methods _generate_explainable_diagnostic_reasoning, _perform_confidence_interval_analysis, and _analyze_diagnostic_mechanisms are now callable")
        else:
            print("\n‚ö†Ô∏è  ISSUES REMAIN: Some Advanced Differential Diagnosis endpoints are still failing")
            print("‚ùå The troubleshoot agent fixes may not have fully resolved the AttributeError issues")
        
        return all_critical_passed


if __name__ == "__main__":
    import sys
    
    # Check if we want to run only the Advanced Differential Diagnosis tests
    if len(sys.argv) > 1 and sys.argv[1] == "--diagnosis-only":
        print("üöÄ RegenMed AI Pro - Advanced Differential Diagnosis System Testing")
        print("=" * 80)
        
        tester = RegenMedAIProTester()
        success = tester.run_advanced_differential_diagnosis_tests_only()
        
        print("\n" + "=" * 80)
        print("üéØ ADVANCED DIFFERENTIAL DIAGNOSIS TESTING RESULTS")
        print("=" * 80)
        print(f"‚úÖ Tests Passed: {tester.tests_passed}")
        print(f"‚ùå Tests Failed: {tester.tests_run - tester.tests_passed}")
        print(f"üìä Total Tests Run: {tester.tests_run}")
        print(f"üéØ Success Rate: {(tester.tests_passed/tester.tests_run)*100:.1f}%")
        
        if success:
            print("\nüéâ ALL ADVANCED DIFFERENTIAL DIAGNOSIS TESTS PASSED!")
            print("The troubleshoot agent fixes have successfully resolved the AttributeError issues!")
        else:
            print("\nüö® SOME ADVANCED DIFFERENTIAL DIAGNOSIS TESTS FAILED!")
            print("The troubleshoot agent fixes may need further attention.")
        
        print("\n" + "=" * 80)
    else:
        print("üöÄ RegenMed AI Pro - Critical Priority Features Testing")
        print("=" * 80)
        
        tester = RegenMedAIProTester()
        
        # Run only the critical priority features tests
        tester.run_critical_priority_features_tests()
        
        print("\n" + "=" * 80)
        print("üéØ CRITICAL PRIORITY FEATURES TESTING RESULTS")
        print("=" * 80)
        print(f"‚úÖ Tests Passed: {tester.tests_passed}")
        print(f"‚ùå Tests Failed: {tester.tests_run - tester.tests_passed}")
        print(f"üìä Total Tests Run: {tester.tests_run}")
        print(f"üéØ Success Rate: {(tester.tests_passed/tester.tests_run)*100:.1f}%")
        
        if tester.tests_passed == tester.tests_run:
            print("\nüéâ ALL CRITICAL FEATURES TESTS PASSED!")
            print("Living Evidence Engine, Advanced Differential Diagnosis, and Enhanced Explainable AI systems are fully functional!")
        elif tester.tests_passed / tester.tests_run >= 0.8:
            print("\n‚úÖ EXCELLENT! Most critical features tests passed.")
        elif tester.tests_passed / tester.tests_run >= 0.6:
            print("\n‚ö†Ô∏è  GOOD! Majority of critical features tests passed. Some issues need attention.")
        else:
            print("\nüö® ATTENTION NEEDED! Multiple critical features test failures detected.")
        
        print("\n" + "=" * 80)